[
["index.html", "APS 135: Introduction to Exploratory Data Analysis with R Information and overview Aims How to use the book Getting help Updates", " APS 135: Introduction to Exploratory Data Analysis with R Dylan Z. Childs 2017-03-09 Information and overview This is the online course book for the Introduction to Exploratory Data Analysis with R component of (APS 135) module. You can view this book in any modern desktop browser, as well as on your phone or tablet device. The site is self-contained—it contains all the material you are expected to learn this year. Dylan Childs is the author. Please email him if you spot any problems. Aims You will be introduced to the R ecosystem. R is now very widely used by biologists and environmental scientists to access data, carry out interactive data analysis, build mathematical models and produce high quality figures. We will teach you a little basic R programming so that you are in a position to address these needs in future if you need to. You don’t have to become an expert programmer to have a successful career in science, but knowing a little bit about programming has become (almost) a prerequisite for doing biological research in the 21st century. You will learn how to begin using R to carry out data manipulation and visualisation. Designing good experiments, collecting data, and analysis are hard, and these activities often takes a great deal time and money. If you want to effectively communicate your hard-won, latest, greatest results, it is difficult to beat a good figure or diagram (conversely, if you want to be ignored, put everything into a boring table). R is really good at producing figures, so even if you end up just using it as a platform for visualising data, your time hasn’t been wasted. This book provides a foundation for learning statistics later on. If you want to be a biologist, particularly one involved in research, there is really no way to avoid using statistics. You might be able to dodge it by becoming a theoretician, but if that really is your primary interest you should probably being studying for a mathematics degree. For the rest of us who collect data, or at least analyse other people’s data, knowing about statistics is essential: it allows us to distinguish between real patterns (the “signal”) and chance variation (the “noise”). Topics The topics we will cover in this book are divided into three sections: The Getting Started with R block introduces the R language and the RStudio environment for working with R. We aim to run through much of what you need to know to start using R to improve your productivity. This includes some basic terminology, how to use R packages, and how to access help. As noted earlier, we are not trying to turn you into an expert programmer. That takes too long (not everyone enjoys programming, though many of you may be surprised to discover that you do in fact like it). By the end of this block you will know enough about R to begin learning the more practical material that follows. The Data Wrangling with R block aims to show you how to manipulate your data with R. The truth is that if you regularly work with data, a large amount of time will inevitably be spent getting data into the format you need. The informal name for this is “data wrangling”. This is a topic that is not often taught well to undergraduates, which is a shame, because mastering the art of data wrangling saves you a lot of time in the long run. This block will briefly cover two R packages to help you do this: dplyr and tidyr. We’ll learn how to get data into and out of R, makes subsets of important variables, create new variables, summarise your data, and so on. The Exploratory Data Analysis block is all about using R to help you understand and describe your data. The first step in any analysis after you have managed to wrangle the data into shape almost always involves some kind of visualisation and/or numerical summary (or at least that should be the next step if you are serious about getting your analysis right). In this block you will learn how to do this using one of the best plotting systems in R: ggplot2. We will review the different kinds of variables you might have to analyse, discuss the different ways you can describe them, both visually and with numbers, and learn how to explore relationships between variables. How to use the book This book covers all the material you need to get to grips with this year, some of which we will not have time to cover in the practicals. No one is expecting you to memorise everything in the book. It is designed to serve as a resource for you to refer to over the next 2-3 years (and beyond) as needed. However, you should aim to familiarise yourself with the content so that you know where to look for information or examples when needed. Try to understand the important concepts and then worry about the specific details. What should you be doing as you read about each topic? There is a lot of R code embedded in the book, most of which you can just copy and paste into RStudio and then run. You are strongly encouraged to do this when you first work through a topic. The best way to learn something like R is to use it actively, not just read about it. Experimenting with different code snippets by changing them is also a very good way to learn what they do. You can’t really break R (well you can, but it is quite hard), and working out why something does or does not work will help you learn to use it. Text, instructions, and explanations Normal text, instructions, explanations etc. are written in the same type as this document, we will tend to use bold for emphasis and italics to highlight specific technical terms when they are first introduced (italics will also crop up with Latin names from time to time, but this is unlikely to produce too much confusion!) At various points in the text you will come across text in different coloured boxes. These are designed to highlight stand-alone exercises or little pieces of supplementary information that might otherwise break the flow. There are three different kinds of boxes: This is an action box. We use these when we want to say something important. For example, we might be summarising a key learning outcome or giving you instructions to do something. This is a warning box. These contain a warning or a common “gotcha”. There are a number of common pitfalls that trip up new users of R. These boxes aim to highlight these and show you how to avoid them. It’s a good idea to pay attention to these. This is an information box. These aim to offer a not-too-technical discussion of how or why something works the way it does. You do not have to understand everything in these boxes to use R, but the information will help you understand how it works. R code and output in this book We will try to illustrate as many ideas as we can using snippets of real R code. Stand alone snippets will be formatted like this: tmp &lt;- 1 print(tmp) ## [1] 1 At this point it does not matter what the above actually means. You just need to understand how the formatting of R code in this book works. The lines that start with ## show us what R prints to the screen after it evaluates an instruction and does whatever was asked of it, that is, they show the output. The lines that do not start with ## show us the instructions, that is, they show us the input. So remember, the absence of ## shows us what we are asking R to do, otherwise we are looking at something R prints in response to these instructions. This typeface is used to distinguish R code within a sentence of text: e.g. “We use the mutate function to change or add new variables.” A sequence of selections from an RStudio menu is indicated as follows: e.g. File ▶ New File ▶ R Script File names referred to in general text are given in upper case in the normal typeface: e.g. MYFILE.CSV. Getting help You will learn various ways of finding help about R in this book. If you find yourself stuck at any point these should your first port of call. If you are still struggling, try the following, in this order: Google is your friend. One of the nice consequences of R’s growing popularity and the rise of blogging – take a look at R Bloggers for a flavour of R-specific blogs – is that the web is now packed full of useful tutorials and tips, many of which are aimed at beginners. One of the objectives of this book is turn you into a self sufficient useR. Learning how to solve your own R-related problems is an essential pre-requisite for this to happen. Solving your own problems will also help you learn how to use R more effectively. If an hour of Googling does not solve a problem, post a question on the APS 135 Facebook page. If you find something difficult, the chances are that someone else finds it difficult too. You are strongly encouraged to try to address one anothers’ problems via this page. Thinking through and explaining the answer to a question someone else has posed is a really good way of learning. Dylan will check the Facebook page from time to time, and will offer a solution if no-one else has suggested one. We would much prefer you to help each other though. We encourage you to try options 1 and 2 first. Nonetheless, on occasion Google may turn out not to be your friend and a post to the Facebook page might not elicit a satisfactory response. In these instances you are welcome to email Dylan with your query. You are unlikely to receive an answer at the weekend though. Updates We may occasionally decide to update the book in light of the results of comments and questions we receive from you. This is another reason why it is important for you to ask or post questions—it allows us to see where people are struggling. It is also a motivation for choosing to use a website rather than a static document—we can very easily adapt or extend the content to address problems as they arise. If we do update the book, we will let you know what has changed. "],
["get-up-and-running-with-r-and-rstudio.html", "Get up and running with R and RStudio What is R? What is RStudio (and why use it)? Working at the Console", " Get up and running with R and RStudio What is R? The answer to this question very much depends on who we ask. The geeky answer is something like this… R is a dialect of the S language, which was developed by John Chambers and colleagues at Bell Laboratories in the mid 1970s. It was designed to offer an interactive computing environment for statisticians and scientists to carry out data analysis. There are essentially two widely used versions of S (though others have started to appear), a commercial one called S-Plus, and the open source implementation known as R. S-Plus came first, and although it is still around, it is used less each year. Development of R was begun in the late 1990s by two academics, Ross Ihaka and Robert Gentleman, at the University of Auckland. Their motivation was to create an open source language to enable researchers in computational statistics to explore new ideas. That language quickly evolved into something that looked more and more S-like, which we now know as R (GNU R, to be overly precise). We could go on and on about the various features that R possesses. R is a functional programming language, it supports object orientation, etc etc… but these kinds of explanations are only helpful to someone who already knows about computer languages. It is useful to understand why so many people have turned to R to meet their data analysis needs. When a typical R user talks about “R” they are often referring to two things at once, the GNU R language and the ecosystem that exists around the language: R is all about data analysis. We can carry out any standard statistical analysis in R, as well as access a huge array of more sophisticated tools with impressive names like “structural equation model”, “random forests” and “penalized regression”. These days, when statisticians and computer scientists develop a new analysis tool, they often implement it in R first. This means a competent R user can always access the latest, cutting edge analysis tools. R also has the best graphics and plotting facilities of any platform. With sufficient expertise, we can make pretty much any type of figure we need (e.g. scatter plots, phylogenetic trees, spatial maps, or even volcanoes). In short, R is a very productive environment for doing data analysis. Because R is such a good environment for data analysis, a very large community of users has grown up around it. The size of this community has increased steadily since R was created, but this growth has really increased up in the last 5-10 years or so. In the early 2000s there were very few books about R and the main way to access help online was through the widely-feared R mailing lists. Now, there are probably hundreds of books about different aspects of R, online tutorials written by enthusiasts, and many websites that exist solely to help people learn R. The resulting ecosystem is vast, and though it can be difficult to navigate at times, when we run into an R-related problem the chances are that the answer is already written down somewhere1. R is not just about data analysis—though we will mostly use it this way. It is a fully-fledged programming language, meaning that once you become moderately proficient with it you can do things such as construct numerical simulation models, solve equations, query websites, send emails, access the foaas web service, and carry out many other tasks we don’t have time to write down. We won’t do any of this year or next but it is worth noting that R can do much more than just analyse data if we need it to. Getting and installing R R is open source, meaning anyone can download the source code – the collection of computer instructions that define R – and assuming they have enough time, energy and expertise, they are free to alter it as they please. Open source does not necessarily mean free, as in it costs £0 to download and use, but luckily R is free in this sense. If you are working on the University managed desktops it should already have been installed and is ready for you to use. We encourage you to install a copy on your own laptop so that you can work at home, in the library, at a café, or wherever else you find you are productive. Do not use R on its own though. Use it in combination with the RStudio IDE discussed in the next section. In order to install R you need to download the appropriate installer from the Comprehensive R Archive Network (CRAN). We are going to use the “base distribution” as this contains everything you need to use R under normal circumstances. There is a single installer for Windows. On a Mac, it’s important to match the installer to the version of OS X. In either case, R uses a the standard install mechanism that should be familiar to anyone who has installed an application on their machine. There is no need to change the default settings—doing so will probably lead to problems later on. Go ahead and install R on your own computer now. You won’t be able to make much use of this book without it. After installing R it should be visible in the Programs menu on a Windows computer or in the Applications folder on a Mac. However, it would be a good idea to read the next section before launching R… What is RStudio (and why use it)? R and RStudio are not the same thing. We can run R without RStudio if we need to, but we cannot run RStudio without R. Remember that! R is essentially just a computer program that sits there and waits for instructions in the form of text. Those instructions can be typed in by a user like you or me, or they can be sent to it from another program. This means you can run R in a variety of different environments. The job of RStudio is to provide an environment that makes R a more pleasant and productive tool. One way to get a sense of why RStudio is a Very Good Thing is to look at what running R without it is like. The simplest way to run it on a Linux or Unix-based machine (like a Mac) is to use something called the Terminal. It’s well beyond the scope of this book to get into what this is, but in a nutshell, the Terminal provides a low-level, text-based way to interact with a computer. Here is what R looks like running inside a Terminal on a Mac: We can run R in much the same way on Windows using the “Command Prompt” if we need to. The key thing you need to take away from that screenshot is that running R like this is very “bare bones”. We typed the letter “R” in the Terminal and hit Enter to start R. It printed a little information as it started up and then presented us with “the prompt” (&gt;), waiting for input. This is where we type or paste in instructions telling R what to do. There is no other way to interact with it when we run R like this – no menus or buttons, just a lonely prompt. The developers of R on Windows PCs and Macs provide a slightly nicer way to work with R. When we download and install R for either of these two operating systems, in addition to the basic R program that we just saw running in a Terminal, we also get another program that acts as a Graphical User Interface (GUI) for R. This is the thing labelled “R” in the Programs menu on a Windows computer or the Applications folder on a Mac. If you launch the R GUI on your computer you will be presented with roughly the same thing on either a Windows PC or a Mac. There will be something called the Console, which is where you interact directly with R by typing things at the prompt (which looks like this: &gt;), and a few buttons and menus for managing common tasks. We will not go through these two GUIs in any more detail because we are not going to use them. We just need to know they exist so we don’t confuse them with RStudio. So what is RStudio? The first thing to note is that it is a different program from R. Remember that! RStudio is installed installed separately from R and occupies its own place in the Programs menu (Windows PC) or Applications folder (Mac). In one sense RStudio is just another Graphical User Interface for R which improves on the “bare bones” experience. However, it is a GUI on steroids. It is more accurate to describe it as an Integrated Development Environment (IDE). There is no all-encompassing definition of an IDE, but they all exist to make programmer’s lives easier by integrating various useful tools into a single piece of software. From the perspective of this book, there are four key features that we care about: The R interpreter—the thing that was running in the Terminal above—runs inside RStudio. It’s accessed via a window labelled Console. This is where we type in instructions we want to execute when we are working directly with R. The Console also shows us any output that R prints in response to these instructions. So if we just want the “bare bones” experience, we can still have it. RStudio provides facilities for working with R programs using something called a Source Code Editor. An R program ( also called a “script”)&quot; is just is a collection of instructions in the R language that have been saved to a text file. Nothing more! However, it is much easier to work with a script using a proper Source Code Editor than an ordinary text editor like Notepad. An good IDE like RStudio also gives you a visual, point-and-click means of accessing various language-specific features. This is a bit difficult to explain until we have have actually used some of these, but trust us, being able to do things like manage packages, set working directories, or inspect objects we’ve made simplifies day-to-day use of R. This especially true for new users. RStudio is cross-platform—it will run on a Windows PC, a Linux PC or a Mac. In terms of the appearance and the functionality it provides, RStudio is exactly the same on each of these platforms. If we learn to work with R via RStudio on a Windows PC, it’s no problem migrating to a Mac or Linux PC later on if we need to. This is a big advantage for those of us who work on multiple platforms. We’re only going to scratch the surface of what RStudio can do and there are certainly alternative bits of software that could meet our immediate needs. The reason for introducing a powerful tool like RStudio is because one day you may need to access things like debugging facilities, package building tools, repository management. RStudio makes it easy to use these advanced tools. Getting and installing RStudio RStudio is developed and maintained by a for-profit company called… RStudio. They make their money by selling software tools and services related to R and RStudio. The basic desktop version of RStudio is free to download and use though. It can be downloaded from the RStudio download page. The one to go for is the Open Source Edition of RStudio Desktop, not the commercial version of RStudio Desktop. RStudio installs like any other piece of software, so there’s nothing to configure after installation. If you haven’t already done it, go ahead and install RStudio Desktop on your own computer. You are going to need it. The anatomy of RStudio Once it’s installed RStudio is run like any other stand-alone application, via the Programs menu or the Applications folder on a Windows PC or Mac, respectively2. We’ll say this one last time—RStudio only works if we’ve also installed R. Here is how RStudio appears the first time it runs: There are three panes inside a single window, which we have labelled with red numbers. Each of these has a well-defined purpose. Let’s take a quick look at these: The large window on the left is the Console. We have already told you what this is for—the Console lets you know what R is doing and provides a mechanism to interact with R by typing instructions. All this happens at the prompt, &gt;. We will be working in the Console in a moment so won’t say any more about this here. The window at the top right contains two tabs. The first of these, labelled Environment, allows us to see all the different R objects we can access. There are also some buttons that help us to get data into and out of R. The second, labelled History, allows us to see a list of instructions we’ve previously sent to R. The buttons in this tab allow us to reuse or save these instructions. The window at the bottom right contains five tabs. The first, labelled Files, gives us a way to interact with the files and folders. The next tab, labelled Plots, is where any figures we produce are displayed. This tab also allows you to save your figures to file. The Packages tab is where we view, install and update packages used to extend the functionality of R. The Help tab is where you can access and display various different help pages. The Viewer is essentially an embedded web browser for working with interactive output—we won’t be using it in this course. Don’t be alarmed if RStudio looks different on your computer. There are a couple of reasons why this might be the case. First, the appearance of RStudio is highly customisable. Take a quick look at the Tools &gt; Global Options... window to see what we mean. Second, there is a fourth window that is sometimes be visible when we work with RStudio—the source code Editor we mentioned above. RStudio saves its state between different sessions, so if we have already messed about with RStudio’s appearance or left a script open last time we used it you will see these changes. RStudio will change over time Keep in mind that RStudio is very actively developed, which means features tend to appear or change over time. Consequently, if you update it regularly expect the odd thing to change here and there. This is generally a good thing—it usually means new features have been added—but it does require you to occasionally adjust to new additions. Working at the Console R was designed to be used interactively—it is what is known as an interpreted language, which we can interact with via something called a Command Line Interface (CLI). This is just a fancy way of saying that we can type an instructions to “do something” directly into the Console and those instructions will then be interpreted when we hit the Enter key. If our R expression does not contain any errors, R will then do something like read in some data, perform a calculation, make a figure, and so on. What actually happens obviously depends on what we ask it to do. Let’s briefly see what all this means by doing something very simple with R. Type 1 + 3 at the Console and hit the Enter key: 1+3 ## [1] 4 The first line above just reminds us what we typed into the Console. The line after that beginning with ## shows us what R printed to the Console after reading and evaluating our instructions. What just happened? We can ignore the [1] bit for now (the meaning of this will become clear later in the course). What are we left with – the number 2. The instruction we gave R was in effect “evaluate the expression 1 + 3”. R read this in, decided it was a valid R expression, evaluated the expression, and then printed the result to the Console for us. Unsurprisingly, the expression 1 + 3 is a request to add the numbers 1 and 3, and so R prints the number 4 to the Console. OK, that was not very exciting. In the next chapter we will start learning to use R to carry out more useful calculations. The important take-away from this is that this sequence of events—reading instructions, evaluating those instructions and printing their output—happens every time we type or paste something into the Console and hit Enter. The printing bit is optional by the way. Whether or not it happens depends on whether you decide to capture the output or not. Just remember, if R does not print anything to the Console it does not necessarily mean nothing has happened. Why do we keep using that word expression? It has a very specific meaning in computer science. The Wikipedia page says: An expression in a programming language is a combination of explicit values, constants, variables, operators, and functions that are interpreted according to the particular rules of precedence and of association for a particular programming language, which computes and then produces another value. That probably doesn’t make much sense, but it at least demonstrates why we don’t let computer scientists teach biologists about programming. In simple terms, an R expression is a small set of instructions written in human readable(ish) text that tell R to do something. That’s it. We could write “instructions” instead of “expressions” throughout this book but we may as well use the correct word. Whatever we call them, our aim is to learn how to combine sequences of expressions to Get Things Done in R. That’s what this book is about. The other big change is that R is finally starting to become part of the commercial landscape—learning how to use it can only improve your job prospects.↩ If you use a computer running Linux we assume you know what you are doing when it comes to installing and running software.↩ "],
["a-quick-introduction-to-r.html", "Chapter 1 A quick introduction to R 1.1 Using R as a big calculator 1.2 Storing and reusing results 1.3 How does assignment work? 1.4 Global environment 1.5 Naming rules and conventions", " Chapter 1 A quick introduction to R 1.1 Using R as a big calculator 1.1.1 Basic arithmetic The end of the Get up and running with R and RStudio chapter demonstrated that R can handle familiar arithmetic operations: addition, subtraction, multiplication, division. If we want to add or subtract a pair of numbers just place the + or - symbol in between two numbers, hit Enter, and R will read the expression, evaluate it, and print the result to the Console. This works exactly as we expect it to: 3 + 2 ## [1] 5 5 - 1 ## [1] 4 Multiplication and division are no different, though we don’t use x or ÷ for these operations. Instead, we use * and / to multiply and divide: 7 * 2 ## [1] 14 3 / 2 ## [1] 1.5 We can also exponentiate a numbers: raise one number to the power of another. We use the ^ operator to do this: 4^2 ## [1] 16 This raises 4 to the power of 2 (i.e. we squared it). In general, we can raise a number x to the power of y using x^y. Neither x or y need to be a whole numbers either. Arithmetic operations can also be combined into one expression. Assume we want to subtract 6 from 23. The expression to perform this calculation is: 2^3 - 6 ## [1] 2 \\(2^3=8\\) and \\(8-6=2\\). Simple enough, but what if we had wanted to carry out a slightly longer calculation that required the last answer to then be divided by 2? This is the wrong the way to do it: 2^3 - 6 / 2 ## [1] 5 The answer we were looking for is \\(1\\). So what happened? R evaluated \\(6/2\\) first and then subtracted this answer from \\(2^3\\). If that’s obvious, great. If not, it’s time to learn a bit about the order of precendence used by R. R uses a standard set of rules to decide the order in which arithmetic calculations feed into one another so that it can unambiguously evaluate any expression. It uses the same order as every other computer language, which thankfully is the same one we all learned in mathematics class at school. The order of precedence used is: exponents and roots (“taking powers”) multiplication and division additional and subtraction BODMAS and friends If you find it difficult to remember order of precedence used by R, there are a load of mnemonics that can to help. Pick one you like and remember that instead. In order to get the answer we were looking for we need to take control of the order of evaluation. We do this by enclosing grouping the necessary bits of the calculation inside parentheses (“round brackets”). That is, we place ( and ) either side of them. The order in which expressions inside different pairs of parentheses are evaluated follows the rules we all had to learn at school. The R expression we should have used is therefore: (2^3 - 6) / 2 ## [1] 1 We can use more than one pair of parentheses to control the order of evaluation in more complex calculations. For example, if we want to find the cube root of 2 (i.e. 21/3) rather than 23 in that last calculation we would instead write: (2^(1/3) - 6) / 2 ## [1] -2.370039 The parentheses around the 1/3 in the exponent are needed to ensure this is evaluated prior to being used as the exponent. 1.1.2 Problematic calculations Now is a good time to highlight how R handles certain kinds of awkward numerical calculations. One of these involves division of a number by 0. Some programming languages will respond to an attempt to do this with an error. R is a bit more forgiving: 1/0 ## [1] Inf Mathematically, division of a finite number by 0 equals A Very Large Number: infinity. R has a special built in data value that allows it to handle this kind of thing. This is Inf, which of course stands for “infinity”. The other special kind of value we sometimes run into can be generated by numerical calculations that don’t have a well-defined result. For example, it arises when we try to divide 0 or infinity by themselves: 0/0 ## [1] NaN The NaN in this result stands for Not a Number. R produces NaN because \\(0/0\\) is not defined mathematically: it produces something that is Not a Number. The reason we are pointing out Inf and NaN is not because we expect to use them. It’s important to know what they represent because they often arise as a result of a mistake somewhere in a program. It’s hard to track down such mistakes if we don’t know how Inf and NaN arise. That is enough about using R as a calculator for now. What we’ve seen—even though we haven’t said it yet—is that R functions as a REPL: a read-eval-print loop (there’s no need to remember this term). R takes user input, evaluates it, prints the results, and then waits for the next input. This is handy, because it means we can use it interactively, working through an analysis line-by-line. However, to use R to solve for complex problems we need to learn how to store and reuse results. We’ll look at this in the next section. Working efficiently at the Console Working at the Console soon gets tedious if we have to retype similar things over and over again. There is no need to do this though. Place the cursor at the prompt and hit the up arrow. What happens? This brings back the last expression sent to R’s interpreter. Hit the up arrow again to see the last-but-one expression, and so on. We go back down the list using the down arrow. Once we’re at the line we need, we use the left and right arrows to move around the expression and the delete key to remove the parts we want to change. Once an expression has been edited like this we hit Enter to send it to R again. Try it! 1.2 Storing and reusing results So far we’ve not tried to do anything remotely complicated or interesting, though we now know how to construct longer calculations using parentheses to control the order of evaluation. This approach is fine if the calculation is very simple. It quickly becomes unwieldy for dealing with anything more. The best way to see what we mean is by working through a simple example—solving a quadratic equation. Quadratic equations looks like this: \\(a + bx + cx^2 = 0\\). If we know the values of \\(a\\), \\(b\\) and \\(c\\) then we can solve this equation to find the values of \\(x\\) that ensure the left hand side equals the right hand side. Here’s the well-known formula for these solutions: \\[ x = \\frac{-b\\pm\\sqrt{b^2-4ac}}{2a} \\] Let’s use R to calculate these solutions for us. Say that we want to find the solutions to the quadratic equation when \\(a=1\\), \\(b=6\\) and \\(c=5\\). We just have to turn the above equation into a pair of R expressions: (-6 + (6^2 -4 * 1 * 5)^(1/2)) / (2 * 1) ## [1] -1 (-6 - (6^2 -4 * 1 * 5)^(1/2)) / (2 * 1) ## [1] -5 The output tells us that the two values of \\(x\\) that satisfy this particular quadratic equation are -1 and -5. What should we do if we now need to solve a different quadratic equation? Working at the Console, we could bring up the expressions we typed (using the up arrow) and then go through each of these, changing the numbers to match the new values of \\(a\\), \\(b\\) and \\(c\\). Editing individual expressions like this is fairly tedious, and more importantly, it’s fairly error prone because we have to make sure we substitute the new numbers at exactly the right positions. A partial solution to this problem is to store the values of \\(a\\), \\(b\\) and \\(c\\). We’ll see precisely why this is useful in a moment. First, we need to learn how to store results in R. The key to this is to use the assigment operator, written as a left arrow &lt;-. Sticking with our original example, we need to store the numbers 1, 6 and 5. We do this using three expressions, one after the another: a &lt;- 1 b &lt;- 6 c &lt;- 5 Notice that we don’t put a space between &lt; and -—R won’t like it if we try to add one. R didn’t print anything to screen, so what actually happened? We asked R to first evaluate the expression on the right hand side of each &lt;- (just a number in this case) and then assign the result of that evaluation instead of printing it. Each result has a name associated with it, which appears on the left hand side of the &lt;-. RStudio shortcut We use the assignment operator &lt;- all the time when working with R, and because it’s inefficient to have to type the &lt; and - characters over and over again, RStudio has a built in shortcut for typing the assignment operator: Alt + - . Try it. Move the curser to the Console, hold down the Alt key (‘Option’ on a Mac), and press the - sign key. RStudio will auto-magically add insert &lt;-. The net result of all this is that we have stored the numbers 1, 6 and 5 somewhere in R, associating them with the letters a, b and c, respectively. What does this mean? Here’s what happens if we type the letter a into the Console and hit Enter: a ## [1] 1 It looks the same as if we had typed the number 1 directly into the Console. The result of typing b or c is hopefully obvious. What we just did was to store the output that results from evaluating three separate R expressions, associating each a name so that we can access them again3. Whenever we use the assignment operator &lt;- we are telling R to keep whatever kind of value results from the calculation on the right hand side of &lt;-, giving it the name on the left hand side so that we can access it later. Why is this useful? Let’s imagine we want to do more than one thing with our three numbers. If we want to know their sum or their product we can now use: a + b + c ## [1] 12 a * b * c ## [1] 30 So once we’ve stored a result and associated it with a name we can reuse it wherever it’s needed. Returning to our motivating example, we can now calculate the solutions to the quadratic equation by typing these two expressions into the Console: (-b + (b^2 -4 * a * c)^(1/2)) / (2 * a) ## [1] -1 (-b - (b^2 -4 * a * c)^(1/2)) / (2 * a) ## [1] -5 Imagine we’d like to find the solutions to a different quadratic equation where \\(a=1\\), \\(b=5\\) and \\(c=5\\). We just changed the value of \\(b\\) here to keep things simple. To find our new solutions we have to do two things. First we change the value of the number associated with b… b &lt;- 5 …then we bring up those lines that calculate the solutions to the quadratic equation and run them, one after the other: (-b + (b^2 -4 * a * c)^(1/2)) / (2 * a) ## [1] -1.381966 (-b - (b^2 -4 * a * c)^(1/2)) / (2 * a) ## [1] -3.618034 We didn’t have to retype those two expressions. We could just use the up arrow to bring each one back to the prompt and hit Enter. This is much simpler than editing the expressions. More importantly, we are beginning to see the benefits of using something like R: we can break down complex calculations into a series of steps, storing and reusing intermediate results as required. 1.3 How does assignment work? It’s important to understand, at least roughly, how assignment works. The first thing to note is that when we use the assignment operator &lt;- to associate names and values, we informally refer to this as creating (or modifying) a variable. This is much less tedious than using words like “bind”, “associate”, value“, and”name&quot; all the time. Why is it called a variable? What happens when we run these lines: myvar &lt;- 1 myvar &lt;- 7 The first time we used &lt;- with myvar on the left hand side we created a variable myvar associated with the value 1. The second line myvar &lt;- 7 modified the value of myvar to be 7. This is why we refer to myvar as a variable: we can change the its value as we please. What happened to the old value associated with myvar? In short, it is gone, kaput, lost… forever. The moment we assign a new value to myvar the old one is destroyed and can no longer be accessed. Remember this. Keep in mind that the expression on the right hand side of &lt;- can be any kind of calculation, not just just a number. For example, if I want to store the number 1, associating it with answer, I could do this: answer &lt;- (1 + 2^3) / (2 + 7) That is a strange way to assign the number 1, but it illustrates the point. More generally, as along as the expression on the right hand side generates an output it can be used with the assignment operator. For example, we can create new variables from old variables: newvar &lt;- 2 * answer What happened here? Start at the right hand side of &lt;-. The expression on this side contained the variable answer so R went to see if answer actually exists in the global environment. It does, so it then substituted the value associated with answer into the requested calculation, and then assigned the resulting value of 2 to newvar. We created a new variable newvar using information associated with answer. Now look at what happens if we just copy a variable using the assignment operator: myvar &lt;- 7 mycopy &lt;- myvar At this point we have two variables, myvar and mycopy, each associated with the number 7. There is something very important going on here: each of these is associated with a different copy of this number. If we change the value associated with one of these variables it does not change the value of the other, as this shows: myvar &lt;- 10 myvar ## [1] 10 mycopy ## [1] 7 R always behaves like this unless we work hard to alter this behaviour (we never do this in this book). So remember, every time we assign one variable to another, we actually make a completely new, independent copy of its associated value. For our purposes this is a good thing because it makes it much easier to understand what a long sequence of R expressions will do. That probably doesn’t seem like an obvious or important point, but trust us, it is. 1.4 Global environment Whenever we associate a name with a value we create a copy of both these things somewhere in the computer’s memory. In R the “somewhere” is called an environment. We aren’t going to get into a discussion of R’s many different kinds of environments—that’s an advanced topic well beyond the scope of this book. The one environment we do need to be aware of though is the Global Environment. Whenever we perform an assignment in the Console the name-value pair we create (i.e. the variable) is placed into the Global Environment. The current set of variables are all listed in the Environment tab in RStudio. Take a look. Assuming that at least one variable has been made, there will be two columns in the Environment tab. The first shows us the names of all the variables, while the second summarises their values. The Global Environment is temporary By default, R will save the Global Environment whenever we close it down and then restore it in the next R session. It does this by writing a copy of the Global Environment to disk. In theory this means we can close down R, reopen it, and pick things up from where we left off. Don’t do this—it only increases the risk of making a serious mistake. Assume that when R and RStudio are shut down, everything in Global Environment will be lost. 1.5 Naming rules and conventions We don’t have to use a single letter to name things in R. The words tom, dick and harry could be used in place of a, b and c. It might be confusing to use them, but tom, dick and harry are all legal names as far as to R is concerned: A legal name in R is any sequence of letters, numbers, ., or _, but the sequence of characters we use must begin with a letter. Both upper and lower case letters are allowed. For example, num_1, num.1, num1, NUM1, myNum1 are all legal names, but 1num and _num1 are not because they begin with 1 and _. R is case sensitive—it treats upper and lower case letters as different characters. This means that num and Num are treated as distinct names. Forgetting about case sensitivity is a good way to create errors when using R. Try to remember that. Don’t begin a name with . We are allowed to begin a name with a ., but this usually is A Bad Idea. Why? Because variable names that begin with . are hidden from view in the Global Environment—the value it refers to exists but it’s invisible. This behaviour exists to allow R to create invisible variables that control how it behaves. This is useful, but it isn’t really meant to be used by the average user. Technically, this is called binding the name to a value. You really don’t need to remember this though.↩ "],
["scripts.html", "Chapter 2 Building scripts 2.1 Introduction 2.2 Writing scripts in RStudio 2.3 Running scripts in RStudio 2.4 Spotting problems", " Chapter 2 Building scripts 2.1 Introduction We have seen that using variables is useful because it enables us to break down a problem into a series of simpler steps. So far however, we have only been working in the Console. If we want to reuse a calculation when we’re working like this, we have to change a variable or two and then evaluate the expressions that do the job of solving an equation, make a graph, whatever. We also have to do all of this in the correct order, or things will not work as intended. Working directly at the Console like this is the simplest way to use R. Generally speaking, we do not recommend working this way unless you only need to do something very simple which involves a handful of steps. For more complicated activities you should store your instructions in a script. We will look at how to do this at the end of this chapter. The aim of this section is to demonstrate how to use R to perform simple calculations and introduce a few key concepts that will help you understand how R works. At this stage – for the purposes of learning how to use R – typing expressions directly into the Console is fine. You can see that working in the Console is not going to be practical most of the time. So what should we do? The answer is: put your sequence of R expressions into a text file, called a script (We already told you the answer earlier). Calling it a script makes it sound a bit fancy and clever—“I spent all day debugging my script”. It is not. It is a boring text file that could be opend up in something like Notepad.exe. We just call it a script to signify the fact that the text contained in the file is a series of instructions telling our computer to do something. 2.2 Writing scripts in RStudio To open a new script in RStudio navigate to File &gt; New File &gt; R Script. This will open the new file in a fourth pane. This pane is the Source Code Editor we mentioned in the Get up and running with R and RStudio chapter. The name of the tab where this new file lives will be set to Untitled1 if you haven’t opened any other new R Scripts. Here is what RStudio looks like after we do this (we’ve highlighted the new pane with a red asterisk): When you work with a script you type the required sequence of R expressions into the Editor pane, not directly into the Console. This is important—if you mix and match you will make mistakes. The worst of these is that you will write a script that seems to work, only to find it is broken when you open it up and use it again later. This usually happens because you typed something into the Console that is needed to make the whole script run when you were preparing it, but then forget to put it into the script. Just don’t mix and match between using the Console and the Editor and you will avoid this. The easiest way to appreciate the benefits of using a script is to work with one. If you have not already done so, open up a new script using File &gt; New File &gt; R Script and copy-paste this into the Editor pane: a &lt;- 1 b &lt;- 6 c &lt;- 5 sqrt.b2ac &lt;- (b^2 -4 * a * c)^(1/2) (-b + sqrt.b2ac) / (2 * a) (-b - sqrt.b2ac) / (2 * a) Here is a partial screenshot of the Editor pane showing this script on our computer: MISSING SENTENCE. Syntax highlighting is a must have feature of any Editor. In a nutshell, syntax is a bit like the grammar of a computer language. It is the set of rules that determine how we form valid expressions, assign variables, and so on. The purpose of syntax highlighting is to draw attention to different components of syntax. You can see that when we use the Cobalt highlighting option, RStudio sets the background to black and displays variables in white, parentheses and arithmetic operators in orange, and numbers in red. It doesn’t matter so much what the colours are. What matters is that we have a visual means to distinguish these different kinds of elements, making it much easier to read a script. Choose your own colour scheme The first thing you will probably notice is that this Editor looks a little different from yours. We said earlier that RStudio was highly customisable. What we did above was change the way it does something called syntax highlighting. You can do this by navigating to Tools &gt; Global Options…, selecting the Appearance button, and picking the Cobalt option under Editor theme (try it!). The other kind of elements RStudio has highlighted are in blue. We added these. They are called comments. Comments in R always start with a # symbol—this is called the “hash”&quot; symbol if you are British (or the “pound” symbol if you are North American). Lines that start with # are completely ignored by R. They exist only to allow us, the developers of a script, to add notes and explanations that remind us how it all works. Comments are important At this point we just want to emphasise that you should use comments to remind yourself what your R code is supposed to be doing. Use them liberally to help you understand the logic of each script you write. This is another “take our word”&quot; for it situation – if you do not use comments, then when you come back to your precious script in a few weeks/months/years time you will have no idea what it does. 2.3 Running scripts in RStudio The whole point of writing a script is ultimatley to run it. The phrase “run our code” is shorthand for “send a number of R expressions to the R interpreter to be read and evaluated”. The latter is tedious to write (and read) over and over again, so we will just write “run your/my/our code”. We could run the code in the above script by copying and pasting it into the Console, but this is inefficient. Instead of relying on cut and paste, RStudio gives us different ways to run our code: There is a Run button at the top right of the Editor pane. As you might imagine, clicking on this will run some code. If you haven’t highlighted anything in the Editor, this runs whichever line the cursor is at, i.e. it runs just that one line. If you had highlighted a region inside the Editor, this button will run all of that in sequence. We do not like clicking buttons. If you are like us, you are in luck. Pressing Control+Enter (or Command+Enter on a Mac) does exactly the same thing as the Run button. It also uses the same rules to decide which bits of code to run or not4. Using which ever method you prefer, run every line in the script you just started. Here is what should happen at the Console when you do this: a &lt;- 1 b &lt;- 6 c &lt;- 5 sqrt.b2ac &lt;- (b^2 -4 * a * c)^(1/2) (-b + sqrt.b2ac) / (2 * a) ## [1] -1 (-b - sqrt.b2ac) / (2 * a) ## [1] -5 This works exactly as though you had typed or pasted the sequence of R expressions into the Console, hitting Enter each time you get to the end of a line. What this means is that we can use this script to find the solutions to any quadratic equation with real roots (if you know what we mean by the phrase “real roots”, great, if not, it does not really matter). All you have to do is edit the values assigned to a, b and c and then rerun the whole script. Do not rerun bits of it. Rerun it all. Try it. Just be sure to only use sets of values where \\(b^2 &gt; 4ac\\) or you will see the dreaded NaN. Now that we have a script that does something a little useful we might wish to use it again. It is just a text file, so we can save the script as we would any other file. We can do this using the familiar menu-based approach (File &gt; Save As...) or via the keyboard shortcut Control+S (or Command+S on a Mac). The only thing to keep in mind is that you should use the file extension .R or .r, e.g. my_great_script.R. This is because RStudio uses the file extension to detect the fact that a file is a script and not an ordinary text file. If you do not do this, then next time you open up the file in RStudio you won’t be able to access the fancy Editor features like syntax highlighting, nor will you be able to send lines to the Console without using copy-paste. From now on you should always work with scripts. No more typing into the Console! 2.4 Spotting problems We may as well get something out of the way early on: you will ask R to do something that turns out to contain an error. Mistakes happen all the time when You will do this a lot. It’s not a problem when this happens. When it does though, it’s important to step back and work out what went wrong. 2.4.1 The dreaded + Be careful when you highlight code to run. RStudio will run exactly the text you highlight. If you start or finish the highlighted region in the middle of an expression then one of three things will usually happen. If you are lucky you will generate an error, since you end up running only part an expression that is not itself a valid expression. We say this is lucky because the error will at least be easy to spot. If you are unlucky, you might end up running part an expression that is itself a valid expression. This is harder to spot because it won’t generate an error, and it will probably create problems further down the line. The third outcome is that your Console will look something like this (ignore the colours – we am still using that Cobalt theme): What happened? Look carefully at the little snippet of R code we sent to the Console. It is not a complete R expression because it is missing a closing parenthesis, ). When R receives only part of an expression like this, which has correct syntax but is not complete, it sits and waits for the rest of the expression. This is what the + at the Console signifies. When you see this happen you have two options. You can manually type in the missing part of the expression and hit Enter, or you can hit the Escape key to return you to the prompt &gt; and start again. The first option is very error prone, so we would generally do the latter. 2.4.2 Errors Here is an example of what happens at the Console when you generate an error: xyz + 2 ## Error in eval(expr, envir, enclos): object &#39;xyz&#39; not found In general terms, what happened is that R read in the instruction xyz + 2, tried to evaluate it, and found it could not. This is because the variable xyz does not exist. Upon running into the error, R printed something to the screen to tell us we have made a mistake (“Error: object ‘xyz’ not found”). When this happens we say R has “thrown an error”. We know this because the message is in a warning colour (prbably red or orange—it depends how RStudio is set up) and it contains the word Error. The bit after the : is an attempt by R to tell me what went wrong. You should always read your error messages. They will be incomprehensible at first, but as you fix the many mistakes you will inevitably make, they will start to make more sense and become helpful (usually—sometimes they make no sense whatsoever, even to experienced users). This way of learning only works if you read the error messages in the first place though. We can also run lines of code via the Code &gt; Run Lines. This is easily the most inefficient method after cut and paste. It’s only there in case we forget our keyboard shortcut or cannot remember where the run button is.↩ "],
["using-functions.html", "Chapter 3 Using functions 3.1 Introduction 3.2 Functions and arguments 3.3 Evaluating arguments and returning results 3.4 Functions do not have “side effects” 3.5 Combining functions 3.6 Specifying function arguments", " Chapter 3 Using functions 3.1 Introduction Functions are a basic building block of any programming language. If you want to use R effectively—even if your needs are very simple—you need to understand how to use functions. We are not aiming to unpick the inner workings of functions in this course. That is an advanced topic that is best saved for a future time when you have gained much more experience of using them5. The aim of this chapter is to explain what functions are for, how to use them, and how to avoid mistakes when doing so. 3.2 Functions and arguments The job of each function in R is to carry out some kind of calculation or computation that would typically require many lines of R code to do “from scratch”. Functions allow us to reuse common computations while offering some control over the precise details of what actually happens. The best way to see what we mean by this is to see one in action. The round function is used to round one or more number(s) to a significant number of digits. To use it, we could type this into the Console and hit Enter: round(x = 3.141593, digits = 2) We have suppressed the output for now so that we can unpack things a bit first. Every time we use a function we always have to work with the same basic construct (there are a few exceptions, but we can ignore these for now). We start with the name of the function, that is, we use the name of the function as the prefix. In this case, the function name is round. After the function name, we need a pair of opening and closing parentheses. It is this combination of name and parentheses that that alerts R to fact that we are trying to use a function. Whenever you see name followed by opening and closing parentheses you are seeing a function in action. What about the bits inside the parentheses? These are called the arguments of the function. That is a horrible name, but it is the one that everyone uses so you will have to get used to it. Depending on how it was defined, a function can take zero, one, or more arguments. We will discuss this idea in more detail later in this section. In the simple example above, we used the round function with two arguments. Each of these was supplied as a name-value pair, separated by a comma. When working with arguments, name-value pairs occur either side of the equals (=) sign, with the name of the argument on the left hand side and the value it takes on the right hand side (notice that the syntax highlighter we used to make this website helpfully colours the argument names differently from the values). The name serves to identify which argument we are working with, and the value is the thing that controls what that argument does in the function. We refer to the process of associating argument names and values as “supplying the arguments” of the function (sometimes we also say “setting the arguments”). Notice the similarly between supplying function arguments and the assignment operation discussed in the last topic. The difference here is that name-value pairs are associated with the = symbol. This association is also temporary: it only lasts as long as it takes for the function to do whatever it does. Use = to assign arguments Do not use the assignment operator &lt;- inside the parentheses when working with functions. This is a “trust us” situation: you will end up in all kinds of difficulty if you do this. The names of the arguments that we are allowed to use are typically determined for us by the function. That is, we are not free to choose whatever name we like. We say “typically”, because R is a very flexible language and so there are certain exceptions to this simple rule of thumb. For now it is simpler to think of the names as pre-determined by whatever function you are using though. The arguments control the behaviour of a function. Our job as users is to set the values of the these to get the behaviour we want. By now it is now probably fairly obvious what is going to happen when we used the round function like this at the Console: round(x = 3.141593, digits = 2) ## [1] 3.14 Remember, we said the round function rounds one or more numbers to a number of significant digits. The argument that specifies the focal number(s) is x; the second argument, digits, specifies the number of decimal places we require. Based on the supplied values of these arguments, 3.141593 and 2, respectively, the round function spits out a value of 3.14, which is then printed to the Console. If we had wanted to the answer to 3 significant digits we would use digits = 3. This is what we mean when we say the values of the supplied arguments controls the behaviour of the function. 3.3 Evaluating arguments and returning results Whenever R evaluates a function we refer to this action as “calling the function”. In our simple example, we called the round function with arguments x and digits (in this course we treat the phrases “use the function” and “call the function” as synonyms, as the former is more natural to new users). What we have just seen—though you may not realise it yet—is that when we call functions they first evaluate their arguments, then perform some kind of action, and finally (optionally) return a value to us when they finish doing whatever it is they do We will discuss that word “return” in a moment. What do we mean by the word “evaluate” in this context? Take a look at this second example which uses round again: round(x = 2.3 + 1.4, digits = 0) ## [1] 4 When you call a function, what typically happens is that everything on the right hand side of an = is first evaluated, the result of this evaluation becomes associated with the corresponding argument name, and then the function does its calculations using the resulting name-value pairs. We say “typically” because other kinds of behaviours are possible—remember, R is a very flexible language—though for the purposes of this course we can assume that what we just wrote is always true. What happened above is that R evaluated 2.3 + 1.4, resulting in the number 3.7, which was then associated with the argument x. We set digits to 0 this time so that round just returns a whole number, 4. The important thing to realise is that the expression(s) on the right hand side of the = can be anything you like. This third example essentially equivalent to the last one: myvar &lt;- 2.3 + 1.4 round(x = myvar, digits = 0) ## [1] 4 This time we created a new variable called myvar and then supplied this as the value of the x argument. When we call the round function like this, the R interpreter spots the fact that something on the right hand side of an = is a variable and associates the value of this variable with x argument. As long as we have actually defined the numeric variable myvar at some point we can use it as the value of an argument. Keeping in mind what you have just learned, take a careful look at this example: x &lt;- 0 round(x = 3.7, digits = x) ## [1] 4 What is going on here? The key to understanding this is to realise that the symbol x is used in two different ways here. When it appears on the left hand side of the = it represents an argument name. When it appears on the right hand side it is treated as a variable name, which must have a value associated with it for the above to be valid. This is admittedly a slightly confusing way to use this function, but it is perfectly valid. The message here is that what matters is where things appear relative to the =, not the symbols used to represent them. We said at the beginning of this section that a function may optionally return a value to us when they finish complete their task. That word “return” is just jargon that refers to the process by which a function outputs a value. If you use a function at the Console this will be the value printed at the end. We can use this value in other ways too. For example, there is nothing to stop you combining function calls with the arithmetic operations: 2 * round(x = 2.64, digits = 0) ## [1] 6 Here the The R interpreter first evaluates the function call, and then multiplies the value it returns by 2. As you would expect, if we want to reuse this value we have to assign the result of function call, for example: roundnum &lt;- 2 * round(x = 2.64, digits = 0) Using a function with &lt;- is really no different from the examples using multiple arithmetic operations in the last topic. The R interpreter starts on the right hand side of the &lt;-, evaluates the function call there, and only then assigns the value to roundnum. 3.4 Functions do not have “side effects” There is one more idea about functions and their arguments that you really need to understand in order to avoid confusing yourself later on in this course. It relates to how functions modify their arguments, or more accurately, how they do not modify their arguments. Take a look at this example: myvar &lt;- 3.7 round(x = myvar, digits = 0) ## [1] 4 myvar ## [1] 3.7 We created a variable myvar with the value 3.7, rounded this to a whole number with round, and then printed the value of myvar. Notice that the value of myvar has not changed after using it as an argument to round. This is important. R functions typically do not alter the values of their arguments. Again, we say “typically” because there are ways to alter this behaviour if we really want to (yes, R is a very flexible language), but we will never ever do this. The standard behaviour—that functions do not alter their arguments—is what is meant by the phrase “functions do not have side effects”. If we had meant to round the value of myvar so that we can use this new value later on, we have to assign the result of function evaluation, like this: myvar &lt;- 3.7 myvar &lt;- round(x = myvar, digits = 0) In this example, we just overwrote the old value, but we could just as easily have created a new variable. The reason this is worth pointing out is that new users sometimes assume certain types of functions will alter their arguments. Specifically, when working with functions manipulate something called a data.frame, there is a tendency to assume that the function changes the data.frame argument. It will not. If you want to make use of the changes, rather than just see them printed to the Console, you need to assign the results. You can do this by creating a new variable or overwriting the old one. You will gain first hand experience of this in the Data Wrangling block. Remember, functions do not have side effects! Some of you will forget this and it will create all kinds of headaches. Don’t be that person. 3.5 Combining functions Up until now we have not tried to do anything very complicated in our examples. Using R to actually get useful work almost always involves multiple steps, very often facilitated a number of different functions. There is more than one way to do this. Here is a simple example that takes an approach you should already understand: myvar &lt;- sqrt(x = 10) round(x = myvar, digits = 0) ## [1] 3 Hopefully you can see what happened here. We calculated the square root of the number 10 and assigned the result to myvar, then we rounded this to a whole number and printed the result to the Console. So one way to use a series of functions in sequence is to assign a name to the result at each step and use this as an argument to the function in the next step. Here is another way to replicate the calculation in the previous example: round(x = sqrt(x = 10), digits = 0) ## [1] 3 The technical name for this is function composition (you might recognise that phrase from secondary school mathematics). Another way of referring to this kind of expression is as a nested function call: we say that the sqrt function is nested inside the round function. The way you should read these constructs is from the inside out. The sqrt(x = 10) expression is on the right hand side of an = symbol, so this is evaluated first, the result is associated with the x argument of the round function, and only then does the round function do its job. There aren’t really any new ideas here. We have already seen that the R interpreter evaluates whatever is on the right hand side of the = symbol first before associating the resulting value with the appropriate argument name. However, new users are often confused by nested function calls, so we want you to see them in action. There is nothing to stop us using multiple levels of nesting either. Take a look at this example: round(x = sqrt(x = abs(x = -10)), digits = 0) ## [1] 3 The abs function takes the absolute value of a number, i.e. removes the - sign if it is there. Remember, read nested calls from the inside out. In this example, first we took the absolute value of -10, then we took the square root of the resulting number (10), and then we rounded this to a whole number. Nested function calls are useful because they make our R code less verbose (we have to write less), but this comes at the cost of reduced readability. Experienced users are used to nesting so they don’t mind it, but we leave it to you to decide whether to use it much. Though we aim to keep function nesting to a minimum in this course, you will occasionally have to work with the nesting construct so you should try to understand it even if you don’t like using it. We will also see a much-easier-to-read method for applying a series of functions in the Data Wrangling block. 3.6 Specifying function arguments We have only been working with functions that carry out mathematical calculations with numbers so far. We will see many more in this course as it unfolds. Some functions are designed to extract information about functions for us. For example, take a look at the args function: args(name = round) ## function (x, digits = 0) ## NULL You might be able to see what args does: it prints a summary of the main arguments of a function to the Console (it doesn’t always print all the available arguments though). What can we learn from the summary of the round arguments? Notice that the first argument, x, is shown without an associated value, whereas the digits part of the summary is printed as digits = 0. The significance of this is that digits has a default value. This means that we can leave out digits when using the round function: round(x = 3.7) ## [1] 4 This is obviously the same result as we would get using round(x = 3.7, digits = 0). This is a very useful feature of R, as it allows us keep our R code concise. Some functions take a large number of arguments, many of which are defined with sensible defaults. Unless we need to change these default arguments, we can just ignore them when we call such functions. The x argument of round does not have a default, which means we have to supply a value. This is sensible, as the whole purpose of round is to round any number we give it. There is another way to simplify our use of functions. Take a look at this example: round(3.72, digits = 1) ## [1] 3.7 What does this demonstrate? We do not have to specify argument names, i.e. there is no need to specify argument names. In the absence of an argument name the R interpreter uses the position of the supplied argument to work out which name to associate it with. In this example we left out the name of the argument at position 1. This is where x belongs, so we end up rounding 3.71 to 1 decimal place. R is even more flexible than this, as it carries out partial matching on argument names: round(3.72, dig = 1) ## [1] 3.7 This also works because R can unambiguously match the argument I named dig to digits. Take note, if there were another argument to round that started with the letters dig this would have caused an error. You need to know your function arguments if you want to rely of partial matching. Be careful with your arguments Here is some advice. Do not rely on partial matching of function names. It just leads to confusion and the odd error. If you use it a lot you end up forgetting the true name of arguments, and if you abbreviate too much you create name matching conflicts. For example, if a function has arguments arg1 and arg2 and you use the partial name a for an argument, there is no way to know which argument you meant. We are pointing out partial matching so that you are aware of the behaviour. It is not worth the hassle of getting it wrong just to save on a little typing, so do not use it. What about position matching? This can also cause problems if we’re not paying attention. For example, if you forget the order of the arguments to a function and then place your arguments in the wrong place, you will either generate an error or produce a nonsensical result. It is nice not to have to type out the name = value construct all the time though, so our advice is to rely positional matching only for the first argument. This is a common convention in R, and it makes sense because it is often obvious what kind of information or data the first argument should carry, so the its name is redundant. At some point, if you really want to understand what happens when you use a function you will need to grapple with ideas like lazy evaluation, environments and scoping.↩ "],
["numeric-vectors.html", "Chapter 4 Numeric vectors 4.1 Introduction 4.2 Atomic vectors 4.3 Numeric vectors 4.4 Constructing numeric vectors 4.5 Named vectors 4.6 Generating sequences of numbers 4.7 Vectorised operations", " Chapter 4 Numeric vectors 4.1 Introduction The term “data structure” computer science jargon for a particular way of organising data on our computers, so that it can be accessed easily and efficiently. Computer languages use many different kinds of data structures, but fortunately, we only need to learn about a couple of relatively simple ones to use R for data analysis. In fact, in this book only two kinds of data structure really matter: “vectors” and “data frames”. We’ll learn how to work with data frames in the next section of the book. The next three chapters will consider vectors. This chapter has two goals. First, we want to learn the basics of how to work with vectors. For example, we’ll see how “vectorised operations” may be used to express a repeated calculation. Second, we’ll learn how to construct and use numeric vectors to perform various calculations. Keep in mind that although we’re going to focus on numeric vectors, many of the principles we learn here can be applied to the other kinds of vectors considered later. 4.2 Atomic vectors A vector is a 1-dimensional object that contains a set of data values, which are accessible by their position: position 1, position 2, position 3, and so one. When people talk about vectors in R they’re often referring to atomic vectors6. An atomic vector is the simplest kind of data structure in R. There are a few different kinds of atomic vector, but the defining feature of each one is that it can only contain data of one type. An atomic vector might contain all integers (e.g. 2, 4, 6, …) or all characters (e.g. “A”, “B”, “C”), but it can’t mix and match integers and characters (e.g. “A”, 2, “C”, 5). The word “atomic” in the name refers to the fact that an atomic vector can’t be broken down into anything simpler—they are the simplest kind of data structure R knows about. Even when working with a single number we’re actually dealing with an atomic vector. It just happens to be of length one. Here’s the very first expression we evaluated in the first chapter: 1 + 1 ## [1] 2 Look at the output. What is that [1] at the beginning? We ignored it before because we weren’t in a position to understand its significance. The [1] is a clue that the output resulting from 1 + 1 is a atomic vector. We can verify this with the is.vector and is.atomic functions: x &lt;- 1 + 1 # what value is associated with x? x ## [1] 2 # is it a vector? is.vector(x) ## [1] TRUE # is it atomic? is.atomic(x) ## [1] TRUE This little exercise demonstrates an important point about R. Atomic vectors really are the simplest kind of data structure in R. Unlike many other languages there is simply no way to represent just a number. Instead, a single number must be stored as a vector of length one7. 4.3 Numeric vectors A lot of work in R involves numeric vectors. After all, data analysis is all about numbers. Here’s a simple way to construct a numeric vector: numeric(length = 50) ## [1] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## [36] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 What happened? We made a numeric vector with 50 elements, each of which is the number 0. The word “element” is used to reference any object (a number in this case) that resides at a particular position in a vector. When we create a vector but don’t assign it to a name using &lt;- R just prints it to the Console for us. Notice what happens when the vector is printed to the screen. Since the length-50 vector can’t fit on one line, it was printed over two. At the beginning of each line there is a [X]: the X is a number that describes the position of the element shown at the beginning of a particular line. Different kinds of numbers Roughly speaking, R stores numbers in two different ways depending of whether they are whole numbers (“integers”) or numbers containing decimal points (“doubles” – don’t ask). We’re not going to worry about this distinction. Most of the time the distinction is fairly invisible to users so it is easier to just think in terms of numeric vectors. We can mix and match integers and doubles in R without having to worry too much about R is storing the numbers. If we need to check that we really have made a numeric vector we can use the is.numeric function to do this: # let&#39;s create a variable that is a numeric vector numvec &lt;- numeric(length = 50) # check it really is a numeric vector is.numeric(numvec) ## [1] TRUE This returns TRUE as expected. A value of FALSE would imply that numvec is some other kind of object. This may not look like the most useful function in the world, but sometimes we need functions like is.numeric to understand what R is doing or root out mistakes in our scripts. Keep in mind that when we print a numeric vector to Console R only prints the elements to 7 significant figures by default. We can see this by printing the built in constant pi to the Console: pi ## [1] 3.141593 The actual value stored in pi is actually much more precise than this. We can see this by printing pi again using the print function: print(pi, digits = 16) ## [1] 3.141592653589793 4.4 Constructing numeric vectors We just saw how to use the numeric function to make a numeric vector of zeros. The size of the vector is controlled by the length argument—we used length = 50 above to make a vector with 50 elements. This is arguably not a particularly useful skill, as we usually need to work vectors of particular values (not just 0). A very useful function for creating custom vectors is the c function. Take a look at this example: c(1.1, 2.3, 4.0, 5.7) ## [1] 1.1 2.3 4.0 5.7 The “c” in the function name stands for “combine”. The c function takes a variable number of arguments, each of which must be a vector of some kind, and combines these into a single, new vector. We supplied the c function with four arguments, each of which was a vector of length 1 (remember: a single number is treated as a length-one vector). The c function combines these to generate a vector of length 4. Simple. Now look at this example: vec1 &lt;- c(1.1, 2.3) vec2 &lt;- c(4.0, 5.7, 3.6) c(vec1, vec2) ## [1] 1.1 2.3 4.0 5.7 3.6 This shows that we can use the c function to concatenate (“stick together”) two or more vectors, even if they are not of length 1. We combined a length-2 vector with a length-3 vector to produce a new length-5 vector. Notice that we did not have to name the arguments in those two examples—there were no = involved. The c function is an example of one of those flexible functions that breaks the simple rules of thumb for using arguments that we set out earlier: it can take a variable number of arguments, and these arguments do not have predefined names. This behaviour is necessary if you think about how c has to work: in order to be useful it needs to be flexible enough to take any combination of arguments. Finding out about a vector R Sometimes it is useful to be able to find out a little extra information about a vector you are working with, especially if it is very large. Three functions that can extract some useful information about a vector for us are length, head and tail. Using a variety of different vectors, experiment with these to find out what they do. Make sure you use vectors that aren’t too short (e.g. with a length of at least 10). Hint: head and tail can be used with a second argument, n. 4.5 Named vectors What happens if we name the arguments to c when constructing a vector? Take a look at this: namedv &lt;- c(a = 1, b = 2, c = 3) namedv ## a b c ## 1 2 3 What happened here is that the argument names were used to define the names of elements in the vector we made. The resulting vector is still a 1-dimensional data structure. When it is printed to the Console the value of each element is printed, along with the associated name above it. We can extract the names from a named vector using the names function: names(namedv) ## [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; Being able to name the elements of a vector is very useful because it enables us to more easily identify relevant information and extract the bits we need—we’ll see how this works in the next chapter. 4.6 Generating sequences of numbers The main limitation of the c function is that we have to manually construct vectors from their elements. This isn’t very practical if we need to construct very long vectors of numeric values. There are various functions that can help with this kind of thing though. These are useful when the elements of the target vector need to follow a sequence or repeating pattern. This may not appear all that useful at first, but repeating sequences are used a lot in R. 4.6.1 Generating sequences of numbers The seq function allows us to generate sequences of numbers. It needs at least two arguments, but there are several different ways to control the sequence produced by seq. The method used is determined by our choice of arguments: from, to, by, length.out and along.with. We don’t need to use all of these though—setting 2-3 of these arguments will often work: Using the by argument generates a sequence of numbers that increase or decrease by the requested step size: seq(from = 0, to = 12, by = 2) ## [1] 0 2 4 6 8 10 12 This is fairly self-explanatory. The seq function constructed a numeric vector with elements that started at 0 and ended 12, with successive elements increasing in steps of 2. Be careful when using seq like this. If the by argument does not lead to a sequence that ends exactly on the value of to then that value won’t appear in the vector. For example: seq(from = 0, to = 11, by = 2) ## [1] 0 2 4 6 8 10 You can generate a descending sequence by using a negative by value, like this: seq(from = 12, to = 0, by = -2) ## [1] 12 10 8 6 4 2 0 Using the length.out argument generates a sequence of numbers where the resulting vector has the length specified by length.out: seq(from = 0, to = 12, length.out = 6) ## [1] 0.0 2.4 4.8 7.2 9.6 12.0 Using the length.out argument will always produce a sequence that starts and ends exactly on the values specified by from and to (if we need a descending sequence we just have to make sure from is larger than to). Using the along.with argument allows us to use another vector to determine the length of the numeric sequence we want to produce: # make any any vector we like my_vec &lt;- c(1, 3, 7, 2, 4, 2) # use it to make a sequence of the same length seq(from = 0, to = 12, along.with = my_vec) ## [1] 0.0 2.4 4.8 7.2 9.6 12.0 It doesn’t matter what the elements of myvec are. The behaviour of seq is controlled by the length of myvec when we use along.with. It’s conventional to leave out the from and to argument names when using the seq function. For example, we could rewrite the first example above as: seq(0, 12, by = 2) ## [1] 0 2 4 6 8 10 12 When we leave out the name of the third argument its value is position matched to the by argument: seq(0, 12, 2) ## [1] 0 2 4 6 8 10 12 However, our advice is to explicitly name the by argument instead of relying on position matching, because this will remind you what you’re doing and will stop you forgetting about the different control arguments. If we do not specify values of either by, length.out and along.with when using seq the default behaviour is to assume we meant by = 1: seq(from = 1, to = 12) ## [1] 1 2 3 4 5 6 7 8 9 10 11 12 Generating sequences of integers that go up or down in steps of 1 is something we do a lot in R. Because of this, there is a special operator to generate these simple sequences—the colon, :. For example, to produce the last sequence again we use: 1:12 ## [1] 1 2 3 4 5 6 7 8 9 10 11 12 When we use the : operator the convention is to not leave spaces either side of it. 4.6.2 Generating repeated sequences of numbers The rep function is designed to replicate the values inside a vector, i.e. it’s short for “replicate”. The first argument (called x) is the vector we want to replicate. There are two other arguments that control how this is done. The times argument specifies the number of times to replicate the whole vector: # make a simple sequence of integers seqvec &lt;- 1:4 seqvec ## [1] 1 2 3 4 # now replicate *the whole vector* 3 times rep(seqvec, times = 3) ## [1] 1 2 3 4 1 2 3 4 1 2 3 4 All we did here was take a sequence of integers from 1 to 4 and replicate this end-to-end three times, resulting in a length-12 vector. Alternatively, we can use the each argument to replicate each element of a vector while retaining the original order: # make a simple sequence of integers seqvec &lt;- 1:4 seqvec ## [1] 1 2 3 4 # now replicate *each element* vector 3 times rep(seqvec, each = 3) ## [1] 1 1 1 2 2 2 3 3 3 4 4 4 This example produced a similar vector to the previous one. It contains the same elements and has the same length, but now the order is different. All the 1’s appear first, then the 2’s, and so on. Predictably, we can also use both the times and each arguments together if we want to: seqvec &lt;- 1:4 rep(seqvec, times = 3, each = 2) ## [1] 1 1 2 2 3 3 4 4 1 1 2 2 3 3 4 4 1 1 2 2 3 3 4 4 The way to think about how this works is to imagine that you apply rep twice, first with each = 2, then with times = 3 (or vice versa). We can achieve the same thing using nested function calls, though it is much uglier: seqvec &lt;- 1:4 rep(rep(seqvec, each = 2), times = 3) ## [1] 1 1 2 2 3 3 4 4 1 1 2 2 3 3 4 4 1 1 2 2 3 3 4 4 4.7 Vectorised operations All the simple arithmetic operators (e.g. + and -) and many mathematical functions are vectorised in R . What this this means is that when we use a vectorised function it operates on vectors on an element-by-element basis. Take a look at this example to see what we mean by this: # make a simple sequence x &lt;- 1:10 x ## [1] 1 2 3 4 5 6 7 8 9 10 # make another simple sequence *of the same length* y &lt;- seq(0.1, 1, length = 10) y ## [1] 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 # add them x + y ## [1] 1.1 2.2 3.3 4.4 5.5 6.6 7.7 8.8 9.9 11.0 We constructed two length-10 numeric vectors, called x and y, where x is a sequence from 1 to 10 and y is a sequence from 0.1 to 1. When R evaluates the expression x + y it does this by adding the first element of x to the first element of y, the second element of x to the second element of y, and so on, working through all 10 elements of x and y. Vectorisation is also implemented in the standard mathematical functions. For example, our friend the round function will round each element of a numeric vector: # make a simple sequence of non-integer values my_nums &lt;- seq(0, 1, length = 13) my_nums ## [1] 0.00000000 0.08333333 0.16666667 0.25000000 0.33333333 0.41666667 ## [7] 0.50000000 0.58333333 0.66666667 0.75000000 0.83333333 0.91666667 ## [13] 1.00000000 # now round the values round(my_nums, digits = 2) ## [1] 0.00 0.08 0.17 0.25 0.33 0.42 0.50 0.58 0.67 0.75 0.83 0.92 1.00 The same behaviour is seen with other mathematical functions like sin, cos, exp, and log. Each of these will apply the relevant function to each element of a numeric vector. Not all functions are vectorised. For example, the sum function takes a vector of numbers and adds them up: sum(my_nums) ## [1] 6.5 Although sum obviously works on a numeric vector it is not “vectorised” in the sense that it works element-by-element to return an output vector of the same length as its main argument. Many functions apply the vectorisation principle to more than one argument. Take a look at this example to see what we mean by this: # make a repeated set of non-integer values my_nums &lt;- rep(2 / 7, times = 6) my_nums ## [1] 0.2857143 0.2857143 0.2857143 0.2857143 0.2857143 0.2857143 # round each one to a different number of decimal places round(my_nums, digits = 1:6) ## [1] 0.300000 0.290000 0.286000 0.285700 0.285710 0.285714 We constructed a length 6 vector containing the number 2/7 (~ 0.285714) and then used the round function to round each element to a different number of decimal places. The first element was rounded to 1 decimal place, the second to two decimal places, and so on. We get this behaviour because instead of using a single number for the digits argument, we used a vector that is an integer sequence from 1 to 6. Vectorisation is not the norm R’s vectorised behaviour may seem like the “obvious” thing to do, but most computer languages do not work like this. In other languages we typically have to write a much more complicated expression to do something so simple. This is one of the reasons R is such a data analysis language: vectorisation allows us to express repetitious calculations in a simple, intuitive way. This behaviour can save us a lot of time. However, not every function treats its arguments in a vectorised way, so we always need to check (most easily, by experimenting) whether this behaviour is available before relying on it. The other common vector is called a “list”. Lists are very useful but we won’t cover them in this book↩ The same is true for things like sets of characters (&quot;dog&quot;, &quot;cat&quot;, &quot;fish&quot;, …) and logical values (TRUE or FALSE) discussed in the next two chapters.↩ "],
["other-kinds-of-vectors.html", "Chapter 5 Other kinds of vectors 5.1 Introduction 5.2 Character vectors 5.3 Logical vectors", " Chapter 5 Other kinds of vectors 5.1 Introduction The data we collect and analyse are often in the form of numbers, so it comes as no surprise that we work with numeric vectors a lot in R. Nonetheless, we also sometimes need to represent other kinds of vectors, either to represent different types of data, or to help us manipulate our data. This chapter introduces two new types of atomic vector to help us do this: character vectors and logical vectors. 5.2 Character vectors The elements of character vectors are what are known as a “character string” (or “string” if we are feeling lazy). The term “character string” refers a sequence of characters, such as “Treatment 1”, “University of Sheffield”, “Population Density”. A character vector is an atomic vector that stores an ordered collection of one or more character strings. If we want to construct a character vector in R we have to place double (&quot;) or single (') quotation marks around the characters. For example, we can print the name “Dylan” to the Console like this: &quot;Dylan&quot; ## [1] &quot;Dylan&quot; Notice the [1]. This shows that what we just printed is an atomic vector of some kind. We know it’s a character vector because the output is printed with double quotes around the value. We often need to make simple character vectors containing only one value—for example, to set the values of arguments to a function. The quotation marks are not optional—they tell R we want to treat whatever is inside them as a literal value. The quoting is important. If we try to do the same thing as above without the quotes we end up with an error: Dylan ## Error in eval(expr, envir, enclos): object &#39;Dylan&#39; not found What happened? When the interpreter sees the word Dylan without quotes it assumes that this must be the name of a variable, so it goes in search of it in the global environment. We haven’t made a variable called Dylan, so there is no way to evaluate the expression and R spits out an error to let us know there’s a problem. Longer character vectors are typically constructed to represent data of some kind. The c function is often a good starting point for this kind of thing: # make a length-3 character vector my_name &lt;- c(&quot;Dylan&quot;, &quot;Zachary&quot;, &quot;Childs&quot;) my_name ## [1] &quot;Dylan&quot; &quot;Zachary&quot; &quot;Childs&quot; Here we made a length-3 character vector, with elements corresponding to a first name, middle name, and last name. If we want to extract one or more elements from a character vector by their position Take note, this is not equivalent to the above : my_name &lt;- c(&quot;Dylan Zachary Childs&quot;) my_name ## [1] &quot;Dylan Zachary Childs&quot; The only element of this character vector is a single character string containing the first, middle and last name separated by spaces. We didn’t need to use the the c function here because we were only ever working with a length-1 character vector. i.e. we could have typed &quot;Dylan Zachary Childs&quot; and we would have ended up with exactly the same text printed at the Console. We can construct more complicated, repeating character vectors with rep. This works on character vectors in exactly the same way as it does on numeric vectors: c_vec &lt;- c(&quot;Dylan&quot;, &quot;Zachary&quot;, &quot;Childs&quot;) rep(c_vec, each = 2, times = 3) ## [1] &quot;Dylan&quot; &quot;Dylan&quot; &quot;Zachary&quot; &quot;Zachary&quot; &quot;Childs&quot; &quot;Childs&quot; &quot;Dylan&quot; ## [8] &quot;Dylan&quot; &quot;Zachary&quot; &quot;Zachary&quot; &quot;Childs&quot; &quot;Childs&quot; &quot;Dylan&quot; &quot;Dylan&quot; ## [15] &quot;Zachary&quot; &quot;Zachary&quot; &quot;Childs&quot; &quot;Childs&quot; Each element was replicated twice (each = 2), and then the whole vector was replicated three times (times = 3), end to end. What about the seq function? Hopefully it is fairly obvious that we can’t use this function to build a character vector. The seq function is designed to make sequences of numbers, from a starting value, to another. The notion of a sequence of character strings – for example, from &quot;Dylan&quot; to &quot;Childs&quot; – is meaningless. 5.3 Logical vectors The elements of logical vectors only take two values: TRUE or FALSE. Don’t let the simplicity of logical vectors fool you—they’re very useful. As with other kinds of atomic vectors the c and rep functions can be used to construct a logical vector: l_vec &lt;- c(TRUE, FALSE) rep(l_vec, times = 2) ## [1] TRUE FALSE TRUE FALSE Hopefully nothing about that output is surprising by this point. So why are logical vectors useful? Their allow us to represent the results of questions such as, “is x greater than y” or “is x equal to y”. The results of such comparisons may then be used to carry out various kinds of subsetting operations based. Let’s first look at how we use logical vectors to evaluate comparisons. Before we can do that though we need to introduce relational operators. These sound fancy, but they are very simple: we use relational operators to evaluate the relative value of vector elements. Six are available in R: x &lt; y: is x less than y? x &gt; y: is x greater than y? x &lt;= y: is x less than or equal to y? x &gt;= y: is x greater than or equal to y? x == y: is x equal to y? x != y: is x not equal to y? The easiest way to understand how these work is to simply use them. We need a couple of numeric variables first: x &lt;- 11:20 y &lt;- seq(3, 30, by = 3) x ## [1] 11 12 13 14 15 16 17 18 19 20 y ## [1] 3 6 9 12 15 18 21 24 27 30 Now, if we need to evaluate and represent a question like, “is x greater than than y”, we can use either &lt; or &gt;: x &gt; y ## [1] TRUE TRUE TRUE TRUE FALSE FALSE FALSE FALSE FALSE FALSE What do you notice about this simple example? The x &gt; y expression produces a logical vector, with TRUE values associated with elements in x are less than y, and FALSE otherwise. In this example, x is less than y until we reach the value of 15 in each sequence. Notice that that relational operators are vectorised: the work on an element by element basis. They wouldn’t be much use if this were not the case. What does the == operator do? It compares the elements of two vectors to determine if they are exactly equal: x == y ## [1] FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE The output of this comparison is true only for one element, the number 15, which is at the 5th position in both x and y. The != operator is essentially the opposite of ==. It identifies cases where two elements are not exactly equal. We could step through each of the different the other relational operators, but hopefully they are self-explanatory at this point (if not, experiment with them). = and == are not the same If we want to test for equivalence between the elements of two vectors we must use double equals (==), not single equals (=). Forgetting to do this == instead of = is a very common source of mistakes. The = symbol already has a use in R—assigning name-value pairs—so it can’t also be used to compare vectors because this would lead to ambiguity in our R scripts. Using = when you meant to use == is a very common mistake. If you make it, this will lead to all kinds of difficult-to-comprehend problems with your scripts. Try to remember the difference! "],
["extracting-subsets-of-vectors.html", "Chapter 6 Extracting subsets of vectors 6.1 Introduction 6.2 Subsetting by position 6.3 Subsetting with logical operators", " Chapter 6 Extracting subsets of vectors 6.1 Introduction At the beginning of the last chapter we said that an atomic vector is a 1-dimensional object that contains an ordered collection of data values. Why is this view of a vector useful? It means that we can extract a subset of elements from a vector once we know their position in that vector. There are are two main ways to subset atomic vectors, both of which we’ll cover in this chapter. Whatever the method we use, subsetting involves a pair opening and closing square brackets ([ and ]). These are always used together. 6.2 Subsetting by position We can use the [ construct with a vector to subset its elements directly using their position. Take a look at this example: c_vec &lt;- c(7.2, 3.6, 2.9) c_vec[2] ## [1] 3.6 The c_vec variable is a length 3 character vector, with elements corresponding to his first, middle and last name. In order to subset it an retain only the first element – his first name – we used the [ ] construct with the number 1 inside, placing [1] next to the name of the vector. Notice that we do not place a space anywhere in this construct. We could, but this is not conventional. Remember, “the number 1” is in fact a numeric vector of length 1. This suggest we might be able to use longer vectors to extract more than one element: # make a numeric sequence my_vec &lt;- seq(0, 0.5, by = 0.1) my_vec ## [1] 0.0 0.1 0.2 0.3 0.4 0.5 # make an indexing vector i &lt;- c(1, 3) # extract a subset of values my_vec[i] ## [1] 0.0 0.2 We first constructed a numeric vector of length 2 called index, which has elements 1 and 3. We then used this to extract his first and third value by placing i inside the [ ]. We didn’t have to carry out the subsetting operation in two steps. This achieves the same result: my_vec[c(1, 3)] ## [1] 0.0 0.2 Notice that when we subset a vector of a particular type, we get a vector of the same type back, e.g. subsetting a numeric vector produces another numeric vector. We can also subset a vector by removing certain elements. We use the - operator to do this. Here’s an example that produces the same result as the last example, but in a different way: my_vec[-c(2, 4, 5, 6)] ## [1] 0.0 0.2 The my_vec[-c(2, 4, 5, 6)] expression indicates that we want all the elements of c_vec except those that at position 2, 4, 5, and 6. In the previous chapter we learned how use the [ construct with a numeric vector of integer(s) to subset the elements of vector by their position. This works exactly the same way with character vectors: c_vec &lt;- c(&quot;Dylan&quot;, &quot;Zachary&quot;, &quot;Childs&quot;) c_vec[1] ## [1] &quot;Dylan&quot; The c_vec variable is a length 3 character vector, with elements corresponding to his first, middle and last name. We used the [ ] construct with the number 1 inside, to extract the first element (i.e. the first name). Longer vectors can be used to extract more than one element, and we can use negative numbers to remove elements: # extract the first and third value c_vec[c(1, 3)] ## [1] &quot;Dylan&quot; &quot;Childs&quot; # drop the second value (equivalent) c_vec[-2] ## [1] &quot;Dylan&quot; &quot;Childs&quot; 6.3 Subsetting with logical operators Subsetting vectors by position suffers from once major drawback—we have to know where the elements we want sit in the vector. A second way to subset a vector makes use of logical vectors alongside [ ]. This is usually done using two vectors of the same length: the focal vector we wish to subset, and a logical vector that specifies which elements to keep. Here is a very simple example: i &lt;- c(TRUE, FALSE, TRUE) c_vec[i] ## [1] &quot;Dylan&quot; &quot;Childs&quot; This should be fairly self-explanatory. The logical vector method of subsetting works element-by-element, and an element in c_vec is retained wherever the corresponding element in i contains a TRUE; otherwise it is discarded. Why is this better that using position indexing? After all, using a vector of positions to subset a vector is much more direct. The reason is that we can use relational operators to help us select elements according to particular criteria. This is best illustrated with an example. We’ll start by defining two vectors of the same length: name &lt;- c(&quot;cat&quot;, &quot;dog&quot;, &quot;wren&quot;, &quot;pig&quot;, &quot;owl&quot;) name ## [1] &quot;cat&quot; &quot;dog&quot; &quot;wren&quot; &quot;pig&quot; &quot;owl&quot; type &lt;- c(&quot;mammal&quot;, &quot;mammal&quot;, &quot;bird&quot;, &quot;mammal&quot;, &quot;bird&quot;) type ## [1] &quot;mammal&quot; &quot;mammal&quot; &quot;bird&quot; &quot;mammal&quot; &quot;bird&quot; The first, name, is a character vector containing the common names of a few animals. The second, type, is another character vector whose elements denote the type of animal, in this case, a bird or a mammal. The vectors are arranged such that the information is associated via the position of elements in each vector (cats and dogs are mammals, a wren is a bird, and so on). Let’s assume that we want to create a subset of name that only contains the names of mammals. We can do this by creating a logical vector from type, where the values are TRUE when an element is equal to &quot;mammal&quot; and FALSE otherwise. We know how to do this using the == operator: i &lt;- type == &quot;mammal&quot; i ## [1] TRUE TRUE FALSE TRUE FALSE We stored the result in a variable called i. Now all we need to do is use with i inside the [ ] construct to subset name: name[i] ## [1] &quot;cat&quot; &quot;dog&quot; &quot;pig&quot; We did this the long way to understand the logic of subsetting vectors with logical operators. This is quite verbose though, and we usually combine the two steps into a single R expression: name[type == &quot;mammal&quot;] ## [1] &quot;cat&quot; &quot;dog&quot; &quot;pig&quot; We can use any of the relational operators to subset vectors like this. If we define a numeric variable that contains the mean mass (in grams) of each animal, we can use this to subset names according to the associated mean mass. For example, if we want a subset that contains only those animals where the mean mass is greater than 1kg we use: mass &lt;- c(2900, 9000, 10, 18000, 2000) name[mass &gt; 1000] ## [1] &quot;cat&quot; &quot;dog&quot; &quot;pig&quot; &quot;owl&quot; Just remember, this way of using information in one vector to create subsets of a second vector only works if the information in each is associated via the position of their respective elements. Keeping a bunch of different vectors organised like this is difficult and error prone. In the next block we’ll learn how to use something called a data frame and the dplyr package to make working with a collection of related vectors much easier. "],
["getting-help-1.html", "Chapter 7 Getting help 7.1 Introduction 7.2 Browsing the help system 7.3 Searching for help files 7.4 Navigating help files 7.5 Vignettes and demos", " Chapter 7 Getting help 7.1 Introduction R has a comprehensive built-in help system. This system is orientated around the base R functions and packages. Every good package comes with a set of help files. At a minimum these should provide information about the individual package functions and summaries of the data included with the package. They sometimes give descriptions of how different parts of the package should be used, and if we’re lucky, one or more “vignettes” that offer a practical demonstration of how to use a package. Other files are sometimes shipped with packages. For example, these might give an overview of the mathematical or computational theory a package relies on. We will not worry about these in this course. We may as well get something out of the way early on. The word “help” in the phrase “help file” is a bit of a misnomer. It is probably more accurate to say R has an extensive documentation system. The reason we say this is that the majority help files are associated with functions, and these kinds of files are designed first and foremost to document how a particular function or group of functions are meant to be used. For example, they describe what kinds of arguments a function can take and what kind of objects it will return to us. Help files are also written in a very precise, painful-to-read manner. They contain a lot of jargon which can be hard to decipher unless you already know a lot about R or you have a computer science background. The take-home message is that R help files are aimed more at experienced users than novices. Their primary purpose is to carefully document the different elements of a package, rather than explain how a particular function or the package as whole should be used to achieve a given end. That said, help files often contain useful examples, and many package authors do try to make our life easier by providing functional demonstrations of their package (those “vignettes” we mentioned above are a vehicle for doing this). It is very important to try to get to grips with the built in help system. It contains a great deal of useful information which you need to learn how to decipher in order to start using R effectively. The road to enlightenment is bumpy though. 7.2 Browsing the help system How do we access the help system? Help files are a little like mini web pages, which means we can navigate among them using hyperlinks. This makes it very easy to explore the help system. One way to begin browsing the help system uses the help.start function: help.start() If you type this now at the Console you will see the Package Index page open up in the Help tab of the bottom right pane in RStudio. This lists all the packages currently installed on your computer. We can view all the help files associated with a package by clicking on the appropriate link. For example, the functions that come with the base installation of R have a help file associated with them – click on the link to the R base package (base) to see these. Though you know about a few of these already, you will see a lot of functions listed here. R is huge. The packages that come with the base R installation and those that we install separately from base R have their own set of associated help files. These can be viewed by following the appropriate link on the Package Index page. We will learn how to navigate these in a moment. Take note: it is up to the developer of a package to produce usable help files. Well-designed packages like dplyr and ggplot2 have an extensive help system that covers almost everything the package can do. This isn’t always the the case though, particularly with new or packages or packages that are not widely used. We will only ever use well-documented packages. Notice that the help browser has Forward, Back, and Home buttons, just like a normal web browser. If you get lost in the mire of help pages you can always navigate backward until you get back to a familiar page. Annoyingly, the Home button does not take you to the same page as help.start. Click the home button now if you have not already done so. You will see three sections: The Manuals section looks like it might be useful for novice users. Unfortunately, it is not. Even the “Introduction to R” manual is only helpful if you already have some programming experience, since it assumes you understand what terms like “data structure” and “data type” mean. It is worth reading this manual once you have gained a little more experience though. The others manuals. are more or less impenetrable unless you already know quite a bit about computing in general. The Reference section is a little more helpful. The “Packages” link just takes you to the same page opened by help.start so that you can browse help pages on a package-specific basis. The “Search Engine &amp; Keywords” link takes you to a search engine page (no surprises there). You can use this to search for specific help pages, either by supplying a search term or by navigating through the different keywords. We will discuss the built-in search engine in the next subsection. The Miscellaneous Material section has a couple of potentially useful links. The “User Manuals” link lists any user manuals supplied by package authors. These tend to be aimed at more experienced users and the packages we will learn to use in this course do not provide them. However, it is worth knowing these exist as they are occasionally useful. The “Frequently Asked Questions” link is worth reviewing once you have gained a little experience of R, but again, most of the FAQs are a little difficult for novice users to fully understand. 7.3 Searching for help files If you spend a short amount of time browsing help files via help.start it will quickly become obvious that this way of searching for help is not very efficient. Quite often we know the name of the function we need to use and all we want to do is open its associated help file. We do this using the help function: help(topic = Trig) If you run this simple example you should see RStudio open up the help file for the trigonometry topic in the Help tab. This provides information about the various trigonometric functions such as sin or cos. We will learn how to make sense of help pages in the next subsection. For now we just need to understand how to use help. The help function needs a minimum of one argument: the name of the topic or function of interest. When we use it like this the help function searches across packages, looking for a help file whose name gives an exact match to the name we supplied. In this case, we opened the help file associated with the Trig topic. Most of the time we use the help function to find the help page for a specific function, rather than a general topic. This is fine if you can remember the name of the topic associated with different functions. Most of us cannot. Luckily, the help function will also match help pages by the name of the function(s) they cover: help(topic = sin) Here we searched for help on the sin function. This is part of the Trig topic so help(topic = sin) brings up the same page as the help(topic = Trig). There are several arguments of help that we can set to alter its behaviour. We will just consider one of these. By default, the help function only searches for files associated with the base functions or with packages that we have loaded in the current session with the library function. If you wanted to search for help on the mutate function – which is part of the dplyr package – but you haven’t run library(dplyr) yet this will not work: help(mutate) ## Help on topic &#39;mutate&#39; was found in the following packages: ## ## Package Library ## dplyr /Library/Frameworks/R.framework/Versions/3.3/Resources/library ## plyr /Library/Frameworks/R.framework/Versions/3.3/Resources/library ## ## ## Using the first match ... Instead, you need tell help where to look by setting the package argument: help(mutate, package = dplyr) You should try to start using help every time you get stuck because you cannot get a function to work. It doesn’t matter how experienced an R user you become, you will occasionally forget how to use the odd function. It is for this reason that R has a built in shortcut for help. This is accessed via ?. For example, instead of typing help(topic = sin) (or just help(sin)) at the Console, we can bring up the help page for the sin function by using ? like this: ?sin This is just a convenient shortcut that does the same thing as help. The only difference is that ? does not allow us to set arguments such as package. 7.4 Navigating help files Navigating a typical help file is a little daunting at first. These files can be quite long and they contain a lot of jargon. The help files associated with functions – the most common type – have a consistent structure though. There are a number of distinct sections, whose order is always the same. Wrestling with a help file is much easier if you are at least clear about the purpose of each section. After the title, there are eight of these you need to know about: Description gives us a short overview of what the function is meant to be used for. If the help page covers a family of related functions it gives a collective overview of all the functions. You should always read this before diving into the rest of the help file. Usage shows how the function(s) are meant to be used. It lists each member of the family as well as their common arguments. The argument names are listed on their own if they have no default, or in name-value pairs, where the value gives the default used should we choose not to set it ourselves when we call the function. Arguments lists each of the allowed arguments, along with a short description of what they influence. This also tells us what what kind of data we are allowed to use with each argument, along with the allowable values (if this is relevant). You should always read this section. Details describes precisely how the function(s) behave, often in painful, jargon-laden detail. It is usually the longest and hardest-to-comprehend section but is worth reading as it flags up common “gotchas”. You can sometimes get away with ignoring this section if the previous three are informative enough, but if you really want to understand a function you need to wade through this. Value explains kind of data structure or object a function returns to us when it finishes doing whatever it does. You can often guess what this will be from the type of function, but it is worth checking to see if your reasoning is correct. If it is not, you probably don’t understand the function yet. References just lists the key reference(s) you should read if you really need to know the hows and whys of a function. You almost never need to read this. The one exception is if the function implements a particular statistical tool. In that case it might be sensible to go away and read the literature before trying to use it. See Also gives links to the help pages of related functions. These are usually functions that do something similar to the function of interest or are meant to be used in conjunction with it. You can often learn quite a bit about packages or related functions by following the links in this section. Examples provides one or more examples of how to use the function. These are stand-alone examples, so there is nothing to stop you running them. This is often the most useful section of all. Seeing a function in action is a very good way to cut through the jargon and start to understand how it works. 7.5 Vignettes and demos The Oxford English Dictionary defines a vignette as, “A brief evocative description, account, or episode.” The purpose of a package vignette in R is to provide a relatively brief, practical account of one or more of its features. Not all packages come with vignettes, though many of the best thought out packages do. You can use the vignette function to view all the available vignettes in Rstudio. This will open up a tab that lists each vignette under their associated package name along with a brief description. A package will often have more than one vignette. If you just want to see the vignettes associated with a particular package, you should set the package argument. For example, to see the vignettes associated with dplyr you would use: vignette(package = &quot;dplyr&quot;) Each vignette has a name (the “topic”) and is available either as a PDF or HTML file (or both). We can view a particular vignette by passing the vignette function the package and topic arguments. For example, to view the “data_frames” vignette in the dplyr package we would use: vignette(topic = &quot;data_frames&quot;, package = &quot;dplyr&quot;) The vignette function is fine, though it is usually more convenient to browse the list of vignettes inside a web browser. This allows you to open a particular vignette directly by clicking on its link, rather than working at the Console. We can use the browseVignettes function to do this: browseVignettes() This will open a page in your browser showing the vignettes you can view. As you might expect by now, you can narrow your options to a specific package by setting the package argument. In addition to vignettes, some packages also include one or more demos (i.e. demonstrations). Demos are a little like vignettes, but instead of just opening a file for you to read, the demo function can actually runs a demonstration R scripts for you. We use the demo function without any arguments to list the available demos: demo() When we use the demo function like this it only lists the demos associated with packages that have been loaded in the current session (via library). If we want to see all the demos you can run you need to use the somewhat cryptic demo(package = .packages(all.available = TRUE)). In order to actually run a demo we use the demo function, setting the topic and package arguments. For example, to run the “colors” demo in the grDevices package we would use: ## ## ## demo(colors) ## ---- ~~~~~~ ## ## &gt; ### ----------- Show (almost) all named colors --------------------- ## &gt; ## &gt; ## 1) with traditional &#39;graphics&#39; package: ## &gt; showCols1 &lt;- function(bg = &quot;gray&quot;, cex = 0.75, srt = 30) { ## + m &lt;- ceiling(sqrt(n &lt;- length(cl &lt;- colors()))) ## + length(cl) &lt;- m*m; cm &lt;- matrix(cl, m) ## + ## ## + require(&quot;graphics&quot;) ## + op &lt;- par(mar=rep(0,4), ann=FALSE, bg = bg); on.exit(par(op)) ## + plot(1:m,1:m, type=&quot;n&quot;, axes=FALSE) ## + text(col(cm), rev(row(cm)), cm, col = cl, cex=cex, srt=srt) ## + } ## ## &gt; showCols1() ## ## &gt; ## 2) with &#39;grid&#39; package: ## &gt; showCols2 &lt;- function(bg = &quot;grey&quot;, cex = 0.75, rot = 30) { ## + m &lt;- ceiling(sqrt(n &lt;- length(cl &lt;- colors()))) ## + length(cl) &lt;- m*m; cm &lt;- matrix(cl, m) ## + ## ## + require(&quot;grid&quot;) ## + grid.newpage(); vp &lt;- viewport(w = .92, h = .92) ## + grid.rect(gp=gpar(fill=bg)) ## + grid.text(cm, x = col(cm)/m, y = rev(row(cm))/m, rot = rot, ## + vp=vp, gp=gpar(cex = cex, col = cm)) ## + } ## ## &gt; showCols2() ## Loading required package: grid ## ## &gt; showCols2(bg = &quot;gray33&quot;) ## ## &gt; ### ## &gt; ## &gt; ##&#39; @title Comparing Colors ## &gt; ##&#39; @param col ## &gt; ##&#39; @param nrow ## &gt; ##&#39; @param ncol ## &gt; ##&#39; @param txt.col ## &gt; ##&#39; @return the grid layout, invisibly ## &gt; ##&#39; @author Marius Hofert, originally ## &gt; plotCol &lt;- function(col, nrow=1, ncol=ceiling(length(col) / nrow), ## + txt.col=&quot;black&quot;) { ## + stopifnot(nrow &gt;= 1, ncol &gt;= 1) ## + if(length(col) &gt; nrow*ncol) ## + warning(&quot;some colors will not be shown&quot;) ## + require(grid) ## + grid.newpage() ## + gl &lt;- grid.layout(nrow, ncol) ## + pushViewport(viewport(layout=gl)) ## + ic &lt;- 1 ## + for(i in 1:nrow) { ## + for(j in 1:ncol) { ## + pushViewport(viewport(layout.pos.row=i, layout.pos.col=j)) ## + grid.rect(gp= gpar(fill=col[ic])) ## + grid.text(col[ic], gp=gpar(col=txt.col)) ## + upViewport() ## + ic &lt;- ic+1 ## + } ## + } ## + upViewport() ## + invisible(gl) ## + } ## ## &gt; ## A Chocolate Bar of colors: ## &gt; plotCol(c(&quot;#CC8C3C&quot;, paste0(&quot;chocolate&quot;, 2:4), ## + paste0(&quot;darkorange&quot;, c(&quot;&quot;,1:2)), paste0(&quot;darkgoldenrod&quot;, 1:2), ## + &quot;orange&quot;, &quot;orange1&quot;, &quot;sandybrown&quot;, &quot;tan1&quot;, &quot;tan2&quot;), ## + nrow=2) ## ## &gt; ##&#39; Find close R colors() to a given color {original by Marius Hofert) ## &gt; ##&#39; using Euclidean norm in (HSV / RGB / ...) color space ## &gt; nearRcolor &lt;- function(rgb, cSpace = c(&quot;hsv&quot;, &quot;rgb255&quot;, &quot;Luv&quot;, &quot;Lab&quot;), ## + dist = switch(cSpace, &quot;hsv&quot; = 0.10, &quot;rgb255&quot; = 30, ## + &quot;Luv&quot; = 15, &quot;Lab&quot; = 12)) ## + { ## + if(is.character(rgb)) rgb &lt;- col2rgb(rgb) ## + stopifnot(length(rgb &lt;- as.vector(rgb)) == 3) ## + Rcol &lt;- col2rgb(.cc &lt;- colors()) ## + uniqC &lt;- !duplicated(t(Rcol)) # gray9 == grey9 (etc) ## + Rcol &lt;- Rcol[, uniqC] ; .cc &lt;- .cc[uniqC] ## + cSpace &lt;- match.arg(cSpace) ## + convRGB2 &lt;- function(Rgb, to) ## + t(convertColor(t(Rgb), from=&quot;sRGB&quot;, to=to, scale.in=255)) ## + ## the transformation, rgb{0..255} --&gt; cSpace : ## + TransF &lt;- switch(cSpace, ## + &quot;rgb255&quot; = identity, ## + &quot;hsv&quot; = rgb2hsv, ## + &quot;Luv&quot; = function(RGB) convRGB2(RGB, &quot;Luv&quot;), ## + &quot;Lab&quot; = function(RGB) convRGB2(RGB, &quot;Lab&quot;)) ## + d &lt;- sqrt(colSums((TransF(Rcol) - as.vector(TransF(rgb)))^2)) ## + iS &lt;- sort.list(d[near &lt;- d &lt;= dist])# sorted: closest first ## + setNames(.cc[near][iS], format(d[near][iS], digits=3)) ## + } ## ## &gt; nearRcolor(col2rgb(&quot;tan2&quot;), &quot;rgb&quot;) ## 0.0 21.1 25.8 29.5 ## &quot;tan2&quot; &quot;tan1&quot; &quot;sandybrown&quot; &quot;sienna1&quot; ## ## &gt; nearRcolor(col2rgb(&quot;tan2&quot;), &quot;hsv&quot;) ## 0.0000 0.0410 0.0618 0.0638 0.0667 ## &quot;tan2&quot; &quot;sienna2&quot; &quot;coral2&quot; &quot;tomato2&quot; &quot;tan1&quot; ## 0.0766 0.0778 0.0900 0.0912 0.0918 ## &quot;coral&quot; &quot;sienna1&quot; &quot;sandybrown&quot; &quot;coral1&quot; &quot;tomato&quot; ## ## &gt; nearRcolor(col2rgb(&quot;tan2&quot;), &quot;Luv&quot;) ## 0.00 7.42 7.48 12.41 13.69 ## &quot;tan2&quot; &quot;tan1&quot; &quot;sandybrown&quot; &quot;orange3&quot; &quot;orange2&quot; ## ## &gt; nearRcolor(col2rgb(&quot;tan2&quot;), &quot;Lab&quot;) ## 0.00 5.56 8.08 11.31 ## &quot;tan2&quot; &quot;tan1&quot; &quot;sandybrown&quot; &quot;peru&quot; ## ## &gt; nearRcolor(&quot;#334455&quot;) ## 0.0867 ## &quot;darkslategray&quot; ## ## &gt; ## Now, consider choosing a color by looking in the ## &gt; ## neighborhood of one you know : ## &gt; ## &gt; plotCol(nearRcolor(&quot;deepskyblue&quot;, &quot;rgb&quot;, dist=50)) ## ## &gt; plotCol(nearRcolor(&quot;deepskyblue&quot;, dist=.1)) ## ## &gt; plotCol(nearRcolor(&quot;tomato&quot;, &quot;rgb&quot;, dist= 50), nrow=3) ## ## &gt; plotCol(nearRcolor(&quot;tomato&quot;, &quot;hsv&quot;, dist=.12), nrow=3) ## ## &gt; plotCol(nearRcolor(&quot;tomato&quot;, &quot;Luv&quot;, dist= 25), nrow=3) ## ## &gt; plotCol(nearRcolor(&quot;tomato&quot;, &quot;Lab&quot;, dist= 18), nrow=3) This particular demo shows off some of the pre-defined colours we might use to customise the appearance of a plot. We’ve suppressed the output though because so much is produced. "],
["packages.html", "Chapter 8 Packages 8.1 The R package system 8.2 Task views 8.3 Using packages", " Chapter 8 Packages 8.1 The R package system The R package system is probably the most important single factor driving increased adoption of R among quantitatively-minded scientists. Packages make it very easy to extend the basic capabilities of R. In his book about R packages Hadley Wickam says, Packages are the fundamental units of reproducible R code. They include reusable R functions, the documentation that describes how to use them, and sample data. An R package is just a collection of folders and files in a standard, well-defined format. They bundle together together computer code, data, and documentation in a way that is easy to use and share with other users. The computer code might all be R code, but it can also include code written in other languages. Packages provide an R-friendly interface to use this “foreign” code without the need to understand how it works. The base R distribution it comes with quiet a few pre-installed packages. These are “mature” packages that implement widely used statistical and plotting functionality. These base R packages represent a very small subset of the available R packages. The majority of these are hosted on a network of web servers around the world collectively know as CRAN. This network—known as a repository—is the same one we used to download the base R distribution in the Get up and running with R and RStudio chapter. CRAN stands for the Comprehensive R Archive Network, pronounced either “see-ran” or “kran”. CRAN is a fairly spartan web site, so it’s easy to navigate. When we navigate to CRAN we see about a dozen links of the right hand side of the home page. Under the Software section there is link called Packages. Near the top of this page there is a link called Table of available packages, sorted by name that points to a very long list of all the packages on CRAN. The column on the left shows each package name, followed by a brief description of what the package does on the right. There are a huge number of packages here (over 10000 at the time of writing). 8.2 Task views A big list of packages presented like is overwhelming. Unless we already know the name of the package we want to investigate, it’s very hard to find anything useful by scanning the “all packages” table. A more user-friendly view of many R packages can be found on the Task Views page (the link is on the left hand side, under the section labelled CRAN). A Task View is basically a curated guide to the packages and functions that are useful for certain disciplines. The Task Views page shows a list of these discipline-specific topics, along with a brief description. The Environmentrics Task View maintained by Gavin Simpson contains information about using R to analyse ecological and environmental data. It is not surprising this Task View exists. Ecologists and environmental scientists are among the most enthusiastic R users. This view is a good place to start if you find yourself in need of a package to support a particular analysis in a future project. The Experimental Design, Graphics, Multivariate, Phylogenetics, Spatial, Survival and Time Series Task Views all contain many useful packages for biologists and environmental scientists. 8.3 Using packages Two things need to happen in order for us to use a package. First, we need to ensure that a copy of the folders and files that make up the package are copied to an appropriate folder on our computer. This process of putting the package files into the correct location is called installing the package. Second, we need to load and attach the package for use in a particular R session. As always, the word “session” refers to the time between when we start up R and close it down again. It’s worth unpacking these two ideas a bit, because packages are a frequent source of confusion for new users: If we don’t have a copy of a package’s folders and files in the right format and the right place on our computer we can’t use it. This is probably fairly obvious. The process of making this copy is called installing the package. it is possible to manually install packages by going to the CRAN website, downloading the package, and then using various tools install it. We won’t be using this approach though because it’s both inefficient and error prone. Instead, we’ll use built-in R functions to grab the package from CRAN and install it for us, all in one step. We don’t need to re-install a packages we plan to use every time we start a new R session. It is worth saying that again, there is no need to install a package every time we start up R / RStudio. Once we have a copy of the package on your hard drive it will remain there for us to use. The only exception to this rule is that a major update to R (not RStudio!) will sometimes require a complete re-install of the packages. This is because the R installer will not copy installed packages to the major new version of R. These major updates are fairly infrequent though, occurring perhaps every 1-2 years. Installing a package does nothing more than place a copy of the relevant files on our hard drive. If we actually want to use the functions or the data that comes with a package we need to make them available in our current R session. Unlike package installation this load and attach process as it’s known has to be repeated every time we restart R. If we forget to load up the package we can’t use it. 8.3.1 Viewing installed packages We sometimes need to check whether a package is currently installed. RStudio provides a simple, intuitive way to see which packages are installed on our computer. The Packages tab in the top right pane of RStudio shows the name of every installed package, a brief description (the same one seen on CRAN) and a version number. We can also manage our packages from this tab, as we are about to find out. There are also a few of R functions that can be used to check whether a package is currently installed. For example, the find.package function can do this: find.package(&quot;MASS&quot;) ## [1] &quot;/Library/Frameworks/R.framework/Versions/3.3/Resources/library/MASS&quot; This either prints a “file path” showing us where the package is located, or returns an error if the package can’t be found. Alternatively, the function called installed.packages returns something called a data frame (these are discussed later in the book) containing a lot more information about the installed packages. 8.3.2 Installing packages R packages can be installed from a number of different sources. For example, they can be installed from a local file on your computer, from the CRAN repository, or from a different kind of online repository called Github. Although various alternatives to CRAN are becoming more popular, we’re only going to worry about installing packages that live on CRAN in this book. This is no bad thing—the packages that live outside CRAN tend to be a little more experimental. In order to install a package from an online repository like CRAN we have to first download the package files, possibly uncompress them (like we would a ZIP file), and move them to the correct location. All of this can be done at the Console using a single function: install.packages. For example, if we want to install a package called fortunes, we use: install.packages(&quot;fortunes&quot;) The quotes are necessary by the way. If everything is working—we have an active internet connection, the package name is valid, and so on—R will briefly pause while it communicates with the CRAN servers, we should see some red text reporting back what’s happening, and then we’re returned to the prompt. The red text is just letting us know what R is up to. As long as this text does not include the word “error”, there is usually no need to worry about it. There is nothing to stop us using install.packages to install more than one package at a time. We are going to use dplyr and ggplot2 later in the book. Since neither of these is part of the base R distribution, we need to download and install them from CRAN. Here’s one way to do this: pckg.names &lt;- c(&quot;dplyr&quot;, &quot;ggplot2&quot;) install.packages(pckg.names) There are a couple of things to keep in mind. First, package names are case sensitive. For example, fortunes is not the same as Fortunes. Quite often package installations fail because we used the wrong case somewhere in the package name. The other aspect of packages we need to know about is related to dependencies: some packages rely on other packages in order to work properly. By default install.packages will install these dependencies, so we don’t usually have to worry too much about them. Just don’t be surprised if the install.packages function installs more than one package when only one was requested. Install dplyr and ggplot2 We’re going to be using dplyr and ggplot2 packages later in the book. If they aren’t already installed on your computer (check with find.package), now is a good time to install them so they’re ready to use later. RStudio provides a way of interacting with install.packages via point-and-click. The Packages tab has an “Install”&quot; button at the top right. Clicking on this brings up a small window with three main fields: “Install from”, “Packages”, and “Install to Library”. We only need to work with the “Packages” field – the other two can be left at their defaults. When we start typing in the first few letters of a package name (e.g. dplyr) RStudio provides a list of available packages that match this. After we select the one we want and click the “Install” button, RStudio invokes install.packages with the appropriate arguments at the Console for us. Never use install.packages in scripts Because installing a package is a “do once” operation, it is almost never a good idea to place install.packages in a typical R script. A script may be run 100s of times as we develop an analysis. Installing a package is quite time consuming, so we don’t really want to do it every time we run our analysis. As long as the package has been installed at some point in the past it is ready to be used and the script will work fine without re-installing it. 8.3.3 Loading and attaching packages Once we’ve installed a package or two we’ll probably want to actually use them. Two things have to happen to access a package’s facilities: the package has to be loaded into memory, and then it has to attached to something called a search path so that R can find it. It is beyond the scope of this book to get in to “how” and “why” of these events. Fortunately, there’s no need to worry about these details, as both loading and attaching can be done in a single step with a function called library. Library works exactly as you would expect it to. If we want to start using the fortunes package – which was just installed above – all we need is: library(&quot;fortunes&quot;) ## Warning: package &#39;fortunes&#39; was built under R version 3.3.2 Nothing much happens if everything is working as it should. R just returns us to the prompt without printing anything to the Console. The difference is that now we can use the functions that fortunes provides. As it turns out, there is only one, called fortune: fortune() ## ## Friends don&#39;t let friends use Excel for statistics! ## -- Jonathan D. Cryer (about problems with using Microsoft Excel for ## statistics) ## JSM 2001, Atlanta (August 2001) The fortunes package is either very useful or utterly pointless, depending on your perspective. It dispenses quotes from various R experts delivered to the venerable R mailing list (some of these are even funny). Once again, if we really don’t like working in the Console RStudio can help you out. There is a small button next to each package listed in the Packages tab. Packages that have been loaded and attached have a blue check box next to them, whereas this is absent from those that have not. Clicking on an empty check box will load up the package. Try this. Notice that all it does is invoke library with the appropriate arguments for us (RStudio explicitly sets the lib.loc argument, whereas above we just relied on the default value). Don’t use RStudio for loading packages! We looked at how it works, because at some point most people realise they can use RStudio to load and attach packages. We don’t recommend using this route though. It’s much better to put library statements into a script. Why? Because if we rely on RStudio to load packages, we have to do this every time we want to run a script, and if we forget one we need, the script won’t work. This is another example of where relying on RStudio ultimately makes things a more, not less, challenging. One last tip: we can use library anywhere, but typically the library expressions live at the very beginning of a script so that everything is ready to use later on. 8.3.4 An analogy The package system often confuses new users. The reason for this stems from the fact that they aren’t clear about what the install.packages and library functions are doing. One way to think about these is by analogy with smartphone “Apps”. Think of an R package as being analogous to a smartphone App— a package effectively extends what R can do, just as an App extends what a phone can do. When we want to try out a new App you have to first download it from an App store and install it on our phone. Once it has been downloaded, an App lives permanently on the phone (unless you delete it!) and can be used whenever it’s needed. Downloading and installing the App is something we only have to do once. Packages are no different. When we want to use an R package we first have to make sure it is installed on the computer. This is effectively what install.packages does: it grabs the package from CRAN (the “App store”) and installs it on our computer. Installing a package is a “do once” operation. Once we’ve installed it, we don’t need to install a package again each time you restart R. The package is sat on the hard drive ready to be used. In order to actually use an App which has been installed on a phone we open it up by tapping on its icon. This obviously has to happen every time we want to use the App. The package equivalent of opening a smartphone App is the “load and attach” operation. This is what library does. It makes a package available for use in a particular session. We have to use library to load the package every time you start a new R session if we plan to access the functions in that package: loading and attaching a package via library is a “do every time” operation. "],
["data-frames.html", "Chapter 9 Data frames 9.1 Introduction 9.2 Data frames 9.3 Exploring data frames 9.4 Extracting data from data frames 9.5 Final words", " Chapter 9 Data frames 9.1 Introduction We learned in the A quick introduction to R chapter that the word “variable” is used as short hand for any kind of named object. For example, we can make a variable called num_vec that refers to a simple numeric vector using: num_vec &lt;- 1:10 When a computer scientist talks about variables they’re usually referring to these sorts of name-value associations. However, the word “variable”&quot; has a second, more abstract meaning in the world data analysis and statistics: it refers to anything we can control or measure. For example, if our data comes from an experiment, the data will typically involve variables whose values describe the experimental conditions (e.g. “control plots” vs. “fertiliser plots”) and the quantities we chose to measure (e.g. species biomass and diversity). These kinds of abstract variables are often called “statistical variables”. Statistical variables can be further broken down into a range of different types. We’ll discuss these later in the book. The reason we’re pointing out the dual meaning of the word “variable” now is because we need to be able to work with both interpretations. The dual meaning is confusing at first, but both meanings are in such widespread use that we just have to get used to them. We’ll try to minimise confusion by using the phrase “statistical variable” when we are referring to data, rather than R objects. We’re introducing these ideas now because we’re going to consider a new type of data object in R: the data frame. Real world data analysis involves collections of data (“data sets”) that involve several related statistical variables. We’ve seen that an atomic vector can only be used to store one type of data such as a collection of numbers. This means a vector can be used to store a single statistical variable, How should we keep a large collection of variables organised? We could work with them separately but this is very error prone. Ideally, we need a way to keep related variables together. This is the problem that data frames and are designed to manage. 9.2 Data frames Data frames are one of those R features that mark it out as a particularly good environment for data analysis. You can think of a data frame as table-like objects with rows and columns. They collect together different statistical variables, storing each of them as a different column. Related observations are all found in the same row. This will make more sense in a moment. Let’s consider the columns first. Each column is a vector of some kind. These are usually simple vectors containing numbers or character strings, though it is also possible to include more complicated vectors inside data frames. We’ll only work with data frames made up of relatively simple vectors in this book. The key constraint that a data frame applies is that each vector must have the same length. This is what gives a data frame it table-like structure. The simplest way to get a feel for data frames is to make one. Data frames are usually constructed by reading some external data into R, but for the purposes of learning about them it is better to build one from from its component parts. We’ll make some artificial data describing a hypothetical experiment to do this. Imagine that we’ve conducted a small experiment to examine biomass and community diversity in six field plots. Three plots were subjected to fertiliser enrichment. The other three plots act as experimental controls. We could store the data describing this experiment in three vectors: trt (short for “treatment”) shows which experimental manipulation was used. bms (short for “biomass”) shows the total biomass measured at the end of the experiment. div (short for “diversity”) shows the number of species present at the end of the experiment. Here’s some R code to generate these three vectors (it doesn’t matter what the actual values are, they’re made up): trt &lt;- rep(c(&quot;Control&quot;,&quot;Fertilser&quot;), each = 3) bms &lt;- c(284, 328, 291, 956, 954, 685) div &lt;- c(8, 12, 11, 8, 4, 5) trt ## [1] &quot;Control&quot; &quot;Control&quot; &quot;Control&quot; &quot;Fertilser&quot; &quot;Fertilser&quot; &quot;Fertilser&quot; bms ## [1] 284 328 291 956 954 685 div ## [1] 8 12 11 8 4 5 Notice that the information about different observations are linked by their positions in these vectors. For example, the third control plot had a biomass of ‘291’ and a species diversity ‘11’. We can use the data.frame function to construct a data frame from one or more vectors. To build a data frame from the three vectors we created and print these to the Console, we use: experim.data &lt;- data.frame(trt, bms, div) experim.data ## trt bms div ## 1 Control 284 8 ## 2 Control 328 12 ## 3 Control 291 11 ## 4 Fertilser 956 8 ## 5 Fertilser 954 4 ## 6 Fertilser 685 5 Notice what happens when we print the data frame: it is displayed as though it has rows and columns. That’s what we meant when we said a data frame is a table-like structure. The data.frame function takes a variable number of arguments. We used the trt, bms and div vectors as arguments, resulting in a data frame with three columns. Each of these vectors has 6 elements, so the resulting data frame has 6 rows. The names of the vectors were used to name its columns. The rows do not have names, but they are numbered to reflect their position. The words trt, bms and div are not very informative. If we prefer to work with more informative column names—this is always a good idea—then we have to name the data.frame arguments: experim.data &lt;- data.frame(Treatment = trt, Biomass = bms, Diversity = div) experim.data ## Treatment Biomass Diversity ## 1 Control 284 8 ## 2 Control 328 12 ## 3 Control 291 11 ## 4 Fertilser 956 8 ## 5 Fertilser 954 4 ## 6 Fertilser 685 5 The new data frame contains the same data as the previous one but now the column names correspond to the names we chose. These names are better because they describe each variable using a human-readable word. Don’t bother with row names We can also name the rows of a data frame using the row.names argument of the data.frame function. We won’t bother to show an example of this though. Why? We can’t easily work with the information in row names so there’s not much point adding it. If we need to include row-specific data in a data frame it’s best to include an additional variable, i.e. an extra column. 9.3 Exploring data frames The first things we should do when presented with a new data set is explore its structure to understand what we’re dealing with. This is easy when the data is stored in a data frame. If the data set is reasonably small we can just print it to the Console. This is not very practical for even moderate-sized data sets though. The head and tail functions extract the first and last few rows of a data set, so these can be used to print part of a data set. The n argument controls the number of rows printed: head(experim.data, n = 3) ## Treatment Biomass Diversity ## 1 Control 284 8 ## 2 Control 328 12 ## 3 Control 291 11 tail(experim.data, n = 3) ## Treatment Biomass Diversity ## 4 Fertilser 956 8 ## 5 Fertilser 954 4 ## 6 Fertilser 685 5 The View function can be used to visualise the whole data set in a spreadsheet like view: View(experim.data) This shows the rows and columns of the data frame argument in a table- or spreadsheet-like format. When we run this in RStudio a new tab opens up with the experim.data data inside it. View only displays the data The View function is designed to allow us to display the data in a data frame as a table of rows and columns. We can’t change the data in any way with the View function. We can reorder the way the data are presented, but keep in mind that this won’t alter the underlying data. There are quite a few different R functions that will extract information about a data frame. The nrow and ncol functions return the number of rows and columns, respectively: nrow(experim.data) ## [1] 6 ncol(experim.data) ## [1] 3 The names function is used to extract the column names from a data frame: colnames(experim.data) ## [1] &quot;Treatment&quot; &quot;Biomass&quot; &quot;Diversity&quot; The experim.data data frame has three columns, so names returns a character vector of length three, where each element corresponds to a column name. There is also a rownames function if you need that too. The nrow, ncol, names and rownames functions each return a vector, so we can assign the result if we need to use it later. For example, if we want to extract and store the column names for any reason we could use varnames &lt;- names(experim.data). 9.4 Extracting data from data frames Data frames would not be much use if we could not extract and modify the data in them. In this section we will briefly review how to carry out these kinds of operations using basic R functions. 9.4.1 Extracting and adding a single variable A data frame is just a collection of variables stored in columns, where each column is a vector of some kind. There are several ways to extract these variables from a data frame. If we just want to extract a single variable we have two options. The first way of extracting a variable from a data frame uses a double square brackets construct, [[. For example, we extract the Biomass variable from our example data frame with the double square brackets like this: experim.data[[&quot;Biomass&quot;]] ## [1] 284 328 291 956 954 685 This prints whatever is in the Biomass column to the Console. What kind of object is this? It’s a numeric vector: is.numeric(experim.data[[&quot;Biomass&quot;]]) ## [1] TRUE A data frame really is nothing more than a collection of vectors. Notice that all we did was print the resulting vector to the Console. If we want to actually do something with this numeric vector we need to assign the result: bmass &lt;- experim.data$Biomass bmass^2 ## [1] 80656 107584 84681 913936 910116 469225 Here, we extracted the Biomass variable, assigned it to bmass, and then squared this. The value of Biomass variable inside the experim.data data frame is unchanged. Notice that we used &quot;Biomass&quot; instead of Biomass inside the double square brackets, i.e. we quoted the name of the variable. This is because we want R to treat the word “Biomass” as a literal value. This little detail is important! If we don’t quote the name then R will assume that Biomass is the name of an object and go in search of it in the global environment. Since we haven’t created something called Biomass, leaving out the quotes generates an error: experim.data[[Biomass]] ## Error in (function(x, i, exact) if (is.matrix(i)) as.matrix(x)[[i]] else .subset2(x, : object &#39;Biomass&#39; not found The error message is telling us that R can’t find a variable called Biomass in the global environment. On the other hand, this example does work: vname &lt;- &quot;Biomass&quot; experim.data[[vname]] ## [1] 284 328 291 956 954 685 This works because we first defined vname to be a character vector of length one, whose value is the name of a variable in experim.data. When R encounters vname inside the [[ construct it goes and finds the value associated with it and uses this value to determine the variable to extract. The second method for extracting a variable from a data frame uses the $ operator. For example, to extract the Biomass column from the experim.data data frame, we use: experim.data$Biomass ## [1] 284 328 291 956 954 685 We use the $ operator by placing the name of the data frame we want to work with on the left hand side and and the name of the column (i.e. the variable) we want to extract on the right hand side. Notice that this time we didn’t have to put quotes around the variable name when using the $ operator. We can do this if we want to—i.e. experim.data$&quot;Biomass&quot; also works—but $ doesn’t require it. Why is there more than one way to extract variables from a data frame? There’s no simple way to answer this question without getting into the details of how R represents data frames. The simple answer is that $ and [[ are not actually equivalent, even though they appear to do much the same thing. We’ve looked at the two extraction methods because they are both widely used. However, the $ method is a bit easier to read and people tend to prefer it for interactive data analysis tasks (the [[ construct tends to be used when we need a bit more flexibility). 9.4.2 Adding a variable to a data frame How do we add a new variable to an existing data frame? It turns out that the $ operator is also be used for this job by combining it with the assignment operator. Using it this way is fairly intuitive. For example, if we want to add a new (made up) variable called Elevation to experim.data, we do it like this: experim.data$Elevation &lt;- c(364, 294, 321, 358, 298, 312) This assigns some fake elevation data to a new variable in experim.data using the $ operator. The new variable is called Elevation because that was the name we used on the right hand side of $. This changes experim.data, such that it now contains four columns (variables): head(experim.data, n = 3) ## Treatment Biomass Diversity Elevation ## 1 Control 284 8 364 ## 2 Control 328 12 294 ## 3 Control 291 11 321 The [[ operator can also be used with &lt;- to add variables to a data frame. We won’t bother to show an example, as it works in exactly the same way as $ and we won’t be using the [[ method in this book. 9.4.3 Subsetting data frames What do we do if, instead of just extracting a single variable from a data frame, we need to select a subset of rows and/or columns? We use the single square brackets construct, [, to do this. There are two different ways we can use single square brackets, both of which involve the use of indexing vector(s) inside the [ construct. The first use of [ allows us to subset one or more columns while keeping all the rows. This works exactly as the [ does for vectors. Just think of columns as the elements of the data frame. For example, if we want to subset experim.data such that we are only left with the first and second columns (Treatment and Biomass), we can use a numeric indexing vector: experim.data[c(1:2)] ## Treatment Biomass ## 1 Control 284 ## 2 Control 328 ## 3 Control 291 ## 4 Fertilser 956 ## 5 Fertilser 954 ## 6 Fertilser 685 However, this is not a very good way to subset columns because we have to know the position of each variable. If for some reason we change the order of the columns, we have to update our R code accordingly. A better approach uses a character vector of column names inside the [: experim.data[c(&quot;Treatment&quot;, &quot;Biomass&quot;)] ## Treatment Biomass ## 1 Control 284 ## 2 Control 328 ## 3 Control 291 ## 4 Fertilser 956 ## 5 Fertilser 954 ## 6 Fertilser 685 The second use of [ ] is designed to allow us to subset rows and columns at the same time. We have to specify both the rows and the columns we require, using a comma (“,”) to separate a row and column index vector. This is easiest to understand with an example: # row index rindex &lt;- 1:3 # column index cindex &lt;- c(&quot;Treatment&quot;, &quot;Biomass&quot;) # subset the data farme experim.data[rindex, cindex] ## Treatment Biomass ## 1 Control 284 ## 2 Control 328 ## 3 Control 291 This example extracts a subset of experim.data corresponding to rows 1 through 3, and columns “Treatment” and “Biomass”. The rindex is a numeric vector of row positions, and cindex is a character vector of column names. This shows that rows and columns can be selected by referencing their position or their names. The rows are not named in experim.data, so we specified the positions. Storing the index vectors first is quite a long-winded way of subsetting a data frame. However, there is nothing to stop us doing everything in one step: experim.data[1:3, c(&quot;Treatment&quot;, &quot;Biomass&quot;)] ## Treatment Biomass ## 1 Control 284 ## 2 Control 328 ## 3 Control 291 If we need to subset just rows or columns we just leave out the appropriate index vector: experim.data[1:3, ] ## Treatment Biomass Diversity Elevation ## 1 Control 284 8 364 ## 2 Control 328 12 294 ## 3 Control 291 11 321 The absence of an index vector before/after the comma indicates that we want to keep every row/column. Here we kept all the columns but only the first three rows. Be careful with [ Subsetting with the [rindex, cindex] construct produces another data frame. This should be apparent from the way the last example was printed. This is usually how this construct works. We say usually, because subsetting just one column produces a vector. This is very unfortunate, as it produces unpredictable behaviour if we’re not paying attention. The [ construct works with three types of index vectors. We’ve just seen that the index vector can be a numeric or character type. The third approach uses a logical index vector. For example, we can subset the experim.data data frame, keeping just the rows where the Treatment variable is equal to “Control”, using: # make a logical index vector rindex &lt;- experim.data $ Treatment == &quot;Control&quot; rindex ## [1] TRUE TRUE TRUE FALSE FALSE FALSE # experim.data[rindex, ] ## Treatment Biomass Diversity Elevation ## 1 Control 284 8 364 ## 2 Control 328 12 294 ## 3 Control 291 11 321 Notice that we construct the logical rindex vector by extracting the Treatment variable with the $ operator and using the == operator to test for equality with “Control”. Don’t worry too much if that seems confusing. We combined many different ideas in that example. We’re going to learn a much more transparent way to achieve the same result in later chapters. 9.5 Final words We’ve seen how to extract/add variables and subset data frames using the $, [[ and [ constructs. The last example also showed that we can use a combination of relational operators (e.g. ==, != or &gt;=) and the square brackets construct to subset a data frame according to one or more criteria. There are also a number of base R functions that allow us to manipulate data frames in a slightly more intuitive way. For example, there is a function called transform that adds new variables and changes existing ones, and a function called subset to select variables and subset rows in a data frame according to the values of its variables. We’ve shown you these approaches because they are still used by many people. However, we will rely on the dplyr package to handle operations like subsetting and transforming data frame variables in this book. The dplyr package provides a much cleaner, less error prone framework for manipulating data frames, and can be used to work with similar kinds of objects that store data in consistent way. Before we can do that though, we need to learn a little bit about how to organise and import data into R. "],
["working-directories-and-data-files.html", "Chapter 10 Working directories and data files 10.1 Introduction 10.2 Data files: the CSV format 10.3 The working directory 10.4 Importing data with read.csv 10.5 Importing data with RStudio (Avoid this!) 10.6 Package data", " Chapter 10 Working directories and data files 10.1 Introduction R is able to access data from a huge range of different data storage formats and repositories. With the right tools, we can use R to pull in data from various data bases, proprietary storage formats (e.g. Excel), online web sites, or plain old text files. We aren’t going to evaluate the many packages and functions used to pull data into R—a whole book could be written about this topic alone. Instead, we’re going to examine the simplest method for data import: reading in data from a text file. We’ll also briefly look at how to access data stored in packages. 10.2 Data files: the CSV format Just about every piece of software that stores data in some kind of table-like structure can export those data to a CSV file. The CSV acronym stands for “Comma Separated Values”. CSV files are just ordinary text files. The only thing that makes them a CSV file is the fact that they store data in a particular format. This format is very simple: each row of a CSV file corresponds to a row in the data, and each value in a row (corresponding to a different column) is separated by a comma. Here is what the artificial data from the last chapter looks like in CSV format: ## &quot;trt&quot;,&quot;bms&quot;,&quot;div&quot; ## &quot;Control&quot;,284,8 ## &quot;Control&quot;,328,12 ## &quot;Control&quot;,291,11 ## &quot;Fertilser&quot;,956,8 ## &quot;Fertilser&quot;,954,4 ## &quot;Fertilser&quot;,685,5 The first line contains the variable names, with each name separated by a comma. It’s usually a good idea to include the variable names in a CSV file, though this is optional. After the variable names, each new line is a row of data. Values which are not numbers have double quotation marks around them; numeric values lack these quotes. Notice that this is the same convention that applies to the elements of atomic vectors. Quoting non-numeric values is actually optional, but reading CSV files into R works best when non-numeric values are in quotes because this reduces ambiguity. 10.2.1 Exporting CSV files from Excel Those who work with small or moderate data sets (i.e. 100s-1000s of lines) often use Excel to manage and store their data. There are good reasons for why this isn’t necessarily a sensible thing to do—for example, Excel has a nasty habit of “helpfully” formatting data values. Nonetheless, Excel is a ubiquitous and convenient tool for data management, so it’s important to know how to pull data into R from Excel. It is possible to read data directly from Excel into R, but this way of doing things can be error prone for an inexperienced user and requires us to use an external package (the readxl package is currently the best option). Instead, the simplest way to transfer data from Excel to R is to first export the relevant worksheet to a CSV file, and then import this new file using R’s standard file import tools. We’ll discuss the import tools in a moment. The initial export step is just a matter of selecting the Excel worksheet that contains the relevant data, navigating to Save As..., choosing the Comma Separated Values (.csv), and following the familiar file save routine. That’s it. After following this step our data are free of Excel and ready to be read into R. Always check your Excel worksheet Importing data from Excel can turn into a frustrating process if we’re not careful. Most problems have their origin in the Excel worksheet used to store the data, rather than R. Problems usually arise because we haven’t been paying close attention to a worksheet. For example, imagine we’re working with a very simple data set, which contains three columns of data and a few hundred rows. If at some point we accidentally (or even intentionally) add a value to a cell in the forth column, Excel will assume the fourth column is “real” data. When we then export the worksheet to CSV, instead of the expected three columns of data, we end up with four columns, where most of the fourth column is just missing information. This kind of mistake is surprisingly common and is a frequent source of confusion. The take-home message is that when Excel is used to hold raw data should be used to do just that—the worksheet containing our raw data should hold only that, and nothing else. 10.3 The working directory Before we start worrying about data import, we first need to learn a bit about how R searches for the files that reside on our computer’s hard drive. The key concept is that of the “working directory”. A “directory” is just another word for “folder”. The working directory is simply a default location (i.e. a folder) R uses when searching for files. The working directory must always be set, and there are standard rules that govern how this is chosen when a new R session starts. For example, if we start R by double clicking on an script file (i.e. a file with a “.R” extension), R will typically set the working directory to be the location of the R script. We say typically, because this behaviour can be overridden. There’s no need to learn the rules for how the default working directory is chosen, because we can always use R/RStudio to find out which folder is currently set as the working directory. Here are a couple of options: When RStudio first starts up, the Files tab in the bottom right window shows us the contents (i.e. the files and folders) of the working directory. Be careful though, if we use the file viewer to navigate to a new location this does not change the working directory. The getwd function will print the location to the working directory to the Console. It does this by displaying a file path. If you’re comfortable with file paths then the output of getwd will make perfect sense. If not, it doesn’t matter too much. Use the RStudio Files tab instead. Why does any of this matter? We need to know where R will look for files if we plan to read in data. Fortunately, it’s easy to change the working directory to a new location if we need to do this: Using RStudio, we can set the working directory via the Session &gt; Set Working Directory... &gt; Choose Directory... menu. Once this menu item is selected we’re presented with the standard file/folder dialogue box to choose the working directory. Alternatively, we can use a function called setwd at the Console, though once again, we have to be comfortable with file paths to use this. Using RStudio is easier, so we won’t demonstrate how to use setwd. 10.4 Importing data with read.csv Now that we know roughly how a CSV file is formatted, and where R will look for such files, we need to understand how to read them into R. The standard R function for reading in a CSV file is called read.csv. There are a few other options (e.g. read_csv from the readr package), but we’ll use read.csv because it’s part of the base R distribution, which means we can use it without relying on an external package. The read.csv function does one thing: given the location of a CSV file, it will read the data into R and return it to us as a data frame. There are a couple of different strategies for using read.csv. One is considered good practice and is fairly robust. The second is widely used, but creates more problems than it solves. We’ll discuss both, and then explain why the first strategy is generally better than the second. 10.4.0.1 Strategy 1—set the working directory first Remember, the working directory is the default location used by R to search for files. This means that if we set the working directory to be wherever our data file lives, we can use the read.csv function without having to tell R where to look for it. Let’s assume our data is in a CSV file called “my-great-data.csv”. We should be able to see “my-great-data.csv” in the Files tab in RStudio if the working directory is set to its location. If we can’t see it there, the working directory still needs to be set (e.g. via Session &gt; Set Working Directory... &gt; Choose Directory...). Once we’ve set the working directory to this location, reading the “my-great-data.csv” file into R is simple: my_data &lt;- read.csv(file = &quot;my-great-data.csv&quot;, stringsAsFactors = FALSE) R knows where to find the file because we first set the working directory to be the location of the file. If we forget to do this R will complain and throw an error. We have to assign the output a name so that we can actually use the new data frame (my_data in this example), otherwise all that will happen is the resulting data frame is read in and printed to the Console. 10.4.0.2 Strategy 2—use the full path to the CSV file If we are comfortable with “file paths” then read.csv can be used without bothering to set the working directory. For example, if we have the CSV file called “my-great-data.csv” on the in a folder called “r_data”, then on a Unix machine we might read it into a data frame using something like: my_data &lt;- read.csv(file = &quot;~/r_data/my-great-data.csv&quot;, stringsAsFactors = FALSE) When used like this, we have to give read.csv the full path to the file. This assumes of course that we understand how to construct a file path—the details vary depending on the operating system. 10.4.0.3 Why use the first strategy? Both methods get to the same end point: after running the code at the Console we should end up with an object called my_data in the global environment, which is a data frame containing the data in the “my-great-data.csv” file. So why should we prefer Many novice R users with no experience of programming struggle with file paths, leading to a lot of frustration and wasted time trying to specify them. The first method only requires us to set the working directory with RStudio and know the name of the file we want to read in. There’s no need to deal with file paths. The second strategy creates problems when we move your data around or work on different machines, as the file paths will need to be changed in each new situation. The first strategy is robust to such changes. For example, if we move all our data to a new location, we just have to set the working directory to the new location and our R code will still work. 10.5 Importing data with RStudio (Avoid this!) It is also possible to import data from a CSV file into R using RStudio. The steps are as follows: Click on the Environment tab in the top right pane of RStudio Select Import Dataset &gt; From Text File... Select the CSV file you want to read in to R and click Open Enter a name (no spaces allowed) or stick with the default and click Import We’re only pointing out this method because new users are often tempted to use it—we do not recommend it. Why? It creates the same kinds of problems as the second strategy discussed above. All RStudio does generate the correct usage of a function called read_csv (from the readr package) and evaluate this at the Console. The code isn’t part of a script so we have to do this every time we want to work with the data file. It’s easy to make a mistake using this approach, e.g. by accidentally misnaming the data frame or reading in the wrong data. It may be tempting to copy the generated R code into a script. However, we still have the portability problem outlined above to deal with. Take our word for it. The RStudio-focussed way of reading data into R just creates more problems than it solves. Don’t use it. 10.6 Package data Remember what Hadley Wickam said about packages? “… include reusable R functions, the documentation that describes how to use them, and sample data.” Many packages come with one or more sample data sets. These are very handy, as they’re used in examples and package vignettes. We can use the data function to get R to list the data sets hiding away in packages: data(package = .packages(all.available = TRUE)) The mysterious .packages(all.available = TRUE) part of this generates a character vector with the names of all the installed packages in it. If we only use data() then R only lists the data sets found in a package called datasets, and in packages we have loaded and attached in the current R session using the library function. The datasets package is part of the base R distribution. It exists only to store example data sets. The package is automatically loaded when we start R, i.e. there’s no need to use library to access it, meaning any data stored in this package can be accessed every time we start R. We’ll use a couple of data sets in the datasets package later to demonstrate how to work with the dplyr and ggplot2 packages. "],
["dplyr-and-the-tidy-data-concept.html", "Chapter 11 dplyr and the tidy data concept 11.1 Introduction 11.2 The value of dplyr 11.3 Tidy data 11.4 A quick look at dplyr", " Chapter 11 dplyr and the tidy data concept 11.1 Introduction [Data wrangling] 11.2 The value of dplyr The dplyr package has been carefully designed to make life easier to manipulate data frames and other kinds of similar objects. A key reason for its ease-of-use is that dplyr is very consistent in the way its functions work. For example, the first argument of the main dplyr functions is always an object containing our data. This consistency makes it very easy to get to grips with each of the main dplyr functions—it’s often possible to understand how one works by seeing one or two examples of its use. A second reason for favouring dplyr is that it is orientated around a few core functions, each of which is designed to do one thing well. The key dplyr functions are often referred to as “verbs”, reflecting the fact that they “do something” to data. For example: (1) select is used to obtain a subset of variables; (2) mutate is used to construct new variables; (3) filter is used to obtain a subset of rows; (4) arrange is used to reorder rows; and (5) summarise is used to calculate information about groups. We’ll cover each of these verbs in detail in later chapters, as well as a few additional functions such as rename and group_by. Apart from being easy to use, dplyr is also fast compared to base R functions. This won’t matter much for the small data sets we use in this book, but dplyr is a good option for large data sets. The dplyr package also allows us to work with data stored in different ways, for example, by interacting directly with a number of database systems. We won’t work with anything other than data frames (and the closely-related “tibble”) but it is worth knowing about this facility. Learning to use dplyr with data frames makes it easy to work with these other kinds of data objects. A dplyr cheat sheet The developers of RStudio have produced a very usable cheat sheat that summarises the main data wrangling tools provided by dplyr. Our advice is to download this, print out a copy and refer to this often as you start working with dplyr. 11.3 Tidy data dplyr will work with any data frame, but it’s at its most powerful when our data are organised as tidy data. The word “tidy” has a very specific meaning in this context. Tidy data has a specific structure that makes it easy to manipulate, model and visualise. A tidy data set is one where each variable is in only one column and each row contains only one observation. This might seem like the “obvious” way to organise data, but many people fail to adopt this convention. We aren’t going to explore the tidy data concept in great detail, but the basic principles are not difficult to understand. We’ll use an example to illustrate what the “one variable = one column” and “one observation = one row” idea means. Let’s return to the made-up experiment investigating the response of communities to fertilizer addition. This time, imagine we had only measured biomass, but that we had measured it twice over the course of the experiment. We’ll examine some artificial data for the experiment and look at two ways to organise it to help us understand the tidy data idea. The first way uses a separate column for each biomass measurement: ## Treatment BiomassT1 BiomassT2 ## 1 Control 284 324 ## 2 Control 328 400 ## 3 Control 291 355 ## 4 Fertilser 956 1197 ## 5 Fertilser 954 1012 ## 6 Fertilser 685 859 This often seems like the natural way to store such data, especially for experienced Excel users. However, this format is not tidy. Why? The biomass variable has been split across two columns (“BiomassT1” and “BiomassT2”), which means each row corresponds to two observations. We won’t go into the “whys” here, but take our word for it: adopting this format makes it difficult to efficiently work with data. This is not really an R-specific problem. This non-tidy format is sub-optimal in many different data analysis environments. A tidy version of the example data set would still have three columns, but now these would be: “Treatment”, denoting the experimental treatment applied; “Time”, denoting the sampling occasion; and “Biomass”, denoting the biomass measured: ## Treatment Time Biomass ## 1 Control T1 284 ## 2 Control T1 328 ## 3 Control T1 291 ## 4 Fertilser T1 956 ## 5 Fertilser T1 954 ## 6 Fertilser T1 685 ## 7 Control T2 324 ## 8 Control T2 400 ## 9 Control T2 355 ## 10 Fertilser T2 1197 ## 11 Fertilser T2 1012 ## 12 Fertilser T2 859 These data are tidy: each variable is in only one column, and each observation has its own unique row. These data are well-suited to use with dplyr. Always try to start with tidy data The best way to make sure your data set is tidy is to store in that format when it’s first collected and recorded. There are packages that can help convert non tidy data into the tidy data format (e.g. the tidyr package), but life is much simpler if we just make sure our data are tidy from the very beginning. 11.4 A quick look at dplyr We’ll finish up this chapter by taking a quick look at a few features of the dplyr package, before really drilling down into how it works. The package is not part of the base R installation, so we have to install it first via install.packages(&quot;dplyr&quot;). Remember, we only have to install dplyr once, so there’s no need to leave the install.packages line in script that uses the package. We do have to add library to the top of any scripts using the package to load and attach it: library(&quot;dplyr&quot;) We need some data to work with. We’ll use two data sets to illustrate the key ideas in the next few chapters: the iris data set in the datasets package and the storms data set in the nasaweather package. The datasets package ships with R and is loaded and attached at start up, so there’s no need to do anything to make iris available. The nasaweather package doesn’t ship with R so it needs to be installed via install.packages(&quot;nasaweather&quot;). Finally, we have to add library to the top of our script to load and attach the package: library(&quot;nasaweather&quot;) The nasaweather package is a bare bones data package. It doesn’t contain any new R functions, just data. We’ll be using the storms data set from nasaweather: this contains information about tropical storms in North America (from 1995-2000). We’re just using it as a convenient example to illustrate the workings of the dplyr, and later, the ggplot2 packages. 11.4.1 Tibble (tbl) objects The primary purpose of the dplyr package is to make it easier to manipulate data interactively. In order to facilitate this kind of work dplyr implements a special kind of data object known as a tbl (pronounced “tibble”). We can think of a tibble as a special data frame with a few extra whistles and bells. We can convert an ordinary data frame to a a tibble using the tbl_df function. It’s a good idea (though not necessary) to convert ordinary data frames to tibbles. Why? When a data frame is printed to the Console R will try to print every column and row until it reaches a (very large) maximum permitted amount of output. The result is a mess of text that’s virtually impossible to make sense of. In contrast, when a tibble is printed to the Console, it does so in a compact way. To see this, we can convert the iris data set to a tibble using tbl_df and then print the resulting object to the Console: # make a &quot;tibble&quot; copy of iris iris_tbl &lt;- tbl_df(iris) # print it iris_tbl ## # A tibble: 150 × 5 ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fctr&gt; ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa ## 7 4.6 3.4 1.4 0.3 setosa ## 8 5.0 3.4 1.5 0.2 setosa ## 9 4.4 2.9 1.4 0.2 setosa ## 10 4.9 3.1 1.5 0.1 setosa ## # ... with 140 more rows Notice that only the first 10 rows are printed. This is much nicer than trying to wade through every row of a data frame. 11.4.2 The glimpse function Sometimes we just need a quick, compact summary of a data frame or tibble. This is the job of the glimpse function from dplyr. The glimpse function is very similar to str: glimpse(iris_tbl) ## Observations: 150 ## Variables: 5 ## $ Sepal.Length &lt;dbl&gt; 5.1, 4.9, 4.7, 4.6, 5.0, 5.4, 4.6, 5.0, 4.4, 4.9,... ## $ Sepal.Width &lt;dbl&gt; 3.5, 3.0, 3.2, 3.1, 3.6, 3.9, 3.4, 3.4, 2.9, 3.1,... ## $ Petal.Length &lt;dbl&gt; 1.4, 1.4, 1.3, 1.5, 1.4, 1.7, 1.4, 1.5, 1.4, 1.5,... ## $ Petal.Width &lt;dbl&gt; 0.2, 0.2, 0.2, 0.2, 0.2, 0.4, 0.3, 0.2, 0.2, 0.1,... ## $ Species &lt;fctr&gt; setosa, setosa, setosa, setosa, setosa, setosa, ... The function takes one argument: the name of a data frame or tibble. It then tells us how many rows it has, how many variables there are, what these variables are called, and what kind of data are associated with each variable. This function is useful when we’re working with a data set containing many variables. "],
["working-with-variables.html", "Chapter 12 Working with variables 12.1 Introduction 12.2 Subset variables with select 12.3 Creating variables with mutate", " Chapter 12 Working with variables 12.1 Introduction This chapter will explore the the dplyr select and mutate verbs, as well as the related rename and transmute verbs. These four verbs are considered together because they all operate on the variables (i.e. the columns) of a data frame or tibble: The select function selects a subset of variables to retain and (optionally) renames them in the process. The mutate function creates new variables from preexisting ones and retains the original variables. The rename function renames one or more variables while keeping the remaining variable names unchanged. The transmute function creates new variables from preexisting ones and drops the original variables. 12.1.1 Getting ready A script that uses dplyr will typically start by loading and attaching the package: library(&quot;dplyr&quot;) Obviously we need to have first installed dplyr package for this to work. We’ll use the iris data set in the datasets package to illustrate the ideas in this chapter. The datasets package ships with R and is loaded and attached at start up, so there’s no need to do anything to make iris available. The iris data set is an ordinary data frame. Before we start working, it’s handy to convert this to a tibble so that it prints to the Console in a compact way: iris_tbl &lt;- tbl_df(iris) We gave the new tibble version a new name. We didn’t have to do this, but it will remind us that we’re working with tibbles. 12.2 Subset variables with select We use select to select variables from a data frame or tibble. This is typically used when we have a data set with many variables but only need to work with a subset of these. Basic usage of select looks like this: select(data_set, vname1, vname2, ...) take note: this is not an example we can run. This is a “pseudo code” example, designed to show, in abstract terms, how we use select: The first argument, data_set (“data object”), must be the name of the object containing our data. We then include a series of one or more additional arguments, where each one is the name of a variable in data_set. We’ve expressed this as vname1, vname2, ..., where vname1 and vname2 are names of the first two variables, and the ... is acting as placeholder for the remaining variables (there could be any number of these). It’s easiest to understand how a function like select works by seeing it in action. We select the Species, Petal.Length and Petal.Width variables from iris_tbl like this: select(iris_tbl, Species, Petal.Length, Petal.Width) ## # A tibble: 150 × 3 ## Species Petal.Length Petal.Width ## &lt;fctr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 setosa 1.4 0.2 ## 2 setosa 1.4 0.2 ## 3 setosa 1.3 0.2 ## 4 setosa 1.5 0.2 ## 5 setosa 1.4 0.2 ## 6 setosa 1.7 0.4 ## 7 setosa 1.4 0.3 ## 8 setosa 1.5 0.2 ## 9 setosa 1.4 0.2 ## 10 setosa 1.5 0.1 ## # ... with 140 more rows Hopefully nothing about this example is surprising or confusing. There are a few things to notice about how select works though: The select function is one of those non-standard functions we briefly mentioned in the Using functions chapter. This means the variable names should not be surrounded by quotes unless they have spaces in them (which is best avoided). The select function is just like other R functions: it does not have “side effects”. What this means is that it does not change the original iris_tbl. We printed the result produced by select to the Console, so we can’t access the new data set. If we need to access the result we have to assign it a name using &lt;-. The order of variables (i.e. the column order) in the resulting object is the same as the order in which they were supplied as arguments. This means we can reorder variables at the same time as selecting them if we need to. The select function will return the same kind of data object it is working on. It returns a data frame if our data was originally in a data frame and a tibble if it was a tibble. In this example, R prints a tibble because we had converted iris_tbl from a data frame to a tibble. It’s sometimes more convenient to use select to subset variables by specifying those we do not need, rather than specifying of the ones to keep. We use the - operator indicate that variables should be dropped. For example, to drop the Petal.Width and Petal.Length columns, we use: select(iris_tbl, -Petal.Width, -Petal.Length) ## # A tibble: 150 × 3 ## Sepal.Length Sepal.Width Species ## &lt;dbl&gt; &lt;dbl&gt; &lt;fctr&gt; ## 1 5.1 3.5 setosa ## 2 4.9 3.0 setosa ## 3 4.7 3.2 setosa ## 4 4.6 3.1 setosa ## 5 5.0 3.6 setosa ## 6 5.4 3.9 setosa ## 7 4.6 3.4 setosa ## 8 5.0 3.4 setosa ## 9 4.4 2.9 setosa ## 10 4.9 3.1 setosa ## # ... with 140 more rows This returns a tibble with just the remaining variables:Sepal.Length, Sepal.Width and Species. The select function can also be used to grab (or drop) a set of variables that occur in a sequence next to one another. We specify a series of adjacent variables using the : operator. This must be used with two variable names, one on the left hand side and one on the right. When we use : like this, select will subset both those variables along with any others that fall in between them. For example, if we need the two Petal variables and Species, we use: select(iris_tbl, Petal.Length:Species) ## # A tibble: 150 × 3 ## Petal.Length Petal.Width Species ## &lt;dbl&gt; &lt;dbl&gt; &lt;fctr&gt; ## 1 1.4 0.2 setosa ## 2 1.4 0.2 setosa ## 3 1.3 0.2 setosa ## 4 1.5 0.2 setosa ## 5 1.4 0.2 setosa ## 6 1.7 0.4 setosa ## 7 1.4 0.3 setosa ## 8 1.5 0.2 setosa ## 9 1.4 0.2 setosa ## 10 1.5 0.1 setosa ## # ... with 140 more rows The : operator can be combined with - if we need to drop a series of variables according to their position in a data frame or tibble: select(iris_tbl, -(Petal.Length:Species)) ## # A tibble: 150 × 2 ## Sepal.Length Sepal.Width ## &lt;dbl&gt; &lt;dbl&gt; ## 1 5.1 3.5 ## 2 4.9 3.0 ## 3 4.7 3.2 ## 4 4.6 3.1 ## 5 5.0 3.6 ## 6 5.4 3.9 ## 7 4.6 3.4 ## 8 5.0 3.4 ## 9 4.4 2.9 ## 10 4.9 3.1 ## # ... with 140 more rows The extra ( ) around are Petal.Length:Species important here — select will throw an error if we don’t include them. 12.2.1 Renaming variables with select and rename In addition to selecting a subset of variables, the select function can also rename variables at the same time. To do this, we have to name the arguments using =, placing the new name on the left hand side. For example, to select theSpecies, Petal.Length and Petal.Width variables from iris_tbl, but also rename Petal.Length and Petal.Width to Petal_Length and Petal_Width, we use: select(iris_tbl, Species, Petal_Length = Petal.Length, Petal_Width = Petal.Width) ## # A tibble: 150 × 3 ## Species Petal_Length Petal_Width ## &lt;fctr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 setosa 1.4 0.2 ## 2 setosa 1.4 0.2 ## 3 setosa 1.3 0.2 ## 4 setosa 1.5 0.2 ## 5 setosa 1.4 0.2 ## 6 setosa 1.7 0.4 ## 7 setosa 1.4 0.3 ## 8 setosa 1.5 0.2 ## 9 setosa 1.4 0.2 ## 10 setosa 1.5 0.1 ## # ... with 140 more rows Renaming variables is a common task when working with data frames and tibbles. What should we do if the only thing we would like to achieve is to rename a variables, rather than rename and select variables? The dplyr provides an additional function called rename for this purpose. This function renames certain variables while retaining all others. It works in a similar way to select. For example, to rename Petal.Length and Petal.Width to Petal_Length and Petal_Width, we use: rename(iris_tbl, Petal_Length = Petal.Length, Petal_Width = Petal.Width) ## # A tibble: 150 × 5 ## Sepal.Length Sepal.Width Petal_Length Petal_Width Species ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fctr&gt; ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa ## 7 4.6 3.4 1.4 0.3 setosa ## 8 5.0 3.4 1.5 0.2 setosa ## 9 4.4 2.9 1.4 0.2 setosa ## 10 4.9 3.1 1.5 0.1 setosa ## # ... with 140 more rows Notice that the rename function also preserves the order of the variables found in the original data. 12.3 Creating variables with mutate We use mutate to add new variables to a data frame or tibble. This is useful if we need to construct one or more derived variables to support an analysis. Basic usage of mutate looks like this: mutate(data_set, &lt;expression1&gt;, &lt;expression2&gt;, ...) Again, this is not an example we can run; it’s pseudo code that highlights in abstract terms how to use mutate. As always with dplyr, the first argument, data_set, should be the name of the object containing our data. We then include a series of one or more additional arguments, where each of these is a valid R expression involving one or more variables in data_set. We’ve have expressed these as &lt;expression1&gt;, &lt;expression2&gt;, where &lt;expression1&gt; and &lt;expression2&gt; represent the first two expressions, and the ... is acting as placeholder for the remaining expressions. Remember, this is not valid R code. It is just intended to demonstrate the general usage of mutate. To see mutate in action, let’s construct a new version of iris_tbl that contains a variable summarising the approximate area of sepals: mutate(iris_tbl, Sepal.Width * Sepal.Length) ## # A tibble: 150 × 6 ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fctr&gt; ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa ## 7 4.6 3.4 1.4 0.3 setosa ## 8 5.0 3.4 1.5 0.2 setosa ## 9 4.4 2.9 1.4 0.2 setosa ## 10 4.9 3.1 1.5 0.1 setosa ## # ... with 140 more rows, and 1 more variables: `Sepal.Width * ## # Sepal.Length` &lt;dbl&gt; This creates a copy of iris_tbl with a new column called Sepal.Width * Sepal.Length (mentioned at the bottom of the printed output). Most of the rules that apply to select also apply to mutate: The expression that performs the required calculation is not surrounded by quotes. This makes sense, because an expression is meant to be evaluated so that it “does something”. It is not a value. Once again, we just printed the result produced by mutate to the Console, rather than assigning the result a name using &lt;-. The mutate function does not have side effects, meaning it does not change the original iris_tbl in any way. The select function returns the same kind of data object as the one it is working on: a data frame if our data was originally in a data frame, a tibble if it was a tibble. Creating a variable called something like Sepal.Width * Sepal.Length is not exactly ideal because it’s a difficult name to work with. The mutate function can name variables at the same time as they are created. We have to name the arguments using =, placing the name on the left hand side, to do this. Here’s how to use this construct to name the new area variable Sepal.Area: mutate(iris_tbl, Sepal.Area = Sepal.Width * Sepal.Length) ## # A tibble: 150 × 6 ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species Sepal.Area ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fctr&gt; &lt;dbl&gt; ## 1 5.1 3.5 1.4 0.2 setosa 17.85 ## 2 4.9 3.0 1.4 0.2 setosa 14.70 ## 3 4.7 3.2 1.3 0.2 setosa 15.04 ## 4 4.6 3.1 1.5 0.2 setosa 14.26 ## 5 5.0 3.6 1.4 0.2 setosa 18.00 ## 6 5.4 3.9 1.7 0.4 setosa 21.06 ## 7 4.6 3.4 1.4 0.3 setosa 15.64 ## 8 5.0 3.4 1.5 0.2 setosa 17.00 ## 9 4.4 2.9 1.4 0.2 setosa 12.76 ## 10 4.9 3.1 1.5 0.1 setosa 15.19 ## # ... with 140 more rows We can create more than one variable by supplying mutate multiple (named) arguments: mutate(iris_tbl, Sepal.Area = Sepal.Width * Sepal.Length, Petal.Area = Petal.Width * Petal.Length, Area.Ratio = Petal.Area / Petal.Area) ## # A tibble: 150 × 8 ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species Sepal.Area ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fctr&gt; &lt;dbl&gt; ## 1 5.1 3.5 1.4 0.2 setosa 17.85 ## 2 4.9 3.0 1.4 0.2 setosa 14.70 ## 3 4.7 3.2 1.3 0.2 setosa 15.04 ## 4 4.6 3.1 1.5 0.2 setosa 14.26 ## 5 5.0 3.6 1.4 0.2 setosa 18.00 ## 6 5.4 3.9 1.7 0.4 setosa 21.06 ## 7 4.6 3.4 1.4 0.3 setosa 15.64 ## 8 5.0 3.4 1.5 0.2 setosa 17.00 ## 9 4.4 2.9 1.4 0.2 setosa 12.76 ## 10 4.9 3.1 1.5 0.1 setosa 15.19 ## # ... with 140 more rows, and 2 more variables: Petal.Area &lt;dbl&gt;, ## # Area.Ratio &lt;dbl&gt; Notice that here we placed each argument on a new line, remembering the comma to separate arguments. There is nothing to stop us doing this because R ignores white space. This is useful though, because it allows us, the user, to makes long function calls easier to read by breaking them up on different lines. This last example reveals a nice feature of mutate: we can use newly created variables in further calculations. Here we constructed approximate sepal and petal area variables, and then used these to construct a third variable containing the ratio of these two quantities, Area.Ratio. 12.3.1 Transforming and dropping variables Occasionally we may want to construct one or more new variables, and then drop all other variables in the original dataset. The transmute function is designed to do this. It works exactly like mutate, but it has a slightly different behaviour: transmute(iris_tbl, Sepal.Area = Sepal.Width * Sepal.Length, Petal.Area = Petal.Width * Petal.Length, Area.Ratio = Petal.Area / Petal.Area) ## # A tibble: 150 × 3 ## Sepal.Area Petal.Area Area.Ratio ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 17.85 0.28 1 ## 2 14.70 0.28 1 ## 3 15.04 0.26 1 ## 4 14.26 0.30 1 ## 5 18.00 0.28 1 ## 6 21.06 0.68 1 ## 7 15.64 0.42 1 ## 8 17.00 0.30 1 ## 9 12.76 0.28 1 ## 10 15.19 0.15 1 ## # ... with 140 more rows Here we repeated the previous example, but now only the new variables were retained in the resulting tibble. If we also want to retain one or more variables without altering them we just have to pass them as unnamed arguments. For example, if we need to retain species identity in the output, we use: transmute(iris_tbl, Species, Sepal.Area = Sepal.Width * Sepal.Length, Petal.Area = Petal.Width * Petal.Length, Area.Ratio = Petal.Area / Petal.Area) ## # A tibble: 150 × 4 ## Species Sepal.Area Petal.Area Area.Ratio ## &lt;fctr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 setosa 17.85 0.28 1 ## 2 setosa 14.70 0.28 1 ## 3 setosa 15.04 0.26 1 ## 4 setosa 14.26 0.30 1 ## 5 setosa 18.00 0.28 1 ## 6 setosa 21.06 0.68 1 ## 7 setosa 15.64 0.42 1 ## 8 setosa 17.00 0.30 1 ## 9 setosa 12.76 0.28 1 ## 10 setosa 15.19 0.15 1 ## # ... with 140 more rows "],
["working-with-observations.html", "Chapter 13 Working with observations 13.1 Introduction 13.2 Subset observations with filter 13.3 Reording observations with arrange", " Chapter 13 Working with observations 13.1 Introduction This chapter will explore the filter and arrange verbs. These are discussed together because they are used to manipulate observations (i.e. rows) of a data frame or tibble: The filter function extracts a subset of observations based on supplied conditions involving the variables in our data. The arrange function reorders the rows according to the values in one or more variables. 13.1.1 Getting ready We should start a new script by loading and attaching the dplyr package: library(&quot;dplyr&quot;) We’re going to use the storms data set in the nasaweather package this time. This means we need to load and attach the nasaweather package to make storms available: library(&quot;nasaweather&quot;) The storms data set is an ordinary data frame, so let’s convert it to a tibble so that it prints nicely: storms_tbl &lt;- tbl_df(storms) 13.2 Subset observations with filter We use filter to subset observations in a an data frame or tibble containing our data. This is often done when we want to limit an analysis to a subset of observations. Basic usage of filter looks something like this: filter(data_set, &lt;expression1&gt;, &lt;expression1&gt;, ...) Remember, this is pseudo code (it’s not an example we can run). The first argument, data_set, must the name of the object containing our data. We then include one or more additional arguments, where each of these is a valid R expression involving one or more variables in data_set. Each expression must return a logical vector. We’ve expressed these as &lt;expression1&gt;, &lt;expression2&gt;, ..., where &lt;expression1&gt; and &lt;expression2&gt; represent the first two expressions, and the ... is acting as placeholder for the remaining expressions. To see how filter in action, we’ll use it to subset observations in the storms_tbl dataset, based on two relational criteria: filter(storms_tbl, pressure &lt;= 960, wind &gt;= 100) ## # A tibble: 199 × 11 ## name year month day hour lat long pressure wind type ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; ## 1 Felix 1995 8 12 0 22.1 -57.8 955 100 Hurricane ## 2 Felix 1995 8 12 6 22.9 -59.0 943 110 Hurricane ## 3 Felix 1995 8 12 12 23.6 -60.2 932 115 Hurricane ## 4 Felix 1995 8 12 18 24.3 -61.0 929 120 Hurricane ## 5 Felix 1995 8 13 0 25.1 -61.6 930 115 Hurricane ## 6 Felix 1995 8 13 6 25.9 -61.9 937 105 Hurricane ## 7 Felix 1995 8 13 12 26.6 -62.3 942 100 Hurricane ## 8 Luis 1995 9 1 6 15.8 -42.6 958 105 Hurricane ## 9 Luis 1995 9 1 12 16.2 -43.6 950 115 Hurricane ## 10 Luis 1995 9 1 18 16.5 -44.7 948 115 Hurricane ## # ... with 189 more rows, and 1 more variables: seasday &lt;int&gt; In this example we’ve created a subset of storms_tbl that only includes observation where the pressure variable is less than or equal to 960 and the wind variable is greater than or equal to 100. Both conditions must be met for an observation to be included in the resulting tibble. The conditions are not combined as an either/or operation. This is probably starting to become repetitious, but there are a few features of filter that we should note: Each expression that performs a comparison is not surrounded by quotes. This makes sense, because the expression is meant to be evaluated to return a logical vector – it is not “a value”. As usual, the result produced by mutate in our example was printed to the Console. The mutate function did not change the original storms_tbl in any way (no side effects!). The filter function will return the same kind of data object it is working on: it returns a data frame if our data was originally in a data frame, and a tibble if it was a tibble. We can achieve the same result as the above example in a different way. This involves the &amp; operator: filter(storms_tbl, pressure &lt;= 960 &amp; wind &gt;= 100) ## # A tibble: 199 × 11 ## name year month day hour lat long pressure wind type ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; ## 1 Felix 1995 8 12 0 22.1 -57.8 955 100 Hurricane ## 2 Felix 1995 8 12 6 22.9 -59.0 943 110 Hurricane ## 3 Felix 1995 8 12 12 23.6 -60.2 932 115 Hurricane ## 4 Felix 1995 8 12 18 24.3 -61.0 929 120 Hurricane ## 5 Felix 1995 8 13 0 25.1 -61.6 930 115 Hurricane ## 6 Felix 1995 8 13 6 25.9 -61.9 937 105 Hurricane ## 7 Felix 1995 8 13 12 26.6 -62.3 942 100 Hurricane ## 8 Luis 1995 9 1 6 15.8 -42.6 958 105 Hurricane ## 9 Luis 1995 9 1 12 16.2 -43.6 950 115 Hurricane ## 10 Luis 1995 9 1 18 16.5 -44.7 948 115 Hurricane ## # ... with 189 more rows, and 1 more variables: seasday &lt;int&gt; Once again, we created a subset of storms_tbl that only includes observation where the pressure variable is less than or equal to 960 and the wind variable is greater than or equal to 100. However, rather than supplying pressure &lt;= 960 and wind &gt;= 100 as two arguments, we used a single R expression, combining them with the &amp;. We’re pointing this out because we sometimes need to subset on an either/or basis, and in those cases we have to use this second approach. For example: filter(storms_tbl, pressure &lt;= 960 | wind &gt;= 100) ## # A tibble: 266 × 11 ## name year month day hour lat long pressure wind type ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; ## 1 Felix 1995 8 12 0 22.1 -57.8 955 100 Hurricane ## 2 Felix 1995 8 12 6 22.9 -59.0 943 110 Hurricane ## 3 Felix 1995 8 12 12 23.6 -60.2 932 115 Hurricane ## 4 Felix 1995 8 12 18 24.3 -61.0 929 120 Hurricane ## 5 Felix 1995 8 13 0 25.1 -61.6 930 115 Hurricane ## 6 Felix 1995 8 13 6 25.9 -61.9 937 105 Hurricane ## 7 Felix 1995 8 13 12 26.6 -62.3 942 100 Hurricane ## 8 Felix 1995 8 13 18 27.4 -62.3 947 95 Hurricane ## 9 Felix 1995 8 14 0 28.2 -62.5 948 90 Hurricane ## 10 Felix 1995 8 14 6 29.0 -62.9 954 80 Hurricane ## # ... with 256 more rows, and 1 more variables: seasday &lt;int&gt; This creates a subset of storms_tbl that only includes observation where the pressure variable is less than or equal to 960 or the wind variable is greater than or equal to 100. We’re also not restricted to using some combination of relational operators such as ==, &gt;= or != when working with filter. The conditions specified in the filter function can be any expression that returns a logical vector. The only constraint is that the length of this logical vector has to equal the lenght of its input vectors. Here’s an example. The group membership %in% operator (part of base R, not dplyr) is used to determine whether the values in one vector occurs among the values in a second vector. It’s used like this: vec1 %in% vec2. This returns a vector where the values are TRUE if an element of vec1 is in vec2, and FALSE otherwise. We can use the %in% operator with filter to select to subset rows by the values of one or more variables: sub_storms_tbl &lt;- filter(storms_tbl, name %in% c(&quot;Roxanne&quot;, &quot;Marilyn&quot;, &quot;Dolly&quot;)) # print the sub_storms_tbl $ name ## [1] &quot;Marilyn&quot; &quot;Marilyn&quot; &quot;Marilyn&quot; &quot;Marilyn&quot; &quot;Marilyn&quot; &quot;Marilyn&quot; &quot;Marilyn&quot; ## [8] &quot;Marilyn&quot; &quot;Marilyn&quot; &quot;Marilyn&quot; &quot;Marilyn&quot; &quot;Marilyn&quot; &quot;Marilyn&quot; &quot;Marilyn&quot; ## [15] &quot;Marilyn&quot; &quot;Marilyn&quot; &quot;Marilyn&quot; &quot;Marilyn&quot; &quot;Marilyn&quot; &quot;Marilyn&quot; &quot;Marilyn&quot; ## [22] &quot;Marilyn&quot; &quot;Marilyn&quot; &quot;Marilyn&quot; &quot;Marilyn&quot; &quot;Marilyn&quot; &quot;Marilyn&quot; &quot;Marilyn&quot; ## [29] &quot;Marilyn&quot; &quot;Marilyn&quot; &quot;Marilyn&quot; &quot;Marilyn&quot; &quot;Marilyn&quot; &quot;Marilyn&quot; &quot;Marilyn&quot; ## [36] &quot;Marilyn&quot; &quot;Marilyn&quot; &quot;Marilyn&quot; &quot;Marilyn&quot; &quot;Marilyn&quot; &quot;Marilyn&quot; &quot;Marilyn&quot; ## [43] &quot;Marilyn&quot; &quot;Marilyn&quot; &quot;Marilyn&quot; &quot;Marilyn&quot; &quot;Marilyn&quot; &quot;Marilyn&quot; &quot;Marilyn&quot; ## [50] &quot;Marilyn&quot; &quot;Marilyn&quot; &quot;Marilyn&quot; &quot;Marilyn&quot; &quot;Marilyn&quot; &quot;Marilyn&quot; &quot;Marilyn&quot; ## [57] &quot;Marilyn&quot; &quot;Marilyn&quot; &quot;Marilyn&quot; &quot;Marilyn&quot; &quot;Marilyn&quot; &quot;Marilyn&quot; &quot;Marilyn&quot; ## [64] &quot;Marilyn&quot; &quot;Marilyn&quot; &quot;Marilyn&quot; &quot;Marilyn&quot; &quot;Marilyn&quot; &quot;Marilyn&quot; &quot;Marilyn&quot; ## [71] &quot;Marilyn&quot; &quot;Marilyn&quot; &quot;Marilyn&quot; &quot;Marilyn&quot; &quot;Marilyn&quot; &quot;Marilyn&quot; &quot;Marilyn&quot; ## [78] &quot;Roxanne&quot; &quot;Roxanne&quot; &quot;Roxanne&quot; &quot;Roxanne&quot; &quot;Roxanne&quot; &quot;Roxanne&quot; &quot;Roxanne&quot; ## [85] &quot;Roxanne&quot; &quot;Roxanne&quot; &quot;Roxanne&quot; &quot;Roxanne&quot; &quot;Roxanne&quot; &quot;Roxanne&quot; &quot;Roxanne&quot; ## [92] &quot;Roxanne&quot; &quot;Roxanne&quot; &quot;Roxanne&quot; &quot;Roxanne&quot; &quot;Roxanne&quot; &quot;Roxanne&quot; &quot;Roxanne&quot; ## [99] &quot;Roxanne&quot; &quot;Roxanne&quot; &quot;Roxanne&quot; &quot;Roxanne&quot; &quot;Roxanne&quot; &quot;Roxanne&quot; &quot;Roxanne&quot; ## [106] &quot;Roxanne&quot; &quot;Roxanne&quot; &quot;Roxanne&quot; &quot;Roxanne&quot; &quot;Roxanne&quot; &quot;Roxanne&quot; &quot;Roxanne&quot; ## [113] &quot;Roxanne&quot; &quot;Roxanne&quot; &quot;Roxanne&quot; &quot;Roxanne&quot; &quot;Roxanne&quot; &quot;Roxanne&quot; &quot;Roxanne&quot; ## [120] &quot;Roxanne&quot; &quot;Roxanne&quot; &quot;Roxanne&quot; &quot;Roxanne&quot; &quot;Roxanne&quot; &quot;Roxanne&quot; &quot;Roxanne&quot; ## [127] &quot;Roxanne&quot; &quot;Roxanne&quot; &quot;Roxanne&quot; &quot;Roxanne&quot; &quot;Roxanne&quot; &quot;Dolly&quot; &quot;Dolly&quot; ## [134] &quot;Dolly&quot; &quot;Dolly&quot; &quot;Dolly&quot; &quot;Dolly&quot; &quot;Dolly&quot; &quot;Dolly&quot; &quot;Dolly&quot; ## [141] &quot;Dolly&quot; &quot;Dolly&quot; &quot;Dolly&quot; &quot;Dolly&quot; &quot;Dolly&quot; &quot;Dolly&quot; &quot;Dolly&quot; ## [148] &quot;Dolly&quot; &quot;Dolly&quot; &quot;Dolly&quot; &quot;Dolly&quot; &quot;Dolly&quot; &quot;Dolly&quot; &quot;Dolly&quot; ## [155] &quot;Dolly&quot; 13.3 Reording observations with arrange We use arrange to reorder the rows of an object containing our data. This is sometimes used when we want to inspect a dataset to look for associations among the different variables. This is hard to do if they are not ordered. Basic usage of arrange looks like this: arrange(data_set, vname1, vname2, ...) Yes, this is pseudo-code. As always, the first argument, data_set, is the name of the object containing our data. We then include a series of one or more additional arguments, where each of these should be the name of a variable in data_set: vname1 and vname2 are names of the first two ordering variables, and the ... is acting as placeholder for the remaining variables. To see arrange in action, let’s construct a new version of storms_tbl where the rows have been reordered first by wind, and then by pressure: arrange(storms_tbl, wind, pressure) ## # A tibble: 2,747 × 11 ## name year month day hour lat long pressure wind ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 Fran 1996 9 9 12 45.7 -72.3 1006 15 ## 2 Fran 1996 9 9 18 46.0 -71.1 1008 15 ## 3 Fran 1996 9 10 0 46.7 -70.0 1010 15 ## 4 Frances 1998 9 13 6 31.7 -96.9 1002 20 ## 5 Dean 1995 7 31 18 30.5 -96.5 1003 20 ## 6 Erin 1995 8 4 12 33.2 -89.7 1003 20 ## 7 Erin 1995 8 4 18 34.1 -90.2 1003 20 ## 8 Erin 1995 8 5 0 34.8 -90.2 1003 20 ## 9 Erin 1995 8 5 6 35.4 -90.1 1003 20 ## 10 Erin 1995 8 5 12 36.3 -89.8 1003 20 ## # ... with 2,737 more rows, and 2 more variables: type &lt;chr&gt;, ## # seasday &lt;int&gt; This creates a new version of storms_tbl where the rows are sorted according to the values of wind and pressure in ascending order – i.e. from smallest to largest. Since wind appears before pressure among the arguments, the values of pressure are only used to break ties within any particular value of wind. For the sake of avoiding any doubt about how arrange works, let’s quickly review its behaviour: The variable names used as arguments of arrange are not surrounded by quotes. The arrange function did not change the original iris_tbl in any way. The arrange function will return the same kind of data object it is working on. There isn’t much else we need to to learn about arrange. By default, it sorts variables in ascending order. If we need it to sort a variable in descending order, we wrap the variable name in the desc function: arrange(storms_tbl, wind, desc(pressure)) ## # A tibble: 2,747 × 11 ## name year month day hour lat long pressure wind ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 Fran 1996 9 10 0 46.7 -70.0 1010 15 ## 2 Fran 1996 9 9 18 46.0 -71.1 1008 15 ## 3 Fran 1996 9 9 12 45.7 -72.3 1006 15 ## 4 Barry 1995 7 5 6 32.0 -72.0 1019 20 ## 5 Barry 1995 7 5 12 32.0 -72.0 1019 20 ## 6 Barry 1995 7 5 18 31.9 -72.0 1018 20 ## 7 Marilyn 1995 9 30 12 34.6 -49.3 1016 20 ## 8 Marilyn 1995 9 30 18 34.7 -50.0 1016 20 ## 9 Marilyn 1995 10 1 0 34.8 -50.5 1016 20 ## 10 Marilyn 1995 10 1 6 35.0 -51.0 1016 20 ## # ... with 2,737 more rows, and 2 more variables: type &lt;chr&gt;, ## # seasday &lt;int&gt; This creates a new version of storms_tbl where the rows are sorted according to the values of wind and pressure, in ascending and descending order, respectively. "],
["helper-functions.html", "Chapter 14 Helper functions 14.1 Introduction 14.2 Working with select 14.3 Working with mutate and transmute 14.4 Working with filter", " Chapter 14 Helper functions 14.1 Introduction There are a number of helper functions supplied by dplyr. Many of these are shown in the handy dplyr cheat sheat. This is a short chapter. We aren’t going to try to cover every single helper function here. Instead, we’ll highlight some of the more useful ones, and point out where the others tend to be used. We also assume that the storms_tbl and iris_tbl tibbles have already been constructed (look over the previous two chapters to see how this is done). 14.2 Working with select There are relatively few of helper functions that can be used with select. The job of these functions is to make it easier to match variable names according to various criteria. We’ll look at the three simplest of these, but look at the examples in the help file for select and the cheat sheat to see what else is available. We can select variables according to the sequence of characters used at the start of their name with the starts_with function. For example, to select all the variables in iris_tbl that begin with the word “Petal”, we use: select(iris_tbl, starts_with(&quot;petal&quot;)) ## # A tibble: 150 × 2 ## Petal.Length Petal.Width ## &lt;dbl&gt; &lt;dbl&gt; ## 1 1.4 0.2 ## 2 1.4 0.2 ## 3 1.3 0.2 ## 4 1.5 0.2 ## 5 1.4 0.2 ## 6 1.7 0.4 ## 7 1.4 0.3 ## 8 1.5 0.2 ## 9 1.4 0.2 ## 10 1.5 0.1 ## # ... with 140 more rows This returns a table containing just Petal.Length and Petal.Width. As you might expect, there is also a helper function to select variables according to characters used at the end of their name. This is the ends_with function (no surprises here). To select all the variables in iris_tbl that end with the word “Length”, we use: select(iris_tbl, ends_with(&quot;length&quot;)) ## # A tibble: 150 × 2 ## Sepal.Length Petal.Length ## &lt;dbl&gt; &lt;dbl&gt; ## 1 5.1 1.4 ## 2 4.9 1.4 ## 3 4.7 1.3 ## 4 4.6 1.5 ## 5 5.0 1.4 ## 6 5.4 1.7 ## 7 4.6 1.4 ## 8 5.0 1.5 ## 9 4.4 1.4 ## 10 4.9 1.5 ## # ... with 140 more rows Notice that we have to quote the character string that we want to match against. This is not optional. However, the starts_with and ends_with functions are not case sensitive by default. For example, I passed starts_with the argument &quot;petal&quot; instead of &quot;Petal&quot;, yet it still selected variables beginning with the character string &quot;Petal&quot;. If you want to select variables on a case-sensitive basis, you need to set an argument ignore.case to FALSE in starts_with and ends_with. The last select helper function we will look at is called contains. You can guess what this does. It allows us to select variables based on a partial match anywhere in their name. Look at what happens if we pass contains the argument &quot;.&quot;: select(iris_tbl, contains(&quot;.&quot;)) ## # A tibble: 150 × 4 ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 5.1 3.5 1.4 0.2 ## 2 4.9 3.0 1.4 0.2 ## 3 4.7 3.2 1.3 0.2 ## 4 4.6 3.1 1.5 0.2 ## 5 5.0 3.6 1.4 0.2 ## 6 5.4 3.9 1.7 0.4 ## 7 4.6 3.4 1.4 0.3 ## 8 5.0 3.4 1.5 0.2 ## 9 4.4 2.9 1.4 0.2 ## 10 4.9 3.1 1.5 0.1 ## # ... with 140 more rows This selects all the variables with a dot in their name. There is nothing to stop us combining the different variable selection methods. For example, we can use this approach to select all the variables whose names start with the word “Petal” or end with the word “Length”: select(iris_tbl, ends_with(&quot;length&quot;), starts_with(&quot;petal&quot;)) ## # A tibble: 150 × 3 ## Sepal.Length Petal.Length Petal.Width ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 5.1 1.4 0.2 ## 2 4.9 1.4 0.2 ## 3 4.7 1.3 0.2 ## 4 4.6 1.5 0.2 ## 5 5.0 1.4 0.2 ## 6 5.4 1.7 0.4 ## 7 4.6 1.4 0.3 ## 8 5.0 1.5 0.2 ## 9 4.4 1.4 0.2 ## 10 4.9 1.5 0.1 ## # ... with 140 more rows When we apply more than one selection criteria like this the select function returns all the variables that match either criteria, rather than just the set that meets all the criteria. 14.3 Working with mutate and transmute There are quite a few helper functions that can be used with mutate. These make it easier to add new variables according to the values of other variables. We won’t explore these here as they tend to be needed only in quite specific circumstances. However, if you run into a situation where you need to construct an unusual variable – for example, one that ranks the values in another variable – you may want to try looking at the examples in the mutate help file or that handy cheat sheat to see what options are available to you. 14.4 Working with filter There is only one dplyr helper function that is useful with filter. This is the between function, which is used to identify the values of a variable that lie inside a defined range: filter(storms_tbl, between(pressure, 960, 970)) ## # A tibble: 213 × 11 ## name year month day hour lat long pressure wind type ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; ## 1 Felix 1995 8 11 18 21.3 -56.5 965 90 Hurricane ## 2 Felix 1995 8 14 12 29.9 -63.4 962 80 Hurricane ## 3 Felix 1995 8 14 18 30.7 -64.1 962 75 Hurricane ## 4 Felix 1995 8 15 0 31.3 -65.1 962 75 Hurricane ## 5 Felix 1995 8 15 6 31.9 -66.2 964 75 Hurricane ## 6 Felix 1995 8 15 12 32.5 -67.4 968 70 Hurricane ## 7 Felix 1995 8 15 18 33.1 -68.8 965 70 Hurricane ## 8 Felix 1995 8 16 0 33.5 -70.1 963 70 Hurricane ## 9 Felix 1995 8 16 6 34.0 -71.3 966 70 Hurricane ## 10 Felix 1995 8 16 12 34.6 -72.4 968 70 Hurricane ## # ... with 203 more rows, and 1 more variables: seasday &lt;int&gt; This example filters the storms dataset such that only values of pressure between 960 and 970 are retained. "],
["grouping-and-summarising-data.html", "Chapter 15 Grouping and summarising data 15.1 Summarising variables with summarise 15.2 Grouped operations using group_by 15.3 Removing grouping information", " Chapter 15 Grouping and summarising data This chapter will explore the summarise and group_by verbs. These two verbs are considered together because they are often used together, and their usage is quite distinct from the other dplyr verbs we’ve encountered: The group_by function adds information into a data object (e.g. a data frame or tibble), which makes subsequent calculations happen on a group-specific basis. The summarise function is a data reduction function calculates single-number summaries of one or more variables, respecting the group structure if present. 15.0.1 Getting ready We can start a new script by loading and attaching the dplyr package: library(&quot;dplyr&quot;) We’re going to use both the storms and iris data sets in the nasaweather and datasets packages, respectively. The datasets package ships is automatically loaded and attached at start up, so we need to make the nasaweather package available: library(&quot;nasaweather&quot;) Finally, let’s convert both data sets to a tibble so they print to the Console cleanly: storms_tbl &lt;- tbl_df(storms) iris_tbl &lt;- tbl_df(iris) 15.1 Summarising variables with summarise We use summarise to calculate summaries of variables in a an object containing our data. We do this kind of calculation all the time when analysing data. In terms of pseudo-code, usage of summarise looks like this: summarise(data_set, &lt;expression1&gt;, &lt;expression2&gt;, ...) The first argument, data_set, must be the name of the data frame or tibble containing our data. We then include a series of one or more additional arguments, each of these is a valid R expression involving at least one variable in data_set. These are given by the pseudo-code placeholder &lt;expression1&gt;, &lt;expression2&gt;, ..., where &lt;expression1&gt; and &lt;expression2&gt; represent the first two expressions, and the ... is acting as placeholder for the remaining expressions. These expressions can be any calculation involving R functions. The only constraint is that they must generate a single value when evaluated. That last sentence was important. It’s easy to use summarise if can we remember one thing: summarise is designed to work with functions that take a vector as their input and return a single value (i.e. a vector of length one). Any calculation that does this can be used with summarise. The summarise verb is best understood by example. The R function called mean takes a vector of numbers (several numbers) and calculates their arithmetic mean (one number). We can use mean with summarise to calculate the mean of the Petal.Length and Petal.Width variables in iris_tbl like this: summarise(iris_tbl, mean(Petal.Length), mean(Petal.Width)) ## # A tibble: 1 × 2 ## `mean(Petal.Length)` `mean(Petal.Width)` ## &lt;dbl&gt; &lt;dbl&gt; ## 1 3.758 1.199333 Notice what kind of object summarise returns: it’s a tibble with only one row and two columns. There are two columns because we calculated two means, and there is one row containing these means. Simple. There are a few other things to note about how summarise works: As with all dplyr functions, the expression that performs the required summary calculation is not surrounded by quotes because it is an expression that it “does some calculations”. The order of the expression in the resulting tibble is the same as the order in which they were used as arguments. Even though the dimensions of the output object have changed, summarise returns the same kind of data object as its input. It returns a data frame if our data was originally in a data frame, or a tibble if it was in a tibble. Notice that summarise used the expressions to name the variables. Variable names like mean(Petal.Length) and mean(Petal.Width) are not very helpful. They’re quite long for one. More problematically, they contain special reserved characters like (, which makes referring to columns in the resulting tibble more difficult than it needs to be: # make a summary tibble an assign it a name iris_means &lt;- summarise(iris_tbl, mean(Petal.Length), mean(Petal.Width)) # extract the mean petal length iris_means$`mean(Petal.Length)` ## [1] 3.758 We have to place ‘back ticks’ (as above) or ordinary quotes around the name to extract the new column when it includes special characters. It’s better to avoid using the default names. The summarise function can name the new variables at the same time as they are created. Predictably, we do this by naming the arguments using =, placing the name we require on the left hand side. For example: summarise(iris_tbl, Mean_PL = mean(Petal.Length), Mean_PW = mean(Petal.Width)) ## # A tibble: 1 × 2 ## Mean_PL Mean_PW ## &lt;dbl&gt; &lt;dbl&gt; ## 1 3.758 1.199333 There are very many base R functions that can be used with summarise. A few useful ones for calculating summaries of numeric variables are: min and max calculate the minimum and maximum values of a vector. mean and median calculate the mean and median of a numeric vector. sd and var calculate the standard deviation and variance of a numeric vector. We can combine more than one function in a summarise expression as long as it returns a single number. This means we can do arbitrarily complicated calculations in a single step. For example, if we need to know the ratio of the mean and median values of petal length and petal width in iris_tbl, we use: summarise(iris_tbl, Mn_Md_PL = mean(Petal.Length) / median(Petal.Length), Mn_Md_PW = mean(Petal.Width) / median(Petal.Width)) ## # A tibble: 1 × 2 ## Mn_Md_PL Mn_Md_PW ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0.863908 0.9225641 Notice that we placed each argument on a separate line in this example. This is just a style issue—we don’t have to do this, but since R doesn’t care about white space, we can use new lines and spaces to keep everything a bit more more human-readable. It pays to organise summarise calculations like this as they become longer. It allows us to see the logic of the calculations more easily, and helps us spot potential errors when they occur. 15.1.1 Helper functions There are a small number dplyr helper functions that can be used with summarise. These generally provide summaries that aren’t available directly using base R functions. For example, the n_distinct function is used to calculate the number of distinct values in a variable: summarise(iris_tbl, Num.PL.Vals = n_distinct(Petal.Length)) ## # A tibble: 1 × 1 ## Num.PL.Vals ## &lt;int&gt; ## 1 43 This tells us that there are 43 unique values of Petal.Length. We won’t explore any others here. The handy cheat sheat is worth looking over to see what additional options are available. 15.2 Grouped operations using group_by Performing a calculation with one or more variables over the whole data set is useful, but very often we also need to carry out an operation on different subsets of our data. For example, it’s probably more useful to know how the mean sepal and petal traits vary among the different species in the iris_tbl data set, rather than knowing the overall mean of these traits. We could calculate separate means by using filter to create different subsets of iris_tbl, and then using summary on each of these to calculate the relevant means. This would get the job done, but it’s not very efficient and very soon becomes tiresome when we have to work with many groups. The group_by function provides a more elegant solution to this kind of problem. It doesn’t do all that much on its own though. All the group_by function does is add a bit of grouping information to a tibble or data frame. In effect, it defines subsets of data on the basis of one or more grouping variables. The magic happens when the grouped object is used with a dplyr verb like summarise or mutate. Once a data frame or tibble has been tagged with grouping information, operations that involve these (and other) verbs are carried out on separate subsets of the data, where the subsets correspond to the different values of the grouping variable(s). Basic usage of group_by looks like this: group_by(data_set, vname1, vname2, ...) The first argument, data_set (“data object”), must be the name of the object containing our data. We then have to include one or more additional arguments, where each of these is the name of a variable in data_set. I have expressed this as vname1, vname2, ..., where vname1 and vname2 are names of the first two variables, and the ... is acting as placeholder for the remaining variables. As usual, it’s much easier to understand how group_by works once we’ve seen it in action. We’ll illustrate group_by by using it alongside summarise with the storms_tbl data set. We’re aiming to calculate the mean wind speed for every type of storm. The first step is to use group_by to add grouping information to storms_tbl: group_by(storms_tbl, type) ## Source: local data frame [2,747 x 11] ## Groups: type [4] ## ## name year month day hour lat long pressure wind ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 Allison 1995 6 3 0 17.4 -84.3 1005 30 ## 2 Allison 1995 6 3 6 18.3 -84.9 1004 30 ## 3 Allison 1995 6 3 12 19.3 -85.7 1003 35 ## 4 Allison 1995 6 3 18 20.6 -85.8 1001 40 ## 5 Allison 1995 6 4 0 22.0 -86.0 997 50 ## 6 Allison 1995 6 4 6 23.3 -86.3 995 60 ## 7 Allison 1995 6 4 12 24.7 -86.2 987 65 ## 8 Allison 1995 6 4 18 26.2 -86.2 988 65 ## 9 Allison 1995 6 5 0 27.6 -86.1 988 65 ## 10 Allison 1995 6 5 6 28.5 -85.6 990 60 ## # ... with 2,737 more rows, and 2 more variables: type &lt;chr&gt;, ## # seasday &lt;int&gt; Compare this to the output produced when we print the original storms_tbl data set: storms_tbl ## # A tibble: 2,747 × 11 ## name year month day hour lat long pressure wind ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 Allison 1995 6 3 0 17.4 -84.3 1005 30 ## 2 Allison 1995 6 3 6 18.3 -84.9 1004 30 ## 3 Allison 1995 6 3 12 19.3 -85.7 1003 35 ## 4 Allison 1995 6 3 18 20.6 -85.8 1001 40 ## 5 Allison 1995 6 4 0 22.0 -86.0 997 50 ## 6 Allison 1995 6 4 6 23.3 -86.3 995 60 ## 7 Allison 1995 6 4 12 24.7 -86.2 987 65 ## 8 Allison 1995 6 4 18 26.2 -86.2 988 65 ## 9 Allison 1995 6 5 0 27.6 -86.1 988 65 ## 10 Allison 1995 6 5 6 28.5 -85.6 990 60 ## # ... with 2,737 more rows, and 2 more variables: type &lt;chr&gt;, ## # seasday &lt;int&gt; There is almost no difference between the printed information. group_by really doesn’t do much on its own. If you are paying close attention you will have noticed that the tibble from group_by(storms_tbl, type) has a little bit of additional information printed at the top: Groups: type [4]. The Groups: type part of this tells us that the tibble is grouped by the type variable and nothing else. The [4] part tells us that there are 4 different groups. The only thing group_by did was add this grouping information to a copy of storms_tbl. The original storms_tbl object was not altered in any way. If we actually want to do anything useful useful with the result we need to assign it a name so that we can work with it: storms_grouped &lt;- group_by(storms_tbl, type) Now we have a grouped tibble called storms_grouped, where the groups are defined by the values of type. Any operations on this tibble will now be performed on a “by group” basis. To see this in action, we use summarise to calculate the mean wind speed: summarise(storms_grouped, mean.wind = mean(wind)) ## # A tibble: 4 × 2 ## type mean.wind ## &lt;chr&gt; &lt;dbl&gt; ## 1 Extratropical 40.06068 ## 2 Hurricane 84.65960 ## 3 Tropical Depression 27.35867 ## 4 Tropical Storm 47.32181 When we used summarise on an ungrouped tibble the result was a tibble with one row: the overall global mean. Now the resulting tibble has four rows, one for each value of type: The type variable in the new tibble tells us what these values are; the mean.wind variable shows the mean wind speed for each value. 15.2.1 More than one grouping variable What if we need to calculate summaries using more than one grouping variable? The workflow is unchanged. Let’s assume we want to know the mean wind speed and atmospheric pressure associated with each storm type in each year. We first make a grouped copy of the data set with the appropriate grouping variables: # group the storms_tbl data by storm year + assign the result a name storms_grouped &lt;- group_by(storms_tbl, type, year) # storms_grouped ## Source: local data frame [2,747 x 11] ## Groups: type, year [24] ## ## name year month day hour lat long pressure wind ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 Allison 1995 6 3 0 17.4 -84.3 1005 30 ## 2 Allison 1995 6 3 6 18.3 -84.9 1004 30 ## 3 Allison 1995 6 3 12 19.3 -85.7 1003 35 ## 4 Allison 1995 6 3 18 20.6 -85.8 1001 40 ## 5 Allison 1995 6 4 0 22.0 -86.0 997 50 ## 6 Allison 1995 6 4 6 23.3 -86.3 995 60 ## 7 Allison 1995 6 4 12 24.7 -86.2 987 65 ## 8 Allison 1995 6 4 18 26.2 -86.2 988 65 ## 9 Allison 1995 6 5 0 27.6 -86.1 988 65 ## 10 Allison 1995 6 5 6 28.5 -85.6 990 60 ## # ... with 2,737 more rows, and 2 more variables: type &lt;chr&gt;, ## # seasday &lt;int&gt; We grouped the storms_tbl data by type and year and assigned the grouped tibble the name storms_grouped. When we print this to the Console we see Groups: type, year [24] near the top, which tells us that the tibble is grouped by two variables with 24 unique combinations of values. We then calculate the mean wind speed and pressure of each storm type in each year: summarise(storms_grouped, mean_wind = mean(wind), mean_pressure = mean(pressure)) ## Source: local data frame [24 x 4] ## Groups: type [?] ## ## type year mean_wind mean_pressure ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Extratropical 1995 38.67521 995.2991 ## 2 Extratropical 1996 40.42105 991.2526 ## 3 Extratropical 1997 38.94737 999.7632 ## 4 Extratropical 1998 42.92208 990.6104 ## 5 Extratropical 1999 38.86364 992.0909 ## 6 Extratropical 2000 39.68254 996.7619 ## 7 Hurricane 1995 81.99187 969.6016 ## 8 Hurricane 1996 85.50336 969.1275 ## 9 Hurricane 1997 80.39474 976.3947 ## 10 Hurricane 1998 87.04142 972.4260 ## # ... with 14 more rows This calculates mean wind speed and atmospheric pressure for different combination of type and year. The first line shows us that the mean wind speed and pressure associated with extra-tropical storms in 1995 was 38.7 mph and 995 millibars, the second line shows us that the mean wind speed and pressure associated with extra-tropical storms in 1995 was 40.4 mph and 991 millibars, and so on. There are 24 rows in total because there were 24 unique combinations of type and year in the original storms_tbl. 15.2.2 Using group_by with other verbs The summarise function is the only dplyr verb we’ll use with grouped tibbles in this book. However, all the main verbs alter their behaviour to respect group identity when used with tibbles with grouping information. When mutate or transmute are used with a grouped object they still add new variables, but now the calculations occur “by group”. Here’s an example using transmute: # group the storms data by storm name + assign the result a name storms_grouped &lt;- group_by(storms_tbl, name) # create a data set &#39;mean centred&#39; wind speed variable transmute(storms_grouped, wind_centred = wind - mean(wind)) ## Adding missing grouping variables: `name` ## Source: local data frame [2,747 x 2] ## Groups: name [79] ## ## name wind_centred ## &lt;chr&gt; &lt;dbl&gt; ## 1 Allison -14.393939 ## 2 Allison -14.393939 ## 3 Allison -9.393939 ## 4 Allison -4.393939 ## 5 Allison 5.606061 ## 6 Allison 15.606061 ## 7 Allison 20.606061 ## 8 Allison 20.606061 ## 9 Allison 20.606061 ## 10 Allison 15.606061 ## # ... with 2,737 more rows In this example we calculated the “group mean-centered” version of the wind variable. The new wind_centred variable contains the difference between the wind speed and the mean of whichever storm type is associated with the observation. 15.3 Removing grouping information On occasion it’s necessary to remove grouping information from a data object. This is most often done when working with “pipes” (the topic of the next chapter) when we need to revert back to operating on the whole data set. The ungroup function removes grouping information: ungroup(storms_grouped) ## # A tibble: 2,747 × 11 ## name year month day hour lat long pressure wind ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 Allison 1995 6 3 0 17.4 -84.3 1005 30 ## 2 Allison 1995 6 3 6 18.3 -84.9 1004 30 ## 3 Allison 1995 6 3 12 19.3 -85.7 1003 35 ## 4 Allison 1995 6 3 18 20.6 -85.8 1001 40 ## 5 Allison 1995 6 4 0 22.0 -86.0 997 50 ## 6 Allison 1995 6 4 6 23.3 -86.3 995 60 ## 7 Allison 1995 6 4 12 24.7 -86.2 987 65 ## 8 Allison 1995 6 4 18 26.2 -86.2 988 65 ## 9 Allison 1995 6 5 0 27.6 -86.1 988 65 ## 10 Allison 1995 6 5 6 28.5 -85.6 990 60 ## # ... with 2,737 more rows, and 2 more variables: type &lt;chr&gt;, ## # seasday &lt;int&gt; Looking at the top right of the printed summary, we can see that the Group: part is now gone—the ungroup function effectively created a copy of storms_grouped that is identical to the original storms_tbl tibble. "],
["building-piplines.html", "Chapter 16 Building piplines 16.1 Why do we need ‘pipes’?", " Chapter 16 Building piplines This chapter will introduce something called the pipe operator: %&gt;%. We don’t often use the various dplyr verbs in isolation. Instead, starting with our raw data, they are combined in a sequence to prepare the data for further analysis (e.g. making a plot, calculating summaries, fitting a statistical model, and so on). The function of the pipe operator is to make the data wrangling part of such a workflow as transparent as possible. 16.1 Why do we need ‘pipes’? We’ve seen that carrying out calculations on a per-group basis can be achieved by grouping a tibble, assigning this a name, and then applying the summarise function to the new tibble. For example, if we need the mean wind speed for every storm recorded in storms_tbl, we could use: # 1. make a grouped copy of the storms data storms_grouped &lt;- group_by(storms_tbl, name) # 2. calculate the mean wind speed for each storm summarise(storms_grouped, mean.wind = mean(wind)) ## # A tibble: 79 × 2 ## name mean.wind ## &lt;chr&gt; &lt;dbl&gt; ## 1 Alberto 63.04598 ## 2 Alex 35.38462 ## 3 Allison 44.39394 ## 4 Ana 32.10526 ## 5 Arlene 39.03846 ## 6 Arthur 35.22727 ## 7 Barry 39.76190 ## 8 Bertha 60.00000 ## 9 Beryl 36.11111 ## 10 Bill 50.55556 ## # ... with 69 more rows There’s nothing wrong with this way of doing things. However, this approach to building up an analysis is quite verbose—especially if an analysis involves more than a couple of steps—because we have to keep storing intermediate steps. It also tends to clutter the global environment with lots of data objects we don’t need. One way to make things more concise is to use a nested function call (we examined these in the Using functions chapter), like this: summarise(group_by(storms_tbl, type), mean.wind = mean(wind)) ## # A tibble: 4 × 2 ## type mean.wind ## &lt;chr&gt; &lt;dbl&gt; ## 1 Extratropical 40.06068 ## 2 Hurricane 84.65960 ## 3 Tropical Depression 27.35867 ## 4 Tropical Storm 47.32181 Here we placed the group_by function call inside the list of arguments to summarise. Remember, you have to read nested function calls from the inside out to understand what they are doing. This is exactly equivalent to the previous example. We get the same result without having to store intermediate data. However, there are a couple of good reasons why this approach is not advised: Experienced R users probably don’t mind this approach because they’re used to nested functions calls. Nonetheless, no reasonable person would argue that nesting functions inside one another is intuitive. Reading outward from the inside of a large number of nested functions is hard work. Even for experienced R users, using function nesting is a fairly error prone approach. For example, it’s very easy to accidentally put an argument or two on the wrong side of a closing ). If we’re lucky this will produce an error and we’ll catch the problem. If we’re not, we may just end up with nonsense in the output. There’s a third option for combing several functions that has the dual benefit of keeping our code concise and readable, while avoiding the need to clutter the global environment with intermediate objects. This third approach involves something called the “pipe” operator: %&gt;% (no spaces allowed). This isn’t part of base R though. Instead, it’s part of a package called magrittr. but there’s no need to install this if we’re using dplyr because dplyr imports it for us. The %&gt;% operator has become very popular in recent years. The main reason for this is because it allows us to specify a chain of function calls in a (reasonably) human readable format. Here’s how we write the previous example using the pipe operator %&gt;%: storms_tbl %&gt;% group_by(., type) %&gt;% summarise(., mean.wind = mean(wind)) ## # A tibble: 4 × 2 ## type mean.wind ## &lt;chr&gt; &lt;dbl&gt; ## 1 Extratropical 40.06068 ## 2 Hurricane 84.65960 ## 3 Tropical Depression 27.35867 ## 4 Tropical Storm 47.32181 How do we make sense of this? Every time we see the %&gt;% operator it means the following: take whatever is produced by the left hand expression and use it as an argument in the function on the right hand side. The . serves as a placeholder for the location of the corresponding argument. This means we can understand what a sequence of calculations is doing by reading from left to right, just as we would read the words in a book. This example says, take the storms_tbl data, group it by type, then take the resulting grouped tibble and apply the summarise function to it to calculate the mean of wind. It is exactly the same calculation we did above. When using the pipe operator we can often leave out the . placeholder. Remember, this signifies the argument of the function on the right of %&gt;% that is associated with the result from on left of %&gt;%. If we choose to leave out the ., the pipe operator assumes we meant to slot it into the first argument. This means we can simplify our example even more: storms_tbl %&gt;% group_by(type) %&gt;% summarise(mean.wind = mean(wind)) ## # A tibble: 4 × 2 ## type mean.wind ## &lt;chr&gt; &lt;dbl&gt; ## 1 Extratropical 40.06068 ## 2 Hurricane 84.65960 ## 3 Tropical Depression 27.35867 ## 4 Tropical Storm 47.32181 This is why the first argument of a dplyr verb is always the data object. This convention ensures that we can use %&gt;% without explicitly specifying the argument to match against. Remember, R does not care about white space, which means we can break a chained set of function calls over several lines if it becomes too long: storms_tbl %&gt;% group_by(type) %&gt;% summarise(mean.wind = mean(wind)) ## # A tibble: 4 × 2 ## type mean.wind ## &lt;chr&gt; &lt;dbl&gt; ## 1 Extratropical 40.06068 ## 2 Hurricane 84.65960 ## 3 Tropical Depression 27.35867 ## 4 Tropical Storm 47.32181 In fact, many dplyr users always place each part of a pipeline onto a new line to help with overall readability. Finally, when we need to assign the result of a chained functions we have to break the left to right rule a bit, placing the assignment at the beginning: new_data &lt;- storms_tbl %&gt;% group_by(type) %&gt;% summarise(mean.wind = mean(wind)) (Actually, there is a rightward assignment operator, -&gt;, but let’s not worry about that) Why is %&gt;% called the ‘pipe’ operator? The %&gt;% operator takes the output from one function and “pipes it” to another as the input. It’s called ‘the pipe’ for the simple reason that it allows us to create an analysis ‘pipeline’ from a series of function calls. Incidentally, if you Google the phrase “magritt pipe” you’ll see that magrittr is a very clever name for an R package. One final piece of advice: learn how to use the %&gt;% method of chaining together functions. Why? Because it’s the simplest and cleanest method for doing this, many of the examples in the dplyr help files and on the web use it, and the majority of people carrying out real world data wrangling with dplyr rely on piping. "]
]
