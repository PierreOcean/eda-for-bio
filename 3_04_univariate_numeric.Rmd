# Exploring one numeric variable

This chapter shows how to... 

We'll use the `storms` data from the `nasaweather` package in this section (remember to load and attach the package). 

## Exploring numerical variables {#explore-numeric}

Wind speed and atmospheric pressure are clearly numeric variables. We can say a bit more. They are both numeric variables that are measured on a ratio scale because zero really is zero: it makes sense to say that 20 mph is twice as fast as 10 mph and 1000 mbar exerts twice as much pressure on objects as 500 mbar. Are these continuous or discrete variables? Think about the possible values that wind speed and atmospheric pressure can take. A wind speed and atmospheric pressure of 40.52 mph and 1000.23 mbar are perfectly reasonable values, so fundamentally, these are continuous variables.

The simplest way to understand our data, if not the most effective, is to view it in its raw form. We can always use the `View` function to do this in RStudio. However, since this doesn't work on a web page, we'll take a quick look at the first 100 values of the `wind` and `pressure` variables in `storms`. We can print these to the Console by extracting each of them with the `$` operator, using the `[` construct to subset the first 100 elements of each vector:
```{r}
# first 100 values of atmospheric pressure
storms$pressure[1:100]
# first 100 values of wind speed
storms$wind[1:100]
```
Notice that even though `pressure` is continuous variables it looks like a discrete variable because it has only been measured to the nearest whole millibar. Similarly, `wind` is only measured to the nearest 5 mph. These differences reflect the limitations of the methodology used to measure each variable, e.g. measuring wind speed is hard because it varies so much in space and time.

This illustrates an important idea: we can't just look at the values a numeric variable takes in a sample to determine whether it is discrete or continuous. In one sense the `pressure` variable is a discrete variable because of the way it was measured, even though we know that atmospheric pressure is really continuous. 

Whether we treat it as continuous or discrete is an analysis decision. These sorts of distinctions often don't matter too much when we're exploring data, but they can matter when we're deciding how to analyse it statistically. We have to make a decision about how to classify a variable based on knowledge of its true nature and the measurement process. For example, imagine that we were only able to measure wind speed to the nearest 25 mph. In this situation we would only "see" a few different categories of wind speed, so it might be sensible to treat the `wind` variable as an ordinal, categorical variable.

## Graphical summaries

We looked at the first 100 values of `wind` and `pressure` because `storms` is such a large data set. It is hard to say much about the sample distribution of these two variables by just looking at these values. If the data set has been sorted, these might not even be representative of the wider sample. What else might we do? One useful tool is "binning". The idea behind binning a variable is very simple. It involves two steps. First, we take the set of possible values of our numeric variable and divide this into a equal size, contiguous intervals. We can use any interval size we like, as long as it is large enough to span at least two observations some of the time, though in practice some are more sensible than others. We then have to work out how many values of the focal variable fall inside each bin. The resulting set of counts tells us quiet a lot about the sample distribution.

This is best illustrated with an example. Binning is very tedious to do by hand. Luckily, there are a couple of base R function that can do this for us, called `cut` and `table`. Here is how to use these to bin the `pressure` variable into intervals of 10 mbar: 
```{r}
which.bins <- cut(storms$pressure, breaks = seq(900, 1020, by = 5), right = FALSE)
table(which.bins)
```
I am not going to explain how `cut` and `table` work as we really only need to understand the output they produce. If you want to understand how they work though, take a look at `which.bins` and experiment with the `breaks` argument, then read the help files. The output of `table` is a named numeric vector. The names of each element describe each interval, and the corresponding values are the observation counts in each interval. What does this tell us? It shows that most pressure observations associated with storm systems are round about 1000 mbar. Values higher than 1000 mbar are rare, but a range of values below this are possible, with lower and lower values becoming less frequent.

These binned data tell us quite a lot about the sample distribution of `pressure`, but it is still difficult to really perceive the information in this output when it is presented as a series of numbers. What we really need is some kind of visualisation to help us interpret these numbers. This is what a __histogram__ provides. Histograms are designed to summarise the sample distribution of a variable by showing the counts of binned data as a series of bars. The position and width of each bar corresponds to an interval, and the height shows the count. Here is the histogram that corresponds to the binned data we just made: 
```{r, echo=FALSE}
ggplot(storms, aes(x = pressure)) + geom_histogram(breaks = seq(900, 1020, by = 5))
```

This gives a very clear summary of the sample distribution of pressure. It reveals: 1) the most common values, which are just above 1000 mbar; 2) the range of the data, which is about 100 mbar; and 3) the shape of the distribution, which is asymmetric, with a tendency toward very very low values.

You can probably tell that I used `ggplot2` to make that histogram. We could do this the long way by building a new data set with the binned data and using this with `ggplot2` to construct the histogram manually. However, there is a much easier way to achieve the same result. Let's see how it works. Rather than do it one one step with a single R expression, we will break the process up into two steps.

The first step uses the `ggplot` function, along with `aes`, to set up the default data and aesthetic mapping:
```{r}
plt.hist <- ggplot(storms, aes(x = pressure))
summary(plt.hist)
```
This is really no different than the extended scatter plot example we stepped through in the last chapter. The only difference is that a histogram requires only one mapping. I supplied the argument `x = pressure` to `aes` because I want to display the map intervals associated with `pressure` to the x axis. We do not need to supply an aesthetic mapping for the y axis because `ggplot2` is going to handle this for us.

The second step adds a layer to the `plt.hist` object. Rather than use the `layer` function, we will use one of the `geom_XX` functions. Unsurprisingly, we need to use the `geom_histogram` function to add the appropriate layer using the `+` operator:
```{r}
plt.hist <- plt.hist + geom_histogram()
summary(plt.hist)
```
Look at the text of the summary of the added layer below the `----`. This shows that `geom_histogram` adds a stat to the layer, the `stat_bin`. What this means is that `ggplot2` is going to take the raw `pressure` data and bin it for us. We don't need to do it. Everything we need to plot a histogram is now set up in the `plt` object. Here is the resulting plot:
```{r}
plt.hist
```

(There is no need to worry about the warning message) The resulting plot is not quite the same as the example I produced above because it uses different intervals. It is generally a good idea to play around with these a little until you arrive at a plot you like. We have to set the properties of the `geom_histogram` to tweak this kind of thing. I set the bin locations in the last plot using a `breaks = seq(900, 1020, by = 5)` argument because I wanted the histogram to exactly match the binned data we had calculated. If you don't need to exactly define where the bins occur, the `binwidth` a simpler way to adjust the width of the bins used. Let's construct the histogram again with 7 mbar wide bins, as well as adjust the colour scheme and axis labels:
```{r}
plt.hist <- ggplot(storms, aes(x = pressure)) + 
            geom_histogram(binwidth = 7, fill = "steelblue", colour="darkgrey", alpha = 0.8) + 
            xlab("Atmospheric Pressure (mbar)") + ylab("Count")
summary(plt.hist)
plt.hist
```

The summary output just shows how the arguments passed to `geom_histogram` feed through to the different components of the resulting layer. Whether or not you think the colour scheme is an improvement or not is a matter of taste. Mostly I wanted to demonstrate what the `fill`, `colour`, and `alpha` arguments change. Notice that the effect of increasing the bin width is to smooth the histogram, i.e. this version looks less jagged than the last. I prefer the bin width in this version because I think it more clearly reveals the shape of the `pressure` sample distribution. 

We can use the same R code to produce a histogram summarising the wind speed sample distribution:
```{r}
plt.hist <- ggplot(storms, aes(x = wind)) + 
            geom_histogram(binwidth = 10, fill = "steelblue", colour="darkgrey", alpha = 0.8) + 
            xlab("Wind Speed (mph)") + ylab("Count")
plt.hist
```

The only things we changed in this example were the aesthetic mapping (obviously) and the bin width, which we set to 10. It reveals that the wind speed during a storm tends to be about 40 mph, though the range of wind speeds is about 100 mph. The shape of the distribution is asymmetric.

We had to choose the bin width carefully in this example. Remember that wind speed is measured to the nearest 5 mph. This means we should choose a bin width that is a multiple of 5 to produce a meaningful histogram. Look what happens if we set the bin width to 3:
```{r}
plt.hist <- ggplot(storms, aes(x = wind)) + 
            geom_histogram(binwidth = 3, fill = "steelblue", colour="darkgrey", alpha = 0.8) + 
            xlab("Wind Speed (mph)") + ylab("Count")
plt.hist
```

We end up with gaps in histogram because some of the bin intervals do not include multiples of 5. This is not a very good histogram. Similar problems would occur if we chose a bin width that is greater than, but not a multiple of 5, because different bins would cover a different number of values that make up the `wind` variable. The take home message is that you have to know your data in order to produce meaningful summaries of it.

## Descriptive statistics

So far we've been describing the properties of sample distributions in very general terms, using phrases like "most common values" and "range of the data" without really saying what we mean. Statisticians have devised specific terms to describe these kinds of properties, as well as different descriptive statistics to quantify them. The two that often matter most are the __central tendency__ and the __dispersion__:

* A measure of __central tendency__ describes a typical ('central') value of a distribution. Most people know at least one measure of central tendency. The "average" that they calculated at school is in fact the arithmetic mean of a sample. There are many different measures of central tendency. Take a look at the [Wikipedia](http://en.wikipedia.org/wiki/Central_tendency) to see the most common ones. 

* A measure of __dispersion__ describes how spread out a distribution is. Dispersion measures quantify the variability or scatter of a variable. If one distribution is more dispersed than another it means that in some sense it encompasses a wider range of values. What this means in practice depends on the kind of measure you are working with though. Basic statistics courses usually focus the two most common measures of the dispersion: the variance, and its square root, the standard deviation. There are a few [others](http://en.wikipedia.org/wiki/Statistical_dispersion) though. 

### Measuring central tendency

There are two quantitative measures that are widely used to describe the central tendency of numeric variables in a sample. The first is the __arithmetic mean__ of a sample. People often say "empirical mean", "sample mean" or just "the mean" when referring to the arithmetic sample mean. This is fine, but keep in mind that there are other kinds of mean (e.g. the harmonic mean and the geometric mean)^[There is also a very important distinction to be made between the sample mean and the (unobserved) population mean, but we can ignore this distinction for now as we are only concerned with samples. The distinction matters when thinking about statistical models and tests].

So how do we calculate the arithmetic sample mean of a variable? Here's the mathematical definition:
$$
\bar{x} = \frac{1}{N}\sum\limits_{i=1}^{N}{x_i}
$$
We need to define the terms to make sense of this. The $\bar{x}$ stands for the arithmetic sample mean. The $N$ in the right hand side of this expression is the sample size, i.e. the number of observations in a sample. The $x_i$ refer to the set of values the variable takes in the sample. The $i$ is an index used to reference each observation: the first observation has value $x_1$, the second has value $x_2$, and so on, up to the last value, $x_N$. Finally, the $\Sigma_{i=1}^{N}$ stands for summation ('adding up') from $i = 1$ to $N$. 

Most people have used this formula at some point, though they may not have realised it. In  R, the `mean` function exists to calculate the arithmetic mean for us:
```{r}
mean(storms$wind)
```
This tells us that the arithmetic sample mean of wind speed is 55 mph. How useful is this? The mean is widely used to summarise the central tendency of samples

One limitation of the arithmetic mean is that it is affected by the shape of a distribution. That is, it gets pulled around by the tails of the distribution. This is why, for example, it does not make much sense to look at the mean income of workers in the UK to get a sense of what a "typical" person earns. Income distribution are highly skewed to right, so those of us lucky enough to earn good salaries tend to shift the mean upward. The sample mean is also strongly affected by the presence of "outliers". It is difficult to give a precise definition of outliers -- the "right" definition depends on the context -- but roughly speaking, these are unusually large or small values. 

Because the sample mean is so sensitive to the shape of a distribution and the presence of outliers, we often prefer to use a second measure of central tendency: the __sample median__. The median of a sample is the number separating the upper half from the lower half. If we have an even sample size, exactly half the data are larger than the median and half the data are smaller than the median. The sample median is then half way between the largest value of the lower half and the smallest value of the upper half. If a sample has an odd number of observations the sample median is just the value of the observation that divides the remaining data into two equal sized high- and low-value sets. We can compute a sample median in R by sorting the focal variable and then finding the observation(s) in the middle of the resulting sequence. There are more efficient ways to do this though. This is implemented with the `median` function. For example, the sample median of `wind` is given by:
```{r}
median(storms$wind)
```
The sample median of wind speed is 50 mph. This is still to the right of the most common values of wind speed, but it shifted less than the mean, as expected.

What does the phrase "the most common values" (e.g. of wind speed) really mean when describing a distribution? In fact, this is an indirect reference to something called the __mode__ of the distribution. The mode of a distribution is essentially its peak, i.e. it locates the most likely value of a variable. Notice that I did not use the phrase "sample mode". It is easy to calculate the mode of theoretical distributions of numerical variables. Unfortunately, it is not a simple matter to reliably estimate the mode of their samples.

If a numeric variable is discrete, and we have a lot of data, we can sometimes arrive at an estimate of the mode by tabulating the number of observations in each numeric category. For example, although in truth wind speed is a continuous variable, it is only measured to the nearest 5mph in the `storms` data set. Consequently, in the sample it behaves like a discrete variable. Let's find the most common value of `wind`. We can use the `table` function to tabulate the number of observations at each value:
```{r}
table(storms$wind)
```
The names of each element in the resulting vector are the wind speeds recorded in `wind`, and the corresponding values in the vector are the associated counts of observations. The most common wind speed is 30 mph, with 345 observations. The categories either side of this (25 and 35 mph) contain much lower counts. This indicates that the mode of the `wind` distribution is about 30mph.

Keep in mind that tabulating the counts in each numeric category to identify the (approximate) mode is only sensible when a numerical variable is genuinely discrete, or at least looks discrete as a result of how it was measured. Even then, there is no guarantee that this approach will produce a sensible estimate of the mode. If a variable is continuous then tabulating counts simply does not work. Imagine that `wind` had been measured to two decimal places. It is very likely that each observation of wind speed will be unique. Methods exist to estimate a mode from a sample, but they are not entirely simple. It is important to know what the mode represents and to be able to identify its approximate value in a sample (e.g. by inspecting a histogram). However, we are not going to consider more rigorous approaches this book.

### Measuring dispersion

There are many different ways to quantify the dispersion of a sample distribution. The most important quantities are often the sample __variance__ and __standard deviation__. The sample variance is the sum of squared deviations (i.e. the differences) of each observation from the sample mean, divided by the sample size minus one. We often use $s^2$ to denote the sample variance. Here is the mathematical definition:
$$
s^2 = \frac{1}{N-1}\sum\limits_{i=1}^{N}{(x_i-\bar{x})^2}
$$
The meaning of these terms is the same as for the sample mean. The $\bar{x}$ is the sample mean, the $N$ is the sample size, and the $x_i$ refers to the set of values the variable takes. Here is one way to use R to apply it to the `wind` variable in `storms`:
```{r}
wnd <- storms$wind
sum((wnd - mean(wnd))^2) / (length(wnd) - 1)
```
This takes advantage of the vectorised nature of the `-` and `^` operators to calculate the set of squared deviations, and then uses the `sum` and `length` functions to sum these and divide by the sample size minus one. There is of course an R function to calculate the sample variance:
```{r}
var(storms$wind)
```
A variance is always non-negative. A small variance indicates that observations all tend to be close to the mean (and to one another), while a high variance indicates that observations are very spread out. A variance of zero only occurs if all the values are identical. However, it is difficult to interpret whether a sample variance is really "small" or "large" because the calculation involves squared deviations. This means the relationship between the dispersion we perceive in a histogram and the associated sample variance is not linear. For example, changing the measurement scale of a variable by 10 involves a 100-fold change (10^2^) in the variance.

The variance is a important quantity in statistics that crops up over and over again. For example, many common statistical tools use changes in variance to formally compare how well different models describe a data set. However, because it is difficult to interpret, we often prefer to use a closely related quantity to describe sample dispersion: the __standard deviation__ of the sample, usually denoted $s$. The standard deviation is just the square root of the variance. We calculate it using the `sd` function:
```{r}
sd(storms$wind)
```
The standard deviation of the wind speed sample is 26. Take another look at the wind speed histogram. This shows that the wind speed measurements span about 5 standard deviations. If we had instead measured wind speed in kilometers per hour (kph), the standard deviation of the sample would be 42, because 1 mph ~ 1.6 kph. If we plot the histogram of wind speed in kph it is still the case that the data are spanned by about 5 standard deviations (try it if you doubt this). The variance on the other hand increases from to approximately 668 to 1730, a factor of 1.6^2^. This is the reason we often use the standard deviation compare dispersion: it reflects the dispersion we perceive in the data.

The sample standard deviation is not without problems though. Like the sample mean, it is sensitive to the shape of a distribution and the presence of outliers (the variance even more so). A measure of dispersion that is more robust to these kinds of features is the __interquartile range__. We need to know what a quartile is before we can define this. We can define three quartiles for a sample. These divide the data into four equal sized groups, from the set of smallest numbers up to the set of largest numbers. The second quartile ($Q_2$) is the median, i.e. it divides the data into an upper and lower half. The first quartile ($Q_1$) is the number between the smallest observation and the median. The third quartile ($Q_3$) is the number between the median and the largest observation. Confusingly, these also have other names. The first quartile is sometimes called the lower quartile or the 25th percentile, the second quartile (the median) is the 50th percentile, and the third quartile is also called the upper quartile or the 75th percentile. 

The interquartile range (IQR) is defined as the difference between the third and first quartile. This means the IQR contains the middle 50% of values of a variable. Obviously, the more spread out the data are, the larger the IQR will be. The reason we sometimes prefer to use IQR to measure dispersion is that it only depends on the data in the "middle" of a sample distribution, making it very robust to the presence of outliers. We can use the `IQR` function to find the interquartile range of the wind variable:
```{r}
IQR(storms$wind)
```
The IQR is used as the basis for a useful data summary plot called a "box and whiskers" plot. We will see how to construct this in the next chapter.

```{r, eval = FALSE, echo = FALSE}
# qrtl <- quantile(storms$pressure, probs = c(0.25, 0.75))
# iqr <- qrtl[2]-qrtl[1]
# mdn <- median(storms$pressure)
# 
# plt.hist <- ggplot(storms, aes(x = pressure)) + 
#             geom_histogram(binwidth = 6, fill = "steelblue", 
#                            colour="darkgrey", alpha = 0.8) + 
#             xlab("Atmospheric Pressure (mbar)") + ylab("Count")
# 
# plt.hist <- plt.hist + 
#             annotate("segment", 
#                      x = mdn - 1.5 * iqr, xend = mdn + 1.5 * iqr, y = 1035, yend = 1035, 
#                      colour = "black", lwd=0.8) +
#             annotate("rect", fill = "white", colour = "black", 
#                      xmin = qrtl[1], xmax= qrtl[2], ymin = 1020, ymax = 1050) + 
#             annotate("segment", colour = "black", 
#                      x = mdn, xend = mdn, y = 1020, yend = 1050)
# 
# 
# plt.hist
```

### Skewness

A well-defined hierarchy has been developed by statisticians to describe and quantify the shape of distributions. It's essential to know about the first two (central tendency and dispersion) because these are the basis of many standard statistical tools. The next most important aspect of a distribution is its __skewness__ (or just 'skew'). Skewness describes the asymmetry of a distribution about its center. Just as with central tendency and dispersion, there are a number of different ways to quantify the skewness of a sample distribution. However, these can be difficult to interpret. We'll just explore the most common case: skewness of a __unimodal__ distribution.

A unimodal distribution is one that has a single peak. We can never say for certain that a sample distribution is unimodal or not---unimodality is really a property of theoretical distributions---but with enough data and a sensible histogram we can at least say that a distribution is probably unimodal. The histograms we produced to describe the sample distributions of `wind` and `pressure` certainly appear to be unimodal. Each has a single distinct peak. 

These two unimodal distributions are also asymmetric. That is, they exhibit skewness. We say that the `pressure` distribution is skewed to the left, because it has a long 'tail' that spreads out in this direction. In contrast, we say that the `wind` distribution is skewed to the right, because it has a long 'tail' that spreads out to right. Left skewness and right skewness are also called negative and positive skew, respectively. A sample distribution that looks symmetric is said to have (approximately) zero skew. Strictly speaking the terms "negative" and "positive" are reserved for situations where we have calculated a quantitative measure of skew. However, they are often used informally in verbal descriptions of skewness.

The reason we care about skewness is that many common statistical models assume that the distribution of data, after controlling for other variables, is not skewed. This is an issue for a stitistics course. For now, we just need to understand what skewness means and be able to describe distributions in terms of left (negative) and right (positive) skew.
